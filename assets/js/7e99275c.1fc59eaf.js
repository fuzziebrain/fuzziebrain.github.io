"use strict";(self.webpackChunknewsite=self.webpackChunknewsite||[]).push([[24074],{12358:e=>{e.exports=JSON.parse('{"blogPosts":[{"id":"/a-yearful-of-ai-but-whats-next","metadata":{"permalink":"/content/a-yearful-of-ai-but-whats-next","source":"@site/posts/a-yearful-of-ai-but-whats-next/index.md","title":"A Yearful of AI But What\'s Next?","description":"As 2023 draws to a close, I reflect on year where frontier models dominate the news and focus in the industry, and my aspirations for 2024.","date":"2023-12-31T15:30:00.000Z","formattedDate":"December 31, 2023","tags":[{"label":"frontier models","permalink":"/content/tags/frontier-models"},{"label":"large language models","permalink":"/content/tags/large-language-models"},{"label":"llms","permalink":"/content/tags/llms"},{"label":"foundation models","permalink":"/content/tags/foundation-models"},{"label":"machine learning","permalink":"/content/tags/machine-learning"},{"label":"oci","permalink":"/content/tags/oci"},{"label":"generative ai","permalink":"/content/tags/generative-ai"},{"label":"knowledge graph","permalink":"/content/tags/knowledge-graph"},{"label":"embeddings","permalink":"/content/tags/embeddings"}],"readingTime":12.275,"hasTruncateMarker":true,"authors":[{"name":"Adrian Png","title":"Senior Cloud Solutions Architect @ Insum","url":"https://github.com/fuzziebrain","imageURL":"https://github.com/fuzziebrain.png","key":"fuzziebrain"}],"frontMatter":{"title":"A Yearful of AI But What\'s Next?","description":"As 2023 draws to a close, I reflect on year where frontier models dominate the news and focus in the industry, and my aspirations for 2024.","image":"./assets/sunrise-whitehorse-1280x720.jpg","tags":["frontier models","large language models","llms","foundation models","machine learning","oci","generative ai","knowledge graph","embeddings"],"categories":["Technology"],"authors":["fuzziebrain"],"date":"2023-12-31T15:30:00.000Z"},"nextItem":{"title":"Generate Art Using Latent Diffusion Models and NFC Tags","permalink":"/content/generate-art-using-diffusion-models-and-nfc-tags"}},"content":"![A beautiful winter sunrise in Whitehorse, Yukon, Canada.](./assets/sunrise-whitehorse-1280x720.jpg)\\n\\nIn the last one year since [OpenAI](https://openai.com) popularised [generative AI](https://wikipedia.org/wiki/Generative_artificial_intelligence), a lot of the excitement and intrigue have been centred on the generative power of large language models (LLMs) or foundational models. Understandably so. However, the generative power of this models only cover a segment of business needs. The underlying promise of these frontier models is in its ability to \\"understand\\" natural language, and that to me, is where we will find more utility.\\n\\nFor the last few years at [Insum](https://www.insum.ca), we\'ve had an almost biannual tradition of reviewing what\'s been going on in the AI/ML space, in particular, around what Oracle has been doing to provide innovative solutions and tools. If you missed those, I have provided the links below. In the absence of an updated review, I am hoping that this final 2023 blog post would serve somewhat as a stopgap.\\n\\n\x3c!-- truncate --\x3e\\n\\nIn late 2020, we did an interview with world-renowned ML expert, [Heli Helskyaho](https://helifromfinland.blog/), where we discussed about AI and its impact on society.\\n\\n<youtube youTubeId=\\"BHX-Vt0OU-s\\" />\\n\\n[Monty Latiolais](http://montylatiolais.blogspot.com/) prophetically asked about whether AI could be used to improve developer productivity. At Insum, we had looked at [GitHub CoPilot](https://github.com/features/copilot) in its beta days, thanks to [Federico Gaete](https://ar.linkedin.com/in/fedegaete). However, at that time, we found that it provided limited help to us as it was trained over public repositories, and PL/SQL is overwhelmingly underrepresented. We haven\'t given up.\\n\\nIt is exciting that Oracle is working towards a \\"no code\\" platform :wink:. In this [blog post](https://blogs.oracle.com/apex/post/generative-ai-apex-1), the authors outline the various facets where AI will assist Oracle APEX and citizen developers create applications and write SQL code. And not to forget, on the Oracle Autonomous Database, there already is [Select AI](https://docs.oracle.com/en/cloud/paas/autonomous-database/serverless/adbsb/sql-generation-ai-autonomous.html). I had [written previously](./speaking-ai-to-your-oracle-database) on my experience, and am still anxiously waiting for the feature that allows you to use a self-hosted LLM.\\n\\nIn early 2022, I did a one-to-one interview with [Marc Ruel](https://ca.linkedin.com/in/marcruel), and we had discussed how we can quite easily build \\"AI APEX applications\\" by calling the various Oracle Cloud Infrastructure (OCI) AI services\' REST APIs, and/or use AutoML to train and deploy in-database ML models.\\n\\n<youtube youTubeId=\\"gavA55-3UXo\\" />\\n\\nHave you tried them out?\\n\\n## OCI Generative AI\\n\\n> **IMPORTANT DISCLAIMER**\\n>\\n> While I, the author, am an active participant in Oracle\'s Generative AI service beta programme, it is important to clarify that the following content does not contain any exclusive or proprietary information that is not already publicly documented in Oracle Cloud Infrastructure (OCI) documentation. Oracle makes extensive resources available to its users, and I encourage everyone to review these materials for accurate and up-to-date information regarding the capabilities and usage of the Generative AI service. The intention of this text is to provide an example or context rather than to disclose any confidential or undisclosed features.\\n\\n2024 will be a year that we should see Oracle releasing new services and features that target the generative AI space. One of them is the Oracle [Generative AI](https://www.oracle.com/artificial-intelligence/generative-ai/) (GenAI) service, a cloud native service that was first revealed at Oracle CloudWorld 2023. I was privileged to have attended an introductory session by [Luis Cabrera-Cordon](https://www.linkedin.com/in/luisecabrera), where he demonstrated some of its features using the OCI Console.\\n\\n![Luis demonstrating the GenAI service during a session at Oracle CloudWorld 2023.](./assets/genai-ocw-2023.png)\\n\\nIf you\'ve used any other similar \\"LLM-as-an-API\\" service, then the user interface would be familiar to you. It allows you to enter a prompt and adjust parameters like *temperature* and *maximum tokens*.\\n\\nThe playground on the OCI Console is a great tool to give the feature a quick spin, try things out, and see how it works. However, the real proof of the pudding is using it inside an application, for example, in Oracle APEX. And like the other Oracle AI services, you can call these models using the OCI REST APIs.\\n\\nFrom the API documentation, the key endpoints that us developers would look at using are\\n* *GenerateTextResult*\\n* *SummarizeTextResult*; and\\n* *EmbedTextResult*\\n\\nLike other OCI REST APIs, the required and optional parameters are passed to the endpoints as a JSON payload in the request body, and the results will be returned as a JSON response.\\n\\n```json\\n{\\n    \\"compartmentId\\": \\"ocid...\\",\\n    \\"input\\": \\"The text to be summarised.\\",\\n    \\"serviceMode\\": {\\n        \\"servingType\\": \\"ON_DEMAND\\",\\n        \\"modelId\\": \\"???\\"\\n    },\\n    \\"additionalCommand\\": \\"Additional instructions for the model.\\",\\n    \\"extractiveness\\": \\"AUTO\\",\\n    \\"format\\": \\"PARAGRAPH\\",\\n    \\"isEcho\\": \\"true\\",\\n    \\"length\\": \\"MEDIUM\\",\\n    \\"temperature\\": 1\\n}\\n```\\n\\nThe attributes are pretty well documented. I am particularly interested in the `serviceMode` attribute. The value can be either of the type *OnDemandServingMode* or *DedicatedServingMode*. Both have the common attribute `servingType` that is a string value of either `ON_DEMAND` or `DEDICATED`. The on-demand serving mode additionally requires the attribute `modelId` that you can obtain by querying the endpoint *ListModels* (under *ModelCollection* in the documentation). This returns a list of elements of type *ModelSummary*. I would assume that the required model identifier can be found in the `id` attribute.\\n\\nThe dedicated serving mode has `endpointId` that points to a [*dedicated AI cluster*](https://docs.oracle.com/iaas/Content/generative-ai/create-ai-cluster-fine-tuning.htm) that you can provision to perform fine-tuning. These are (likely) GPU compute clusters that allow us to fine-tune a base LLM over proprietary datasets in order to achieve more relevant and precise results.\\n\\nCalling and processing the GenAI endpoints will be easily accomplished with the Oracle APEX PL/SQL routines. This will probably work:\\n\\n```plsql\\napex_web_service.g_request_headers(1).name := \'Content-Type\';\\napex_web_service.g_request_headers(1).value := \'application/json\';\\n\\nl_response := apex_web_service.make_rest_request(\\n  p_url => \'https://generativeai.aiservice.us-chicago-1.oci.oraclecloud.com/20231130/actions/summarizeText\'\\n  , p_http_method => \'POST\'\\n  , p_body => json_object(\\n      key \'compartmentId\' value \'ocid...\'\\n      , key \'input\' value \'The text to be summarised.\'\\n      , key \'serviceMode\' value json_object(\\n          key \'servingType\' value \'ON_DEMAND\'\\n          , key \'modelId\' value \'???\'\\n      )\\n      , key \'additionalCommand\' value \'Additional instructions for the model.\',\\n      , key \'extractiveness\' value \'AUTO\',\\n      , key \'format\' value \'PARAGRAPH\',\\n      , key \'isEcho\' value \'true\',\\n      , key \'length\' value \'MEDIUM\',\\n      , key \'temperature\' value 1\\n  )\\n  , p_credential_static_id => \'MY_OCI_CREDENTIALS\'\\n);\\n```\\n\\nAnd after that, all we need to do is parse the results, and there you have it, an AI app! :wink:\\n\\n```json\\n{\\n    \\"id\\": \\"ocid...\\",\\n    \\"input\\": \\"The text to be summarised.\\",\\n    \\"modelId\\": \\"???\\",\\n    \\"modelVersion\\": 1.0,\\n    \\"summary\\": \\"The summarised text that we wanted.\\"\\n}\\n```\\n\\nFinally, since these models are from [Cohere](https://cohere.ai), besides prompt engineering and fine-tuning, I am hoping that we can also perform some in-context learning as well. For example:\\n\\n```\\nReview:\\nIt\'s a great camera system!\\n\\nWe are able to save video clips, and it\'s easy to set up.\\n\\nWe have our motion alerts on high sensitivity, and we receive any motion alerts, which is amazing. The camera quality seems to be clear on our end, including any sounds. We can see everything even at night.\\n\\nWe purchased 2 cameras on sale on Cyber Monday. Amazing deal! Highly recommend\\n\\nRating:\\nFive\\n\\nReview:\\nNot happy with these cameras. You start off by having to sign up for a free 30 day subscription trial. If you stay on the trial you don\u2019t actually know how the USB storage option works for recorded videos (if you don\u2019t want to continue with a monthly subscription for a monthly fee). So, I cancelled my free subscription so I could try the usb storage option rather than pay for subscription. This also allows me the flexibility to return it within 30 days. Interesting that you cannot \u201cpause\u201d free trial and reinstate later. Once cancelled you lose your free trial which seems like a gimmick.\\n\\nThe USB (not included) does work but, retrieving the videos is very slow and trying to access the cameras in \u201clive\u201d view sometimes does not work.\\n\\nThe daytime quality of cameras is ok but the night time quality is poor. Also, they are not weather resistant. I have mine protected under roofline but most mornings the cameras are blurry because of condensation despite having them in a somewhat protected area. We did also put one on a fence as they shown on the website but quickly had to remove it because of condensation build up (when it wasn\u2019t even raining).\\nWill be returning. Not happy at all.\\n\\nRating:\\nOne\\n\\nReview:\\nIt\'s a decent camera system without a live view option for multiple cameras. Important: this kit includes the Sync Module already, I didn\'t see that listed anywhere so I bought separately the sync module and ended up with 2 of them.\\n\\nPros:\\n- Able to communicate with people with 2-way communication.\\n- Able to save video clips.\\n- Easy to set up\\n\\nCons:\\n- No constant live view. It\'s not like a traditional camera security system where you look at a monitor and see multiple cameras with a live view. You need to click on each camera and it will (slowly) load the view at that camera.\\n- I haven\'t received any motion alerts. I have the sensitivity up high, I selected zones, I did everything they said to do and still no alerts for motion.\\n- The camera quality is not like it is shown on the photos. The photos on Amazon are taken with a high end camera and made to look like they\'re the view of these cameras. It is nothing like the photos shown. Picture a digital camera from 2010. That is the image you\'ll get.\\n\\nRating returned as a json object with one attribute named rating:\\n```\\n\\nThat then returns a result like this:\\n\\n```json\\n{\\n    \\"rating\\": 3\\n}\\n```\\n\\n> **Acknowledgement**\\n>\\n> The text in the examples are actual reviews taken from Amazon.\\n\\nAt the moment, it seems to me that creating applications using Large Language Models (LLMs) remains an expensive endeavor due to reliance on GPUs and ongoing supply chain issues, there are promising signs of change with growing interest in alternatives like Intel ARC and AMD Radeon. This diversification of hardware options offers hope for more accessible and affordable fine-tuning opportunities in the near future. Nevertheless, let us not forget to keep a close eye on the ecological impact of these advancements.\\n\\n## Oracle AI Vector Search\\n\\nUnlike the GenAI service, not a lot has been published on this upcoming Oracle Database 23c feature. And I might stress \\"upcoming\\". When news first broke (also) at Oracle CloudWorld 2023, it wasn\'t clear if the vector data type and its powerful search features were part of the 23c general availability (GA) that was announced at the same time. If you missed it, here\'s the [press release](https://www.oracle.com/news/announcement/ocw-integrated-vector-database-augments-generative-ai-2023-09-19/).\\n\\n## Embeddings and Knowledge Graphs\\n\\nIn the absence of publicly available content on the AI vector search database feature, let\'s focus instead on an important concept, *embeddings*. Embeddings in LLMs are a vector of numbers that somewhat represent meaning and context in a body of text. These vectors are derived from the parameters or the weights of a LLM used to generate these embeddings. They can be used to encode and decode both inputs and outputs of your inferencing task. These vectors are what powers \\"semantic\\" search and Retrieval Augment Generation (RAG), and based on the press release, we can expect to perform these tasks when the AI vector search feature becomes generally available.\\n\\nWhile I await for AI vector search to be generally available, there are a few frameworks and alternate vector database solutions that I am currently looking at, including most recently, the newly release [OCI Cache with Redis](https://www.oracle.com/cloud/redis/) service that didn\'t work out as I had expected. :frowning: More on that in a 2024 blog post.\\n\\nOne other area that I am looking to spend some time on in 2024 is on the subject of *knowledge graph embeddings*. I first learned about the Semantic Web and the power of ontologies, knowledge representation, and linked data at a 2006 bioinformatics conference in Boston, where I crashed on a special interest group meeting at the [MIT Computer Science & Artificial Intelligence Laboratory](https://www.csail.mit.edu/). Aside from seeing the back of [Sir Tim Berners-Lee\'s](https://wikipedia.org/wiki/Tim_Berners-Lee), I was truly inspired by the brains around this subject, and remembered listening to a presentation on what would later become [Freebase](https://wikipedia.org/wiki/Freebase_(database)).\\n\\nIn my Kscope17 presentation, [*Enriching Oracle APEX Applications with Semantics*](https://www.insum.ca/portfolio/enriching-oracle-apex-applications-semantics/), I described simple changes to how data is presented so that software agents may access and consume data meaningfully. As we continue to produce yottabytes of data, we should not neglect the importance of curating, documenting, and enriching (and securing) data. Data without meaning is going to be garbage. And the essence of embedding meaning in data is ever increasingly important in this new era of AI. I\'m placing my bets that the development and use of knowledge graphs and frontier models is the next disrupter in the AI space. If anyone\'s looking for a part-time PhD candidate, can I please volunteer? :raising_hand::wink:\\n\\n## Summary\\n\\nIt\'s been an accelerating year for AI developments and I don\'t think anyone\'s going to see the end of it in 2024. For me personally, I continue to be conflicted with the current developments and hype around generative AI. While it presents a lot of promise, it shouldn\'t dwarf the usefulness of traditional ML approaches that I feel still provides a lot of immediate utility. It is also seemingly a technology for those who can afford it, and it has potential to take jobs away from people who do what some might call mundane.\\n\\nTake the banner art that I use for my previous few blogs for example. In case it wasn\'t obvious enough, I have relied on generating the images using the [Stable Diffusion XL](https://huggingface.co/papers/2307.01952) model, when I used to scour online repositories of royalty-free photos. For more professional websites, I\'m sure they source these from companies selling stock art from contributors, providing a source of income for people passionate about photography or graphic art.\\n\\nAI is great, and I\'m passionate about what it can do for civilisation. But in 2024, and in the years following, I do hope that we can all continue to remember that technology should be used to improve the lives of all, and not some.\\n\\nWith that, I\'ll end this blog post and 2023. For those whom I have had the privilege of working with this year, a huge \\"Thank you\\". It\'s been a great year, and I look forward to our continued collaboration to make this world a better place for you, me, and those whom we serve.\\n\\n![Wefie at OCW23.](./assets/happy-new-year.jpg)\\n\\nHappy New Year Everyone!<br/>\\n\u795d\u5927\u5bb6\u65b0\u5e74\u5feb\u4e50\uff01<br/>\\nBonne Ann\xe9e \xe0 Tous!<br/>\\n\u3042\u3051\u307e\u3057\u3066\u304a\u3081\u3067\u3068\u3046\u7686\u3055\u3093\uff01<br/>\\n\\n> **Photo Credits**\\n>\\n> The photo used in the banner image was taken by me quite recently. This is a winter sunrise with the mountains in the backdrop. Moving from big cities into a remote community in the far North has been humbling, and a reminder that I am a very small piece in this humongous puzzle."},{"id":"/generate-art-using-diffusion-models-and-nfc-tags","metadata":{"permalink":"/content/generate-art-using-diffusion-models-and-nfc-tags","source":"@site/posts/generate-art-using-diffusion-models-and-nfc-tags/index.md","title":"Generate Art Using Latent Diffusion Models and NFC Tags","description":"Learn how to host the Stable  for performing text-to-image generation tasks on the Oracle Cloud Infrastructure (OCI). Then, build an Oracle APEX application that uses NFC tags to receive text prompts and use it to generate an image.","date":"2023-11-19T21:00:00.000Z","formattedDate":"November 19, 2023","tags":[{"label":"latent diffusion model","permalink":"/content/tags/latent-diffusion-model"},{"label":"ldm","permalink":"/content/tags/ldm"},{"label":"machine learning","permalink":"/content/tags/machine-learning"},{"label":"oci","permalink":"/content/tags/oci"},{"label":"python","permalink":"/content/tags/python"},{"label":"conda","permalink":"/content/tags/conda"},{"label":"oracle apex","permalink":"/content/tags/oracle-apex"},{"label":"generative ai","permalink":"/content/tags/generative-ai"},{"label":"ai-generated art","permalink":"/content/tags/ai-generated-art"},{"label":"mlops","permalink":"/content/tags/mlops"},{"label":"nfc","permalink":"/content/tags/nfc"},{"label":"text-to-image generation","permalink":"/content/tags/text-to-image-generation"}],"readingTime":10.62,"hasTruncateMarker":true,"authors":[{"name":"Adrian Png","title":"Senior Cloud Solutions Architect @ Insum","url":"https://github.com/fuzziebrain","imageURL":"https://github.com/fuzziebrain.png","key":"fuzziebrain"}],"frontMatter":{"title":"Generate Art Using Latent Diffusion Models and NFC Tags","description":"Learn how to host the Stable  for performing text-to-image generation tasks on the Oracle Cloud Infrastructure (OCI). Then, build an Oracle APEX application that uses NFC tags to receive text prompts and use it to generate an image.","image":"./assets/image20231119221347.png","tags":["latent diffusion model","ldm","machine learning","oci","python","conda","oracle apex","generative ai","ai-generated art","mlops","nfc","text-to-image generation"],"categories":["Technology"],"authors":["fuzziebrain"],"date":"2023-11-19T21:00:00.000Z"},"prevItem":{"title":"A Yearful of AI But What\'s Next?","permalink":"/content/a-yearful-of-ai-but-whats-next"},"nextItem":{"title":"Deploy and Use Fine-Tuned LLMs in Oracle APEX","permalink":"/content/deploy-and-use-fine-tuned-llms-in-oracle-apex"}},"content":"![Photo of a bird perched on a tree, generated using a Stability Diffusion model.](./assets/image20231119221347.png)\\n\\nIn my previous [post](/content/deploy-and-use-fine-tuned-llms-in-oracle-apex), you read about how I performed fine-tuning and deployed a large language model (LLM) on the [Oracle Cloud Infrastructure](https://oracle.com/cloud) (OCI) [Data Science](https://www.oracle.com/artificial-intelligence/data-science/) service. Here, I was hoping to do the same with the [Stable Diffusion XL](https://stability.ai/stable-diffusion), a *Latent Diffusion Model* (LDM), but unfortunately, the platform currently does not support a [*Hugging Face*](https://huggingface.co) pipeline using this model. I found an alternative, and you will read about it later in this article.\\n\\nTo demonstrate its utility, I create a simple [Oracle APEX](https://apex.oracle.com) application that reads NFC tags using the **experimental** [*Web NFC*](https://w3c.github.io/web-nfc/) API, and then displays an image generated by the deployed machine learning (ML) model.\\n\\n\x3c!-- truncate --\x3e\\n\\nI called this toy application \\"Amiibo Art\\". The [Amiibo](https://www.nintendo.com/amiibo/) was created by *Nintendo* to bridge games with the real world using toys, figurines, and collectibles that embedded a radio-frequency tag. I wanted this APEX application to interact with the real world, and then generate a collectible. If you missed it, here\'s the screen recording of the application in action:\\n\\n<youtube youTubeId=\\"aozXI0NWlKs\\" />\\n\\n## Deploying the Model\\n\\nAs mentioned earlier, the OCI Data Science platform did not provide me an easy way to deploy the Stable Diffusion XL model. I found an open-source project called [*Pinferenia*](https://pinferencia.underneathall.app/) that I could use to deploy a Hugging Face pipeline to perform the text-to-image generation task. The code below can be executed and deployed on an OCI Compute, and a GPU, while not required, is desired.\\n\\n### Setup the Server Environment\\n\\nThe first task involved setting up a new [Conda](https://conda.io) environment with the required dependencies using the [YAML](https://yaml.org/) file below:\\n\\n```yaml\\nchannels:\\n    - conda-forge\\n    - pytorch\\n    - nvidia\\n\\ndependencies:\\n    - python=3.9\\n    - pytorch\\n    - pytorch-cuda\\n    - transformers\\n    - pillow\\n    - numpy\\n    - safetensors\\n    - diffusers\\n    - accelerate\\n    - pip\\n    - pip:\\n        - invisible_watermark\\n        - oracle-ads\\n        - pinferencia\\n```\\n\\nIf you are new to Conda, write the YAML contents into a file name `amiibo-art.yaml`, install the Conda software, and then execute the command:\\n\\n```bash\\nconda env create -n amiibo-art -f amiibo-art.yaml\\n```\\n\\nThen, as instructed, activate the environment before running the subsequent code and commands. To activate the newly created environment, execute the command:\\n\\n```bash\\nconda activate amiibo-art\\n```\\n\\n### Write the Server-Side Code\\n\\nCreate the Python file `app.py` with the code provided below. Follow the embedded comments to understand what it is doing.\\n\\n```python\\nfrom diffusers import DiffusionPipeline\\nfrom pinferencia import Server\\nimport torch\\nimport datetime\\nimport os\\nimport oci.object_storage\\n\\ndef get_current_timestamp():\\n    \\"\\"\\"\\n    Return a timestamp for file naming purposes.\\n    \\"\\"\\"\\n    now = datetime.datetime.now()\\n    return now.strftime(\'%Y%m%d%H%M%S\')\\n\\ndef save_object(bucketName, objectName, content):\\n    \\"\\"\\"\\n    Save the generated image in an Object Storage bucket for retrieval later.\\n    \\"\\"\\"\\n    mimeType = \\"image/png\\"\\n\\n    object = client.put_object(\\n        namespace_name = namespace\\n        , bucket_name = bucketName\\n        , object_name = objectName\\n        , content_type = mimeType\\n        , put_object_body = content\\n    )\\n\\ndef generate(prompt):\\n    \\"\\"\\"\\n    This function is registered to the Pinferencia as the model \\"genai\\". It\\n    generates the image using the Hugging Face pipeline, save the output as a\\n    file, then push it to the Object Storage bucket using the function\\n    \\"save_object\\".\\n    \\"\\"\\"\\n    bucketName = \\"amiibo\\"\\n    objectName = f\\"image{get_current_timestamp()}.png\\"\\n    image = pipe(prompt=prompt, width=1280, height=720).images[0]\\n    image.save(objectName)\\n    content = open(objectName, \\"rb\\")\\n    save_object(bucketName=bucketName, objectName=objectName, content=content)\\n    os.remove(objectName)\\n    return {\\"bucket_name\\": bucketName, \\"object_name\\": objectName}\\n\\n\\"\\"\\"\\nWe will use an instance principal to sign the request. Be sure to:\\n1. Create a dynamic group that includes the Compute instance\'s OCID.\\n2. Create an IAM policy that has the necessary statements to allow the dynamic\\n   group to write to the bucket.\\n\\"\\"\\"\\nsigner = oci.auth.signers.InstancePrincipalsSecurityTokenSigner()\\nclient = oci.object_storage.ObjectStorageClient(config={\\"region\\":\\"***\\",\\n    \\"tenancy\\":signer.tenancy_id}, signer=signer)\\nnamespace = client.get_namespace().data\\n\\n\\"\\"\\"\\nCreate a Hugging Face pipeline using the Stable Diffusion XL model hosted on the\\nplatform. We will use the quantized model.\\n\\"\\"\\"\\npipe = DiffusionPipeline.from_pretrained(\\"stabilityai/stable-diffusion-xl-base-1.0\\",\\n        use_safetensors=True, torch_dtype=torch.float16, variant=\\"fp16\\")\\n\\n\\"\\"\\"\\nUse a Nvidia GPU, if available.\\n\\"\\"\\"\\npipe.to(\\"cuda\\" if torch.cuda.is_available() else \\"cpu\\")\\n\\n\\"\\"\\"\\nInstantiate the Pinferencia server.\\n\\"\\"\\"\\nservice = Server()\\n\\n\\"\\"\\"\\nRegister the \\"generate\\" function as the model \\"genai\\".\\n\\"\\"\\"\\nservice.register(model_name=\\"genai\\", model=generate,\\n    metadata={\\"platform\\":\\"Linux\\", \\"device\\": \\"GPU\\"})\\n```\\n\\n### Run the Server\\n\\nThe Pinferencia package includes executables for running a server instance. For simplicity, I ran the server in the background using [tmux](https://github.com/tmux/tmux) (think *screen*), and executing the command:\\n\\n```bash\\npinfer --backend-host 0.0.0.0 --mode backend app:service --reload\\n```\\n\\n> **NOTE**\\n>\\n> By default, the server only binds and listens to the local interface, i.e., `localhost` or IP address `127.0.0.1`. Hence, the command includes the parameter `--backend-host 0.0.0.0`.\\n\\nThe deployed model is accessible at an endpoint URL with a format that looks like this:\\n\\n```\\nhttp://{{HOST_IP}}:8000/v1/models/{{MODEL_NAME}}/predict\\n```\\n\\nFor example:\\n```\\nhttp://10.30.4.23:8000/v1/models/genai/predict\\n```\\n\\n### Deploy an OCI API Gateway\\n\\nAs I am calling this web service from an Oracle Autonomous Database that is **not** using a private endpoint, I needed to proxy the service over HTTPS. The fastest approach to achieve this was to create an OCI API Gateway and route as show below.\\n\\n![OCI API Gateway route for the Pinferencia web service endpoint.](./assets/apigw-route.png)\\n\\nThis serves the web service publicly using an endpoint address that looks like this:\\n```\\nhttps://{{UNIQUE_IDENTIFIER}}.apigateway.eu-frankfurt-1.oci.customer-oci.com/v1/genai/texttoimage\\n```\\n\\n### Testing the Model\\n\\nPinferencia provides a [Swagger](https://swagger.io/) UI that documents all the endpoints it is serving. You can use it to try out the models served. In this diagram, I am testing the image generation model.\\n\\n![Swagger UI showing all the available endpoints and a way to try them.](./assets/swagger-ui.png)\\n\\nThe input is a JSON object containing the prompt:\\n\\n```json\\n{\\n    \\"id\\": \\"string\\",\\n    \\"parameters\\": {},\\n    \\"data\\": \\"A cartoon image of a cat chasing a mouse.\\"\\n}\\n```\\n\\nAfter the image has been placed in the bucket, it returns a HTTP response containing a JSON payload with the `bucket_name` and `object_name` that we can use to retrieve the object.\\n\\n```json\\n{\\n    \\"id\\": \\"string\\",\\n    \\"model_name\\": \\"genai\\",\\n    \\"model_version\\": \\"default\\",\\n    \\"data\\": {\\n        \\"bucket_name\\": \\"amiibo\\",\\n        \\"object_name\\": \\"image20231113223047.png\\"\\n    }\\n}\\n```\\n\\n## The NFC-Enabled Web Application\\n\\nI created an Oracle APEX application with the following features:\\n\\n1. **Page 1.** On the home page, the user scans a NFC tag containing a single text record that stores the prompt to submit to the deployed model.\\n1. An administrative tool that allows an administrator to:\\n    1. **Page 2**. List tags in the inventory with buttons to manage or write prompts to a tag.\\n    1. **Page 3**. Register and manage a tag.\\n    1. **Page 4**. Write prompts to a tag.\\n\\nTo keep this blog post readable, I will focus on two key aspects that allow the user to scan and generate an image.\\n\\nFirst, the *Ajax Callback* process that is called when an NFC tag containing a valid prompt is scanned successfully. Again, follow the embedded comments to understand what this code does.\\n\\n```sql\\ndeclare\\n    /**\\n     * The collection name used for temporary storing the images.\\n     */\\n    c_collection_name apex_collections.collection_name%type := \'AMIIBO_ART\';\\n\\n    /**\\n     * The JavaScript event listener will read the tag and supply prompt using\\n     * the x01 variable.\\n     */\\n    l_prompt apex_application.g_x01%type := apex_application.g_x01;\\n\\n    /**\\n     * Utility variables for reading and parsing the results.\\n     */\\n    l_response clob;\\n    l_content blob;\\n    l_json_response apex_json.t_values;\\nbegin\\n    /**\\n     * Call the deployed model\'s endpoint through the API gateway. The payload\\n     * contains the prompt read from the NFC tag.\\n     */\\n    l_response := apex_web_service.make_rest_request(\\n        p_url => \'https://{{UNIQUE_IDENTIFIER}}.apigateway.eu-frankfurt-1.oci.customer-oci.com/v1/genai/texttoimage\'\\n        , p_http_method => \'POST\'\\n        , p_body => json_object(\\n            key \'data\' value l_prompt\\n        )\\n    );\\n\\n    if apex_web_service.g_status_code = 200 then\\n        /**\\n         * If all goes well, parse the HTTP response\'s JSON payload to read the\\n         * generated image\'s bucket and object name.\\n         */\\n        apex_json.parse(l_json_response, l_response);\\n\\n        /**\\n         * Retrieve the object using the DBMS_CLOUD.GET_OBJECT function. Be sure\\n         * to also create an IAM user, set up the OCI API credentials, and\\n         * create the necessary policy to allow the IAM user to access the\\n         * object.\\n         */\\n        l_content := dbms_cloud.get_object(\\n            credential_name => \'MY_API_CREDENTIALS\'\\n            , object_uri => \'https://objectstorage.eu-frankfurt-1.oraclecloud.com/n/***/b/amiibo/o/\'\\n                ||  apex_json.get_varchar2(\\n                        p_path => \'data.object_name\'\\n                        , p_values => l_json_response\\n                    )\\n        );\\n\\n        /**\\n         * Not the best code here, but I am simply pushing the image into the\\n         * an APEX collection.\\n         */\\n        apex_collection.create_or_truncate_collection(c_collection_name);\\n        apex_collection.add_member(\\n            p_collection_name => c_collection_name\\n            , p_c001 => l_prompt\\n            , p_blob001 => l_content\\n        );\\n\\n        sys.htp.p(\'{\\"message\\":\\"success\\", \\"prompt\\": \\"\'||l_prompt||\'\\"}\');\\n    else\\n        sys.htp.p(\'{\\"message\\":\\"failed\\"}\');\\n    end if;\\nend;\\n```\\n\\nThis can be a long running process, and better strategies should be considered if larger images, the non-quantized model is used, or the pipeline is not running on a server with a GPU with sufficient memory. With a single Nvidia A10, I was able to get images to be generated in less than 20 seconds. While possible, using a CPU took more than 12 minutes in one attempt I made.\\n\\n![Elapsed time using only CPU resources for the pipeline.](./assets/elapsed-time-with-cpu-only.png)\\n\\nOn page 1, an Oracle APEX *Card Region* is added to render the image using the source query:\\n\\n```sql\\nselect\\n    seq_id\\n    , \'My Amiibo Art\' as title\\n    , c001 as prompt\\n    , blob001 as content_blob\\nfrom apex_collections\\nwhere collection_name = \'AMIIBO_ART\'\\n```\\n\\nAnd a button that calls a dynamic action that executes the following JavaScript code:\\n\\n```javascript\\napex.message.alert(\\n    \\"Close this alert, and then scan your Amiibo.\\",\\n    null,\\n    { style: \\"information\\" }\\n);\\n\\ntry {\\n    /**\\n     * Instantiate the NFC reader object when the scan button is clicked.\\n     */\\n    const ndef = new NDEFReader();\\n\\n    /**\\n     * Begin scanning.\\n     */\\n    ndef.scan();\\n\\n    /**\\n     * Attach an event handler when there is an error reading the tag.\\n     */\\n    ndef.addEventListener(\\n        \\"readingerror\\",\\n        () => {\\n            apex.message.alert(\\n                \\"Failed to read data from the Amiibo. Please try again.\\",\\n                null,\\n                { style: \\"warning\\" }\\n            );\\n        }\\n    );\\n\\n    /**\\n     * Attach an event handler to handle the incoming data when a tag is scanned\\n     * successfully. The data contains the serial number and its message\\n     * payload.\\n     */\\n    ndef.addEventListener(\\n        \\"reading\\",\\n        ({ message, serialNumber }) => {\\n            if(message.records.length > 0) {\\n                /**\\n                 * Get the first record.\\n                 */\\n                record = message.records[0];\\n\\n                /**\\n                 * If it is of the expected record type, then process it.\\n                 */\\n                if(record.recordType === \\"text\\") {\\n                    /**\\n                     * First, instantiate a text decoder.\\n                     */\\n                    const decoder = new TextDecoder();\\n\\n                    let spinner = apex.util.showSpinner($(\\"body\\"));\\n\\n                    /**\\n                     * Call the Ajax callback with the text contained in the\\n                     * tag\'s message payload.\\n                     */\\n                    apex.server.process(\\n                        \\"TEXT_TO_IMAGE\\",\\n                        {\\n                            x01: decoder.decode(record.data)\\n                        },\\n                        {\\n                            success: function( data )  {\\n                                /**\\n                                 * TODO:\\n                                 * A page submit here because the Card region\\n                                 * was not refreshing with the new image. Needs\\n                                 * improvement.\\n                                 */\\n                                apex.page.submit();\\n                            },\\n                            error: function( jqXHR, textStatus, errorThrown ) {\\n                                // handle error\\n                            }\\n                        }\\n                    );\\n                }\\n            }\\n        }\\n    );\\n} catch (error) {\\n    apex.message.alert(`General error: {error}`, null, { style: \\"warning\\" });\\n}\\n```\\n\\nAfter the tag is scan, on the backend, you can see Pinferencia hard at work, generating images typically under 20 seconds.\\n\\n<youtube youTubeId=\\"uheDoKlAFdU\\" />\\n\\nAnd if all goes well, the page should be refreshed with the image generated by the model.\\n\\n## Writing to NFC Tags\\n\\nAt the time of writing, the Web NFC API allows developers to read, write, and set a tag to \\"read-only\\" mode. In the application, I had created functionality for an administrator to maintain a collection of NFC tags, and to write prompts to them. On page 4, a button is attached to a dynamic action that runs JavaScript code to write to the tag. A sample of the code is as follows:\\n\\n```javascript\\ntry {\\n    ...\\n\\n    ndef.addEventListener(\\n        \\"reading\\",\\n        ({ message, serialNumber }) => {\\n            if(serialNumber === apex.item(\\"P4_TAG_ID\\").getValue()) {\\n                ndef.write(apex.item(\\"P4_PROMPT\\").getValue());\\n                apex.message.alert(\\n                    \\"Prompt written to tag.\\",\\n                    null,\\n                    {style: \\"information\\"}\\n                );\\n            } else {\\n                apex.message.alert(\\n                    \\"Failed to write to tag. Tag ID mismatch.\\",\\n                    null,\\n                    {style: \\"danger\\"}\\n                );\\n            }\\n        }\\n    );\\n} catch (error) {\\n    apex.message.alert(`General error: {error}`, null, { style: \\"warning\\" });\\n}\\n```\\n\\nEasy eh?\\n\\n## Summary\\n\\nI worked on this fun project with two objectives in mind:\\n\\n1. How can I work with NFC tags in an Oracle APEX application.\\n1. How can I host a Stable Diffusion model in the OCI.\\n\\nDespite the limitations, I have learned quite a bit during this adventure.\\n\\n1. Support for NFC tags is currently limited. Only Google Chrome on an Android device supports the experimental Web NFC API. And even so, we seem to be limited to reading, writing, and setting a tag to \\"read-only\\" mode. Hopefully, there will be future support password-lock a tag as well.\\n1. Not all Hugging Face pipelines are supported and can be deployed to the OCI Data Science platform.\\n1. There are other software platforms that can be used to deploy ML models, for example, Pinferencia. I don\'t consider it ready for production use, but it has been very easy to set up, run, and host a ML model.\\n\\nHope you enjoyed reading about this as much as I had working and writing on the project. If you ar interested to discuss possible solutions to your unique business challenge, please reach out to me using this [form](https://cal.com/fuzziebrain/iaas-consult).\\n\\n> **Credits**\\n>\\n> * Banner image generated using Stability AI\'s [Stable Diffusion](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0) model."},{"id":"/deploy-and-use-fine-tuned-llms-in-oracle-apex","metadata":{"permalink":"/content/deploy-and-use-fine-tuned-llms-in-oracle-apex","source":"@site/posts/deploy-and-use-fine-tuned-llms-in-oracle-apex/index.md","title":"Deploy and Use Fine-Tuned LLMs in Oracle APEX","description":"Learn how Oracle Cloud Infrastructure (OCI) resources can be used to fine-tune and deploy open source large language models (LLMs). Then, build an Oracle APEX application that users may use to submit feedback and have their reviews automatically scored.","date":"2023-11-05T01:35:00.000Z","formattedDate":"November 5, 2023","tags":[{"label":"large language models","permalink":"/content/tags/large-language-models"},{"label":"llms","permalink":"/content/tags/llms"},{"label":"machine learning","permalink":"/content/tags/machine-learning"},{"label":"oci","permalink":"/content/tags/oci"},{"label":"data science","permalink":"/content/tags/data-science"},{"label":"python","permalink":"/content/tags/python"},{"label":"jupyter","permalink":"/content/tags/jupyter"},{"label":"conda","permalink":"/content/tags/conda"},{"label":"oracle apex","permalink":"/content/tags/oracle-apex"}],"readingTime":9.955,"hasTruncateMarker":true,"authors":[{"name":"Adrian Png","title":"Senior Cloud Solutions Architect @ Insum","url":"https://github.com/fuzziebrain","imageURL":"https://github.com/fuzziebrain.png","key":"fuzziebrain"}],"frontMatter":{"title":"Deploy and Use Fine-Tuned LLMs in Oracle APEX","description":"Learn how Oracle Cloud Infrastructure (OCI) resources can be used to fine-tune and deploy open source large language models (LLMs). Then, build an Oracle APEX application that users may use to submit feedback and have their reviews automatically scored.","image":"./assets/llms-for-analyzing-customer-reviews.png","tags":["large language models","llms","machine learning","oci","data science","python","jupyter","conda","oracle apex"],"categories":["Technology"],"authors":["fuzziebrain"],"date":"2023-11-05T01:35:00.000Z"},"prevItem":{"title":"Generate Art Using Latent Diffusion Models and NFC Tags","permalink":"/content/generate-art-using-diffusion-models-and-nfc-tags"},"nextItem":{"title":"Predict the Rugby World Cup 2023 Winner","permalink":"/content/predict-the-rugby-world-cup-2023-winner"}},"content":"![LLMs for analyzing customer reviews.](./assets/llms-for-analyzing-customer-reviews.png)\\n\\nIn my previous [post](/content/predict-the-rugby-world-cup-2023-winner), I wrote about how a [scikit-learn](https://scikit-learn.org) machine learning (ML) could be trained and deployed on the [Oracle Cloud Infrastructure](https://oracle.com/cloud) (OCI) [Data Science](https://www.oracle.com/artificial-intelligence/data-science/) service. The model is deployed on the service\'s managed infrastructure, allowing developers to simply call a HTTP endpoint to perform ML model inference on the submitted data. In my latest adventure, I built an [Oracle APEX](https://apex.oracle.com) application that takes product reviews and then automatically ranks the review using a fine-tuned large language model (LLM) available through [Hugging Face](https://huggingface.co).\\n\\n\x3c!-- truncate --\x3e\\n\\nWhile OpenAI\'s [ChatGPT](https://openai.com/chatgpt) has largely dominated the news headlines when it comes to *Generative AI*, there is a plethora of LLMs out there. Many of them are open sourced and readily available through hubs like Hugging Face. There are a bunch of tutorials that introduce the platform including the available datasets, models, Python, and JavaScript libraries. In this post, I will use the example from their tutorial on fine-tuning a pre-trained model.\\n\\nThe goal is to use the [Yelp review](https://huggingface.co/datasets/yelp_review_full) dataset to fine-tune [BERT](https://arxiv.org/abs/1810.04805), specifically, the [distilbert-base-uncased](https://huggingface.co/distilbert-base-uncased) model, to assign a rank that describes the sentiment expressed in a body of text.\\n\\n## Provisioning the Tools\\n\\nMachine learning tasks can be performed on a CPU, but more often than not, we have a huge performance gained by using a graphical processing unit (GPU). For most people, that\'s going to be a Nvidia chip. On the OCI, we currently have a choice of the P100, V100, and more recently, A10 and A100. I went with an A10 as it was newer, cheaper than the A100, and has adequate amount of GPU memory. And, more importantly, the tenancy that I was on had sufficient number of GPUs assigned.\\n\\nYou can check your tenancy\'s service limits to see if you can provision a GPU instance. The resource that you will need is either `gpu-a10-count` if you wish to run the training on an OCI Compute instance, or `ds-gpu-a10-count` for an OCI Data Science notebook. These limits are assigned differently for Compute instance and Data Science notebooks. If your tenancy is either a trial, or a *PAYG* (Pay-as-you-go) account, then these limits are likely `0`. For *PAYG*, you could try making a service limit request, but for trial accounts, it\'s an uphill battle. And for accounts with a *Universal Credits*, you might find `16` is the number for `gpu-a10-count`. Make a service limit request for `ds-gpu-a10-count` if you wish to run the training in a GPU-powered notebook session.\\n\\nI initially did not have access to GPU resources on OCI Data Science, so I had provisioned a Compute with a GPU, and then all I needed was to install [Miniconda](https://conda.io/miniconda), and I was good to go!\\n\\n## Fine-tune the LLM\\n\\nIn OCI Data Science, you can probably use the [Conda](https://conda.io) environments `generalml_p38_cpu_v1` or `pytorch20_p39_gpu_v2` (if you have a GPU instance provisioned) for fine-tuning the model. However, for deployment, a GPU instance probably isn\'t necessary as the time needed to complete the inference is relatively much shorter.\\n\\nIf you do use the Oracle-provided Conda environments, please note that you might need to add any missing dependencies that are specific for working with Hugging Face models and datasets. For me, I opted to create a custom Conda environment as I had started out training the model on a GPU-powered compute instance. This environment was published and later used for the model deployment. Below is the Conda environment file I used.\\n\\n```yaml\\nchannels:\\n    - conda-forge\\n\\ndependencies:\\n    - python>=3.8\\n    - transformers\\n    - evaluate\\n    - scikit-learn\\n    - accelerate\\n    - pillow\\n    - numpy\\n    - pip\\n    - pip:\\n        - torch==2.1.0\\n        - oracle-ads[data,notebook]\\n```\\n\\n> **NOTE**\\n>\\n> If you\'re running this in an OCI Data Science notebook, then the `jupyterlab` and `ipywidgets` dependencies can be omitted as they are automatically added when you create a conda environment using the `odsc` command.\\n\\n```python\\nfrom datasets import load_dataset\\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\\nimport numpy as np\\nimport evaluate\\nimport torch\\n\\ndef tokenize_function(examples):\\n    return tokenizer(examples[\\"text\\"], padding=\\"max_length\\", truncation=True)\\n\\ndef compute_metrics(eval_pred):\\n    logits, labels = eval_pred\\n    predictions = np.argmax(logits, axis=-1)\\n    return metric.compute(predictions=predictions, references=labels)\\n\\ndevice = torch.device(\\"cuda\\") if torch.cuda.is_available() else torch.device(\\"cpu\\")\\n\\nmodel_name = \\"distilbert-base-uncased\\"\\ndataset = load_dataset(\\"yelp_review_full\\")\\ntokenizer = AutoTokenizer.from_pretrained(model_name)\\nmetric = evaluate.load(\\"accuracy\\")\\n\\ntokenized_datasets = dataset.map(tokenize_function, batched=True)\\n\\nsmall_train_dataset = tokenized_datasets[\\"train\\"].shuffle(seed=42).select(range(1000))\\nsmall_eval_dataset = tokenized_datasets[\\"test\\"].shuffle(seed=42).select(range(1000))\\n\\nmodel = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=5)\\nmodel.to(device)\\ntraining_args = TrainingArguments(output_dir=\\"test_trainer\\",\\n                                  evaluation_strategy=\\"epoch\\",\\n                                  num_train_epochs=20,\\n                                  fp16=True)\\n\\ntrainer = Trainer(\\n    model=model,\\n    args=training_args,\\n    train_dataset=small_train_dataset,\\n    eval_dataset=small_eval_dataset,\\n    compute_metrics=compute_metrics,\\n)\\n\\ntrainer.train()\\n```\\n\\nI wanted to use this model in a different tenancy, and so, after fine-tuning the model in the Compute instance, I exported the model to be deployed in an OCI Data Science project. To save the model, I ran this function:\\n\\n```python\\nmodel.save_pretrained(\\"./mymodel\\")\\n```\\n\\nThe model artifacts are saved in the local directory `mymodel`. It contains the files `config.json` and `pytorch_model.bin`. A tarball was then created, and transferred to the OCI Data Science project where I would deploy the model.\\n\\nThe `distilbert-base-uncased` model is relatively small with only 67 million parameters, and thus required only about 4 GB of GPU memory for the training. Just for fun, I attempted to do the training on `bert-large-uncased` that is a large model with 336 million parameters. Here\'s how much memory it took.\\n\\n![GPU status reported by nvidia-smi command.](./assets/gpu-status.png)\\n\\nUnfortunately, the accuracy wasn\'t as impressive even with more parameters. More work would be required before I could eventually use larger LLMs such as this.\\n\\n## Deploy the Fine-tuned LLM\\n\\nThe Oracle [Accelerated Data Science](https://accelerated-data-science.readthedocs.io) Python libraries provides the data scientists and machine learning engineers as suite of utility packages to load datasets from various sources on the OCI, and other repositories, perform machine learning and MLOps tasks, and many more. Since version 2.8.2, the ADS provides support for Hugging Face models using the class *HuggingFacePipelineModel*. Hugging Face pipelines are convenient facade for developers to use models for inference.\\n\\nIf, like me, you had performed the fine-tuning outside of the OCI Data Science notebook that you will use to prepare and deploy the model, then first upload the tarball containing the model\'s artifacts, and then extract the files. You can then use the following script to create a *Pipeline* and test out some predictions. If not, simply create the pipeline using the trained model.\\n\\n```python\\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\\n\\ntokenizer = AutoTokenizer.from_pretrained(\\"distilbert-base-uncased\\")\\nmodel = AutoModelForSequenceClassification.from_pretrained(\\"./mymodel\\")\\n\\npipe = pipeline(task=\\"sentiment-analysis\\", model=model, tokenizer=tokenizer)\\n\\nreview = \\"\\"\\"\\nI recently purchased Tim Horton\'s Nespresso pods for use at home, and I\\nwas impressed by the consistent quality of the coffee. The pods are easy\\nto store and use, making them a convenient option for busy individuals or\\nsmall households. However, the pods can be expensive, especially if you\\nare a frequent coffee drinker, and may not be available in all locations,\\nlimiting your ability to enjoy Tim Horton\'s espresso on-the-go. Overall, I\\nwould recommend Tim Horton\'s Nespresso pods as a convenient way to enjoy\\nhigh-quality espresso at home, but keep in mind the cost and availability\\nlimitations.\\n\\"\\"\\"\\n\\npipe(review)\\n```\\n\\nHere\'s a sample of the results:\\n\\n```python\\n[{\'label\': \'LABEL_3\', \'score\': 0.9998452663421631}]\\n```\\n\\nThe next task involves using ADS to prepare, save, and deploy the model on OCI Data Science. The following code assumes that:\\n\\n1. A dynamic group has been created and includes both the notebook sessions and model deployment resource types.\\n1. An Identity and Access Management (IAM) policy has been created and contains the required statements to allow the dynamic group to create and manage the associated OCI resources.\\n1. The Conda environment to be used for the model deployment has been published to the assigned Object Storage bucket.\\n\\n```python\\nimport ads\\nfrom ads.common.model_metadata import UseCaseType\\nfrom ads.model.framework.huggingface_model import HuggingFacePipelineModel\\n\\nads.set_auth(auth=\'resource_principal\')\\n\\nartifact_dir = \\"huggingface_pipeline_model_artifact\\"\\nhuggingface_pipeline_model = HuggingFacePipelineModel(estimator=pipe, artifact_dir=artifact_dir)\\n\\n# 1. Prepare\\nhuggingface_pipeline_model.prepare(\\n    inference_conda_env=\\"oci://conda-envs@*****/conda_environments/cpu/review/0.0.1/review_v0_0_1\\",\\n    inference_python_version=\\"3.8\\",\\n    training_conda_env=\\"oci://conda-envs@*****/conda_environments/cpu/review/0.0.1/review_v0_0_1\\",\\n    use_case_type=UseCaseType.SENTIMENT_ANALYSIS,\\n    force_overwrite=True,\\n)\\n\\n# 2. Save\\nhuggingface_pipeline_model.save()\\n\\n# 3. Deploy\\nhuggingface_pipeline_model.deploy(\\n    display_name=\\"Demo Review Model\\",\\n    deployment_instance_shape=\\"VM.Standard.E4.Flex\\",\\n    deployment_ocpus=1,\\n    deployment_memory_in_gbs=16,\\n)\\n\\n# 4. Print the invocation endpoint\\nprint(f\\"Endpoint: {huggingface_pipeline_model.model_deployment.url}/predict\\")\\n```\\n\\nIf everything ran as planned, then you should first see that the model has been saved to the model catalog in your OCI Data Science project.\\n\\n![Hugging Face model saved in the OCI Data Science project.](./assets/model-saved.png)\\n\\nYou should also see that the model has been deployed with the resources specified. The *Invoking your model* view provides the URL to call in your application.\\n\\n![Model deployment details and endpoint for invoking the model.](./assets/model-deployed.png)\\n\\nOnce deployed, you may invoke the endpoint using `APEX_WEB_SERVICE` to perform inference on the submitted text. However, like all secure OCI REST APIs, calling this endpoint will require the same HTTP request signature to authenticate with the service. This is an easy task with APEX web credentials support for OCI API keys. If you are new to this, check out either this Oracle blog [article](https://blogs.oracle.com/connect/post/better-file-storage-in-oracle-cloud), or follow the steps outlined in this LiveLabs [workshop](https://apexapps.oracle.com/pls/apex/r/dbpm/livelabs/view-workshop?wid=3558).\\n\\nAlso, be sure to add an IAM policy with a statement similar to this one:\\n\\n```\\nallow group ApexAgents to {DATA_SCIENCE_MODEL_DEPLOYMENT_PREDICT} in compartment Demo\\n```\\n\\nThe IAM user used by Oracle APEX should be added to this group. This would allow it to call the prediction endpoint for an model deployed in the specified compartment.\\n\\nCreate a page that allows users to enter a review, and then a page process that executes the PL/SQL code below, when the page is submitted.\\n\\n```sql\\ndeclare\\n  l_response clob;\\n  l_values apex_json.t_values;\\n  l_prediction_label varchar2(10);\\nbegin\\n  apex_web_service.g_request_headers(1).name := \'Content-Type\';\\n  apex_web_service.g_request_headers(1).value := \'application/json\';\\n\\n  l_response := apex_web_service.make_rest_request(\\n    p_credential_static_id => \'OCI_CREDENTIALS\'\\n    , p_http_method => \'POST\'\\n    , p_url => \'https://modeldeployment.us-phoenix-1.oci.customer-oci.com/ocid1.datasciencemodeldeployment.oc1.phx.***/predict\'\\n    , p_body => \'[\' || apex_json.stringify(:P4_CONTENT) || \']\'\\n  );\\n\\n  if apex_web_service.g_status_code = 200 then\\n    apex_json.parse(\\n      p_values => l_values\\n      , p_source => l_response\\n    );\\n\\n    l_prediction_label := apex_json.get_varchar2(\\n      p_values => l_values\\n      , p_path => \'prediction[1].label\'\\n    );\\n\\n    insert into review(\\n      prod_id\\n      , content\\n      , review_score\\n      , reviewed_by\\n      , reviewed_on\\n    ) values (\\n      :P4_PROD_ID\\n      , :P4_CONTENT\\n      , case l_prediction_label\\n          when \'LABEL_0\' then 1\\n          when \'LABEL_1\' then 2\\n          when \'LABEL_2\' then 3\\n          when \'LABEL_3\' then 4\\n          when \'LABEL_4\' then 5\\n          else 0\\n        end\\n      , :APP_USER\\n      , systimestamp\\n    );\\n  else\\n    raise_application_error(-20001, \'Failed to process new review.\');\\n  end if;\\nend;\\n```\\n\\nFrom the JSON response from calling `pipe(review)`, we know that it contains two attributes: `label` and `score`. The `label` is what we are after, and thus, we extract the value of `prediction[1].label` using the `APEX_JSON` package. Sometimes, adding the confidence score can be helpful to the user.\\n\\nHere\'s a simple demonstration of the fine-tuned model applied:\\n\\n<youtube youTubeId=\\"YlKM2vUrc0Q\\" />\\n\\n## What About the OCI AI Language Service\\n\\nIf you are familiar with Oracle\'s suite of AI Service, there is the OCI [Language](https://www.oracle.com/artificial-intelligence/language/) service that provides \\"sentiment analysis\\" using a pre-trained model. This is an easy-to-use API, and I have previously written on how to use it for [performing translation tasks](/content/revolutionising-language-learning).\\n\\n![Sentiment analysis results from invoking the OCI Language pre-trained model.](./assets/oci-ai-language-results.png)\\n\\nHowever, with the fine-tuned LLM, my goal was to provide a review score, ranging from 1 to 5, based on the reviewer\'s comments. The OCI Language pre-trained model returns polarity (positive, negative, mixed, or neutral) scores at the document, sentence, and aspect levels. And at this time, there isn\'t a custom model that I could train for sentiment analysis.\\n\\n## Summary\\n\\nThe OCI Data Science platform is a great complement to Oracle APEX when it comes to training and integrating machine learning models for specialized tasks. Both the Hugging Face and Oracle ADS libraries make it very easy for a novice like me to quickly build applications with some intelligence. I hope it excites you as much as it does to me. If you have a use case where this combination of technology might help, I\'d like to talk. You may reach out to me using this [form](https://cal.com/fuzziebrain/iaas-consult).\\n\\n> **Credits**\\n>\\n> * Hugging Face for the comprehensive documentation and tutorials, and making available the datasets and models used in this little experiment.\\n> * My employer, [Insum](https://insum.ca), for kindly sponsoring the GPU resources. It can be rather challenging to get them on the OCI, and I hope that can change soon.\\n> * Banner image generated using Stability AI\'s [Stable Diffusion](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0) model."},{"id":"/predict-the-rugby-world-cup-2023-winner","metadata":{"permalink":"/content/predict-the-rugby-world-cup-2023-winner","source":"@site/posts/predict-the-rugby-world-cup-2023-winner/index.md","title":"Predict the Rugby World Cup 2023 Winner","description":"Create an Oracle APEX application that predicts the winner of the Rugby World Cup 2023 matches using machine learning models trained and deployed using the OCI Data Science platform.","date":"2023-10-19T22:15:00.000Z","formattedDate":"October 19, 2023","tags":[{"label":"machine learning","permalink":"/content/tags/machine-learning"},{"label":"artificial intelligence","permalink":"/content/tags/artificial-intelligence"},{"label":"data science","permalink":"/content/tags/data-science"},{"label":"oci","permalink":"/content/tags/oci"},{"label":"oracle cloud","permalink":"/content/tags/oracle-cloud"},{"label":"oracle apex","permalink":"/content/tags/oracle-apex"},{"label":"python","permalink":"/content/tags/python"}],"readingTime":3.625,"hasTruncateMarker":true,"authors":[{"name":"Adrian Png","title":"Senior Cloud Solutions Architect @ Insum","url":"https://github.com/fuzziebrain","imageURL":"https://github.com/fuzziebrain.png","key":"fuzziebrain"}],"frontMatter":{"title":"Predict the Rugby World Cup 2023 Winner","image":"./assets/woman-in-crystal-ball.png","description":"Create an Oracle APEX application that predicts the winner of the Rugby World Cup 2023 matches using machine learning models trained and deployed using the OCI Data Science platform.","tags":["machine learning","artificial intelligence","data science","oci","oracle cloud","oracle apex","python"],"categories":["Technology"],"authors":["fuzziebrain"],"date":"2023-10-19T22:15:00.000Z"},"prevItem":{"title":"Deploy and Use Fine-Tuned LLMs in Oracle APEX","permalink":"/content/deploy-and-use-fine-tuned-llms-in-oracle-apex"},"nextItem":{"title":"Closing the Era of the APEX Challenge Coin","permalink":"/content/closing-the-era-of-the-apex-challenge-coin"}},"content":"![Woman in a crystal ball](./assets/woman-in-crystal-ball.png)\\n\\nThis has been one of those weeks where I couldn\'t wait to wake up and get to work. Just last week, [Luc Demanche](https://www.linkedin.com/in/luc-demanche-17107b2) threw me a challenge and asked that I help develop an [Oracle APEX](https://apex.oracle.com) application that could be used to predict the winner of the [Rugby World Cup 2023](https://www.rugbyworldcup.com/2023/) (RWC2023). I couldn\'t resist the challenge!\\n\\n\x3c!-- truncate --\x3e\\n\\nThe first challenge we had attempting to predict the outcome of a sports match was data. As with any machine learning (ML), obtaining ample high quality training data greatly influences the accuracy of the prediction models. This can sometimes be hard to find as professional sports is a lucrative market, and the highest quality data, such as player statistics, game performance, and match history, are typically collected, managed, and made available through paid channels. Something that I didn\'t have access to.\\n\\nFortunately, my French teammate [Louis Moreaux](https://www.linkedin.com/in/louis-moreaux) stumbled on a [Kaggle](https://www.kaggle.com/) dataset that contained historical match data that was scrapped from [Wikipedia](https://wikipedia.org) and aggregated by a contributor. The dataset contained minimal data on different rugby matches played since 1871. At first glance, there wasn\'t really much to work on, but fortunately, another contributor had presented a notebook that demonstrated how we can enrich the data and help better predict the match outcomes. This included calculating features such as a team\'s ranking point and form. This was good enough for the proof-of-concept application that we needed to build.\\n\\nIt was only a few months ago that I sat for and passed the [Oracle Cloud Infrastructure Data Science 2023 Professional](https://education.oracle.com/oracle-cloud-infrastructure-data-science-2023-professional/pexam_1Z0-1110-23), so this was the perfect opportunity to put training to practice!\\n\\nWithout any hesitation, I provisioned a new [Oracle Cloud Infrastructure](https://oracle.com/cloud) (OCI) [Data Science](https://www.oracle.com/artificial-intelligence/data-science/) project, and worked on training the prediction model using a notebook session. The notebook session is Oracle\'s managed [Jupyter](https://jupyter.org/) environment for data scientists to perform exploratory data analysis, data visualization, data cleaning and preparation, model training, validation, and deployment. I also utilise the powerful [Oracle Accelerated Data Science](https://accelerated-data-science.readthedocs.io) (ADS) Python library that allows me to perform a wide variety of activities including interfacing with the OCI, accessing data stored in an [Oracle Autonomous Database](https://oracle.com/autonomousdatabase), managing and deploying models. Just to name a few.\\n\\nOnce deployed, using the ML model in a web application is performed by invoking its HTTP endpoint. Of course, this being an OCI managed service, calls to the endpoints require signing. This is no different from calling any OCI REST APIs, and relies on creating the necessary OCI Identity and Access Management (IAM) users, groups, policies, and generating the required API keys. Oracle APEX simplifies the signing process, and you will only need to create the required APEX web credentials and reference its static identifier during the web service call. Below is an example PL/SQL snippet:\\n\\n```sql\\ndeclare\\n  c_model_url constant varchar2(32767) := \'https://modeldeployment.eu-paris-1.oci.customer-oci.com/ocid1.datasciencemodeldeployment.oc1.eu-paris-1.amaaaaa.../predict\';\\n\\n  l_ranking_points_home ml_team.ranking_point%type;\\n  l_ranking_points_away ml_team.ranking_point%type;\\n  l_home_form ml_result.home_form%type;\\n  l_away_form ml_result.home_form%type;\\n\\n  l_request_body clob;\\n  l_response clob;\\nbegin\\n  -- Code to retrieve the required input values based on the teams selected.\\n\\n  l_request_body :=\\n     apex_json.stringify(\\n       json_array(\\n          json_object(\\n            key \'neutral\' value 0\\n            , key \'world_cup\' value 1\\n            , key \'ranking_points_home\' value l_ranking_points_home\\n            , key \'ranking_points_away\' value l_ranking_points_away\\n            , key \'home_form\' value l_home_form\\n            , key \'away_form\' value l_away_form\\n         )\\n       )\\n    );\\n\\n  apex_web_service.g_request_headers(1).name := \'Content-Type\';\\n  apex_web_service.g_request_headers(1).value := \'application/json\';\\n\\n  l_response := apex_web_service.make_rest_request(\\n    p_url => c_model_url\\n    , p_http_method => \'POST\'\\n    , p_body => l_request_body\\n    , p_credential_static_id => pkg_oci_common.g_credential_static_id\\n  );\\n\\n  :P1_WINNING_TEAM :=\\n    case\\n      when json_value(l_response, \'$.prediction[0]\') = \'home_win\' then\\n        :P1_HOME_TEAM\\n      else\\n        :P1_AWAY_TEAM\\n    end;\\nend;\\n```\\n\\nHere\'s the outcome and predictions for the next two [matches](https://www.rugbyworldcup.com/2023/matches) to be held this week:\\n\\n<youtube youTubeId=\\"B9mOhTp6TIs\\" />\\n\\nLet\'s see how our ML model fares this week.\\n\\nDo take the predicted outcomes with a huge pinch of salt. I did not do sufficient justice to the tremendous amount of work required to train a good ML model. In the spirit of rapid application development, we built this demo application within days, but it clearly demonstrates how we can we can train, deploy, and perform inferences using machine learning models with minimal need to manage the underlying infrastructure. I have, unfortunately, left out much of the details as I\'m rushing to have this post published before the semi-final games tomorrow. However, you can expect more details on how to do this on your own, so stay tuned!\\n\\n> **Credits**\\n>\\n> Banner image generated using Stability AI\'s [Stable Diffusion](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0) model."},{"id":"/closing-the-era-of-the-apex-challenge-coin","metadata":{"permalink":"/content/closing-the-era-of-the-apex-challenge-coin","source":"@site/posts/closing-the-era-of-the-apex-challenge-coin/index.md","title":"Closing the Era of the APEX Challenge Coin","description":"Saying goodbye is hard.","date":"2023-10-11T06:00:00.000Z","formattedDate":"October 11, 2023","tags":[{"label":"orclapex","permalink":"/content/tags/orclapex"},{"label":"challenge coin","permalink":"/content/tags/challenge-coin"},{"label":"gratitude","permalink":"/content/tags/gratitude"},{"label":"joelkallmanday","permalink":"/content/tags/joelkallmanday"}],"readingTime":2.11,"hasTruncateMarker":true,"authors":[{"name":"Adrian Png","title":"Senior Cloud Solutions Architect @ Insum","url":"https://github.com/fuzziebrain","imageURL":"https://github.com/fuzziebrain.png","key":"fuzziebrain"}],"frontMatter":{"title":"Closing the Era of the APEX Challenge Coin","image":"./assets/joel-and-friends.jpg","description":"Saying goodbye is hard.","tags":["orclapex","challenge coin","gratitude","joelkallmanday"],"authors":["fuzziebrain"],"categories":["Technology","Community","Oracle Application Express"],"date":"2023-10-11T06:00:00.000Z"},"prevItem":{"title":"Predict the Rugby World Cup 2023 Winner","permalink":"/content/predict-the-rugby-world-cup-2023-winner"},"nextItem":{"title":"Speaking AI to Your Oracle Database","permalink":"/content/speaking-ai-to-your-oracle-database"}},"content":"![Joel and Friends](./assets/joel-and-friends.jpg)\\n\\nI hope Joel doesn\'t mind that I used this image here to commemorate [#JoelKallmanDay](https://twitter.com/search?q=%23JoelKallmanDay). This picture was [posted](https://twitter.com/joelkallman/status/1192268209291976704) by our dear friend, Joel Kallman, in 2019. Shortly after Mike Hichwa was presented with the first coin in the first series of the [Oracle APEX Challenge Coin](/content/id/1727).\\n\\n\x3c!-- truncate --\x3e\\n\\nI first approached Joel about this project on Thursday, January 3, 2019, at 3.05 PM Pacific Time. Presented the initial sketch of the coin, and sought his permission to use the Oracle APEX logo on the main face of the coin. Below is the initial sketch of both sides of the coin.\\n\\n![The initial sketch of the Oracle APEX challenge coin in 2019.](./assets/artwork-original.png)\\n\\nA minute later, I received an out-of-office reply saying:\\n\\n> Thank you for your message.\\n>\\n> I am on vacation until January 7, 2019 and will be unable to respond to email or voicemail during this period.  Happy New Year!\\n>\\n> Joel\\n\\nAt 5.44 PM that day, I unexpectedly received an reply from Joel granting me the needed permission to use the logo and included some suggestions for improvements. These were his exact words, double spaces after the period included.\\n\\n> Please forgive me for offering my opinion where one wasn\'t solicited, but I have a suggestion about some of the words you have on the coin.  Two words seem to be inconsistent with the 4 others - namely  STANDARDS and ON-TIME.  While the other 4 words seem to describe APEX itself, these other two really don\'t.  How about RELIABLE or SCALABLE or PROVEN or GLOBAL or something like that?\\n\\nI finally settled on the six words that you see on the coin today, and they include Joel\'s suggestions of \\"RELIABLE\\" and \\"PROVEN\\". And that\'s what he was. I may be a nobody, but even on while on vacation, he took personal time to think, optimise, suggest, and respond to my email.\\n\\n![The updated sketch before it was sent to the vendor for the final artwork.](./assets/artwork-updated.png)\\n\\nI often remind people with a coin today, that they hold a piece of him. You \\"make Oracle great again\\" by adding value to people\'s lives through the magic you do with Oracle APEX.\\n\\nThe final two coins were presented to [Paige Hanssen](https://www.linkedin.com/in/paige-hanssen-6a8530109) and [Sonja Meyer](https://www.linkedin.com/in/sonnemeyer) at the recently concluded [Oracle CloudWorld 2023](https://www.oracle.com/cloudworld). Congratulations to the both of them!\\n\\nLike everything, there\'s an alpha and omega. I hereby proclaim that the Oracle APEX Challenge Coin project has come to a successful close. Our service, however, continues.\\n\\n<youtube youTubeId=\\"Q7WrvOnBpIg\\" />"},{"id":"/speaking-ai-to-your-oracle-database","metadata":{"permalink":"/content/speaking-ai-to-your-oracle-database","source":"@site/posts/speaking-ai-to-your-oracle-database/index.md","title":"Speaking AI to Your Oracle Database","description":"A woman speaking to an Oracle.","date":"2023-09-23T23:00:00.000Z","formattedDate":"September 23, 2023","tags":[{"label":"generative ai","permalink":"/content/tags/generative-ai"},{"label":"genai","permalink":"/content/tags/genai"},{"label":"artificial intelligence","permalink":"/content/tags/artificial-intelligence"},{"label":"machine learning","permalink":"/content/tags/machine-learning"},{"label":"sql","permalink":"/content/tags/sql"},{"label":"autonomous database","permalink":"/content/tags/autonomous-database"},{"label":"llm","permalink":"/content/tags/llm"},{"label":"large language models","permalink":"/content/tags/large-language-models"},{"label":"oracle apex","permalink":"/content/tags/oracle-apex"},{"label":"orclapex","permalink":"/content/tags/orclapex"}],"readingTime":6.385,"hasTruncateMarker":true,"authors":[{"name":"Adrian Png","title":"Senior Cloud Solutions Architect @ Insum","url":"https://github.com/fuzziebrain","imageURL":"https://github.com/fuzziebrain.png","key":"fuzziebrain"}],"frontMatter":{"title":"Speaking AI to Your Oracle Database","tags":["generative ai","genai","artificial intelligence","machine learning","sql","autonomous database","llm","large language models","oracle apex","orclapex"],"categories":["Technology"],"authors":["fuzziebrain"],"date":"2023-09-23T23:00:00.000Z"},"prevItem":{"title":"Closing the Era of the APEX Challenge Coin","permalink":"/content/closing-the-era-of-the-apex-challenge-coin"},"nextItem":{"title":"Train an AI Model to Recognize Oracle APEX Challenge Coins","permalink":"/content/train-an-ai-model-to-recognize-oracle-apex-challenge-coins"}},"content":"![A woman speaking to an Oracle.](./assets/speak-to-the-oracle.png)\\n\\nOne of the key highlights at [Oracle Cloudworld 2023](https://www.oracle.com/cloudworld/) is general availability of [*Select AI*](https://docs.oracle.com/en/cloud/paas/autonomous-database/serverless/adbsb/sql-generation-ai-autonomous.html) feature that allows anyone to query the *Oracle Database* using natural language. This is likely made possible with a template wrapping the database metadata with the prompt, and then calling either the [*OpenAI*](https://openai.com) or [*Cohere*](https://cohere.com) APIs to generate the intended text.\\n\\n\x3c!-- truncate --\x3e\\n\\nThis feature is already available on an *Oracle Autonomous Database* (ADB), and yes, that includes the *Always Free* instances that I use for the managing the [*Oracle APEX*](https://apex.oracle.com) [*Challenge Coin*](/content/id/1727) registry. I know everyone\'s eager to try it out, and so am I! So here\'s a quick set of instructions to get you going.\\n\\nThe first thing that you\'ll need to do is make sure your application schema has the appropriate rights to use this feature. As the ADB *ADMIN* user, first add the Access Control Entry (ACE) for using the intended API. Also, grant the user access to both the `DBMS_CLOUD` and new `DBMS_CLOUD_AI` packages.\\n\\n```sql\\ndefine schema_name = \'core\'\\n\\nbegin\\n  dbms_network_acl_admin.append_host_ace(\\n    host => \'api.cohere.ai\'\\n    , ace => xs$ace_type(\\n        privilege_list => xs$name_list(\'http\')\\n        , principal_name => upper(\'&schema_name\')\\n        , principal_type => xs_acl.ptype_db\\n      )\\n  );\\nend;\\n/\\n\\ngrant execute on dbms_cloud to &schema_name;\\ngrant execute on dbms_cloud_ai to &schema_name;\\n```\\n> **NOTE:**\\n>\\n> For OpenAI, the `host` value would be `api.openai.com`.\\n\\nNext, as the user, create a credential for storing the required credentials for accessing the target API. For Cohere, you will first need to sign up for an account. For development use, access to the API is free, and an API key should have already been generated for you. A `username` is not required, and thus I replaced with the name associated with the API key.\\n\\n```sql\\nbegin\\n  dbms_cloud.create_credential(\\n    credential_name => \'COHERE_CRED\'\\n    , username => \'default\'\\n    , password => \'***\'\\n  );\\nend;\\n/\\n```\\n\\nCreate a profile using the credential.\\n\\n```sql\\nbegin\\n  dbms_cloud_ai.create_profile(\\n    profile_name => \'COHERE\'\\n    , attributes => json_object(\\n        key \'provider\' value \'cohere\'\\n        , key \'credential_name\' value \'COHERE_CRED\'\\n        , key \'object_list\' value json_array(\\n            json_object(\\n              key \'owner\' value \'core\'\\n              , key \'name\' value \'coin\'\\n            )\\n          )\\n      )\\n  );\\nend;\\n/\\n```\\n\\nThe credential and profile name can be anything that makes sense to you. However, when creating the profile, the `provider` attribute is required and must be either `openai` or `cohere`. The `object_list` attribute provides you the opportunity to scope what database objects should be included. You may optionally specify the `owner` and not a `name` to included **all** objects in the owner\'s schema. There are likely limits to how much data you might want to include in a request, and should also keep in mind that the metadata is submitted to an external system.\\n\\nOther profile attributes include:\\n\\n* `model`\\n* `temperature`\\n* `max_tokens`\\n* `stop_tokens`\\n\\nThis allows further adjustments to how the *Generative AI* model behaves.\\n\\nFinally, simply set the user\'s profile for performing the *Select AI* tasks:\\n\\n```sql\\nbegin\\n  dbms_cloud_ai.set_profile(\\n    profile_name => \'COHERE\'\\n  );\\nend;\\n/\\n```\\n\\nWith that, I can now ask:\\n\\n```sql\\ndefine question = \'how many coins where created for the APEX community?\'\\nselect ai runsql dbms_assert.enquote_literal(\'&question\');\\n```\\n\\nI correctly received the result `200`! But that\'s way too easy for this *Large Language Model* (LLM). So, I thought I\'ll add all the objects in the schema, since it isn\'t too big.\\n\\n```sql\\nbegin\\n  dbms_cloud_ai.drop_profile(\'COHERE\');\\n  dbms_cloud_ai.create_profile(\\n    profile_name => \'COHERE\'\\n    , attributes => json_object(\\n        key \'provider\' value \'cohere\'\\n        , key \'credential_name\' value \'COHERE_CRED\'\\n        , key \'object_list\' value json_array(\\n            json_object(\\n              key \'owner\' value \'core\'\\n              --, key \'name\' value \'coin\'\\n            )\\n          )\\n      )\\n  );\\nend;\\n/\\n```\\n\\nAnd then I asked, \\"How many coins have been sponsored?\\". It said, \\"0\\", and that is incorrect! To figure out why, I used the *action* keyword *showsql* as opposed to *runsql*.\\n\\n```sql\\ndefine question = \'how many coins have been sponsored?\'\\nselect ai showsql dbms_assert.enquote_literal(\'&question\');\\n```\\n\\nYielding the SQL statement:\\n\\n```sql\\nSELECT COUNT(*) FROM \\"CORE\\".\\"SHIPMENT\\"\\nWHERE \\"SHIPMENT_ID\\" IN (\\n  SELECT \\"SHIPMENT_ID\\" FROM \\"CORE\\".\\"SPONSOR_EVENT\\"\\n  WHERE \\"SPONSORSHIP_TYPE\\" = \'SPONSORED\'\\n)\\n```\\n\\nAnd this could be wrong for reasons as follows:\\n\\n* There is no `SPONSORSHIP_TYPE` with the value `SPONSORED`. I wonder if having a lookup table might have helped, but I do have a `CHECK` constraint that specifies what values are allowed.\\n* It might not have understood the intended meaning of the table name `SPONSOR_EVENT`, but I don\'t always name tables such they are semantically accurate. And of course, having lived through the age of 30-character limits on object names, table names might be missing vowels or word separators.\\n* My data model for this project might not have been optimum. `SHIPMENT` was about shipments, but not all sponsor events involved a shipping event. They were delivered personally.\\n\\nI would have expected the `narrate` to explain how it had derived the SQL statement it generated.\\n\\n```sql\\nselect ai narrate dbms_assert.enquote_literal(\'&question\');\\n```\\n\\nHowever, what I got was a different SQL statement. And the same with `chat`.\\n\\nFinally, the next question I had was, \\"Can I use this in APEX?\\".\\n\\n![Using \\"Select AI\\" in Oracle APEX](./assets/select-ai-in-apex.png)\\n\\nMaybe not yet?\\n\\nI don\'t believe that, today, there is a simply way to \\"plugin\\" this new feature. The outputs of the query can be as dynamic as the inputs. Maybe a blend of using the `APEX_EXEC` package and `DBMS_CLOUD_AI.GENERATE` function? I wonder. Also, currently, I don\'t believe there\'s an APEX PL/SQL API that would allow us to create a report dynamically. Perhaps that might change some day. **If you have thoughts about this, please reach out**.\\n\\n## Summary\\n\\nThis technology is still very much in its infancy, but exciting nonetheless. Though I am on the fence if it\'s ready for prime time. I wouldn\'t put this in front of an uninformed user yet. To be fair, I have not tried this with the OpenAI LLM models. Maybe it\'ll do better. Or perhaps, I need the LLMs to suggest improvements to my data models. ;-) And that brings me to the final point - semantics! In the era of big data and machine learning, one concept that often gets brushed away is providing rich semantics to data and metadata. We have had OCI Data Catalog for a while now... how many of us actually use it? Or adding proper comments to the schema metadata, which was clearly lacking in my toy project. If that was included in the prompt, then perhaps I would have had better results.\\n\\n## Update [2023-09-24]\\n\\nSo, I was still trying to get *Select AI* to give me the correct answer to the question:\\n\\n> How many coins have been sponsored?\\n\\nI did find the answer to my earlier question about whether or not comments were included in the prompt. There is one more profile attribute that can be added to the profile. The `comments` attribute supplies the LLM with comments in the `sys.all_col_comments` view. Unfortunately, table comments don\'t seem to be included. Adding comments describing what the tables and views represented didn\'t seem to help. I couldn\'t get it to ignore a view that was never going to tell you if a coin has been sponsored. It was also creating predicates on columns that didn\'t exist, so the third attribute I had tweaked was to specify what database objects are in the `object_list`.\\n\\n```sql\\ndeclare\\n  l_objects_list_jsonstr varchar2(32767);\\nbegin\\n  begin\\n    dbms_cloud_ai.drop_profile(\'COHERE_WITH_COMMENTS\');\\n  exception\\n    when others then null;\\n  end;\\n\\n  select json_arrayagg(\\n    json_object(key \'owner\' value \'core\', key \'name\' value table_name))\\n  into l_objects_list_jsonstr\\n  from user_tables;\\n\\n  dbms_cloud_ai.create_profile(\\n    profile_name => \'COHERE_WITH_COMMENTS\'\\n    , attributes => json_object(\\n        key \'provider\' value \'cohere\'\\n        , key \'credential_name\' value \'COHERE_CRED\'\\n        , key \'comments\' value true\\n        , key \'temperature\' value 0\\n        , key \'object_list\' value l_objects_list_jsonstr\\n      )\\n  );\\nend;\\n/\\n```\\n\\nI still do have the correct answer. :-("},{"id":"/train-an-ai-model-to-recognize-oracle-apex-challenge-coins","metadata":{"permalink":"/content/train-an-ai-model-to-recognize-oracle-apex-challenge-coins","source":"@site/posts/train-an-ai-model-to-recognize-oracle-apex-challenge-coins/index.md","title":"Train an AI Model to Recognize Oracle APEX Challenge Coins","description":"Black Star Line Stock Certificate","date":"2023-09-04T23:00:00.000Z","formattedDate":"September 4, 2023","tags":[{"label":"artificial intelligence","permalink":"/content/tags/artificial-intelligence"},{"label":"machine learning","permalink":"/content/tags/machine-learning"},{"label":"ocr","permalink":"/content/tags/ocr"},{"label":"oracle ai","permalink":"/content/tags/oracle-ai"},{"label":"oracle apex","permalink":"/content/tags/oracle-apex"},{"label":"ai applications","permalink":"/content/tags/ai-applications"},{"label":"challenge coin","permalink":"/content/tags/challenge-coin"},{"label":"innovative technology","permalink":"/content/tags/innovative-technology"},{"label":"community","permalink":"/content/tags/community"},{"label":"image recognition","permalink":"/content/tags/image-recognition"}],"readingTime":7.59,"hasTruncateMarker":true,"authors":[{"name":"Adrian Png","title":"Senior Cloud Solutions Architect @ Insum","url":"https://github.com/fuzziebrain","imageURL":"https://github.com/fuzziebrain.png","key":"fuzziebrain"}],"frontMatter":{"title":"Train an AI Model to Recognize Oracle APEX Challenge Coins","tags":["artificial intelligence","machine learning","ocr","oracle ai","oracle apex","ai applications","challenge coin","innovative technology","community","image recognition"],"categories":["Technology"],"authors":["fuzziebrain"],"date":"2023-09-04T23:00:00.000Z"},"prevItem":{"title":"Speaking AI to Your Oracle Database","permalink":"/content/speaking-ai-to-your-oracle-database"},"nextItem":{"title":"Revolutionising Language Learning: How AI and Oracle APEX Transform Everyday Challenges","permalink":"/content/revolutionising-language-learning"}},"content":"![Black Star Line Stock Certificate](./assets/stock-certificate-1280.jpg)\\n\\n[Doug Gault](https://douggault.com/) had suggested a while ago that I could bring more awareness to the [Oracle APEX Challenge Coin](/content/id/1727) project if I had an [Oracle APEX](https://apex.oracle.com) application that plots the location of each sponsored coin on a world map. I agree. However, aside from a lack of time, I had also wanted to find interesting ways for awardees to submit their claim of ownership. Over the last week, I had worked on making this dream of a \\"claims\\" map a reality with the use of the [Oracle Cloud Infrastructure](https://oracle.com/cloud) (OCI) [Vision](https://www.oracle.com/artificial-intelligence/vision/) AI service, more specifically, its custom model training feature.\\n\\nHere\'s an early preview of the claims process in the *Claim My Oracle APEX Challenge Coin* application.\\n\\n\x3c!-- truncate --\x3e\\n\\n<youtube youTubeId=\\"a0UErWOPUzw\\" />\\n\\n## Background\\n\\nFor those new to Oracle APEX, and those who have not been acquainted with the Oracle APEX Challenge Coin, it was a project I had started in early 2019 to support and acknowledge the value that developers bring to the world community. I had designed and minted a total of 200 coins over two years, and have since distributed almost all of them. As of now, there are only two more coins available for sponsorship. Read the original [article](/content/id/1727) for more details and how to sponsor these challenge coins.\\n\\n## OCI Vision\\n\\nThe OCI Vision is one of several AI services available on the OCI. Three pre-trained models are available for developers to create intelligent applications that need to perform computer vision tasks, such as, image and text recognition. Using the pre-trained models, developers can use the OCI REST API to perform image classifications, object detections, and text detections. It also included document processing tasks like key-value pair extraction, and document classification. However, it has since been spun off to a separate offering called [Document Understanding](https://www.oracle.com/artificial-intelligence/document-understanding/). If you are interested to know more about using Document Understanding in Oracle APEX, please check out this [Oracle LiveLabs workshop](https://apexapps.oracle.com/pls/apex/r/dbpm/livelabs/view-workshop?wid=3728) that I published in July, 2023.\\n\\nOCI Vision uses [deep learning](https://wikipedia.org/wiki/Deep_learning) (DL) approaches, and one of the key features of DL is the ability to perform [transfer learning](https://wikipedia.org/wiki/Transfer_learning). *TL;DR*, transfer learning is process that uses pre-trained, generalist models, and then perform additional training over specialised datasets. This significantly reduces the amount of time, and training data, needed to train a machine learning (ML) model for specific operational needs. For example, recognising an Oracle APEX Challenge Coin! The OCI Vision\'s custom model feature allows developers to fine-tune and train highly specialised computer vision (CV) models to perform either image classification or object detection tasks.\\n\\nIn this project, I will train a custom model for detecting the front and back sides of the coin.\\n\\n## Prepare the Dataset\\n\\nAvailability of large and high quality datasets are crucial to training good CV models that have high precision and recall. Since the project was launched, recipients have spontaneously posted their cherished gifts on social media. Hence, my first challenge was to scour the Internet for these photographs. There are only 200 coins ever minted, and not every recipient has posted a photo of their coin on social media. Thankfully, I was able to harvest about slightly over one hundred of such images. They include photos of the front and back of the coin, different lighting scenes, pose, and proportional sizes.\\n\\nA prerequisite for performing custom model training is to create a data set in the  OCI [Data Labeling](https://www.oracle.com/artificial-intelligence/data-labeling/) service. This is an OCI managed service that lets data scientists and developers to curate and label data that will be used to perform supervised ML training. Creating an image classification and object detection models is a supervised learning task.\\n\\nBefore creating the dataset, assemble and upload all the images to an OCI [Object Storage](https://www.oracle.com/cloud/storage/object-storage/) bucket. Once the images have been staged, the dataset created, and the records generated, you will need to work through all the images to mark and label the dataset records. Here\'s a quick demonstration of the data labelling process for object detection use:\\n\\n<youtube youTubeId=\\"gYtKz2phnsQ\\" />\\n\\n## Custom Model Training\\n\\nAs mentioned earlier, the OCI Vision custom models feature allows data scientists and developers to easily fine-tune a inference model for specialised use cases. After creating the dataset and labelling the 100+ images, next step is to create a project and then a custom model. Choose the appropriate model type for your business use case. For this application, I chose *Object detection*, and then moved on to select the dataset I had created with the Data Labeling service.\\n\\n![Choose model type.](./assets/choose-model-type.png)\\n\\nThere are more options to consider, including the desired training duration time. There are three options available: recommended, quick, and custom modes. In the early phases of the model training, I\'d recommend using the quick mode, just to get a sense of how well the model might perform, given the quality and quantity of labelled samples in your dataset.\\n\\nAfter the training is completed, the model detail will provide you the necessary metrics to decide if you need to repeat the process. The *precision* and *recall* are measurements typically used for judging the performance of ML models. If your model scores poorly, consider sourcing additional data for training and testing, or applying image transformations. Increasing the training duration may help, but sometimes, over-fitting a ML model may yield poorer results.\\n\\nFor convenience, in the lower half of the custom model details page, you will find a simple user interface for applying the custom model to source images.\\n\\n<youtube youTubeId=\\"r38VySvkkHA\\" />\\n\\n## Using the Custom Model in an Application\\n\\nThe custom model is trained to recognise both the front and back of an Oracle APEX challenge coin. However, for this application, I will only need it to detect the back side of the coin. In addition to object detection, I will also have OCI Vision to perform text detection (optical character recognition), and extract the unique serial number found on every coin.\\n\\nThe flow of the application is as follows:\\n\\n1. User begins a new claim using a wizard.\\n1. User uploads a photo of the back of the coin, and chooses the unique serial number. A unique claims identifier is issued, and together with the submitted information, it is added to an APEX collection.\\n1. In the next step, the user will optionally provide a display name that will be rendered in the map. The user will also use the Oracle APEX geocoded address functionality to set the estimated location of the coin.\\n1. User confirms the details to be submitted.\\n1. Upon confirmation, the claim is created in the database.\\n1. The uploaded image is stored in the designated OCI Object Storage bucket.\\n1. The claim is then evaluated using the OCI Vision using its OCI REST API. This involves an HTTP `POST` to the endpoint: `https://vision.aiservice.{{REGION}}.oci.oraclecloud.com/20220125/actions/analyzeImage` (substituted `{{REGION}}` with the desired region).\\n1. The results of the REST call is evaluated. If the desired label and serial number text is found, with the minimum confidence score, then the claim\'s validation status is marked `PASS`. Otherwise, it is marked `FAIL`.\\n1. The map is refreshed, only displaying claims where the validation status value is `PASS`.\\n\\nThis *AnalyzeImage* endpoint expects a JSON payload described by the *AnalyzeImagesDetails*. This payload is sent in the request body. Here\'s what it looks like:\\n\\n```json\\n{\\n\\t\\"compartmentId\\": \\"ocid1.compartment.oc1..<REDACTED>\\",\\n\\t\\"image\\": {\\n\\t\\t\\"source\\": \\"OBJECT_STORAGE\\",\\n\\t\\t\\"namespaceName\\": \\"<REDACTED>\\",\\n\\t\\t\\"bucketName\\": \\"claims-bucket\\",\\n\\t\\t\\"objectName\\": \\"submission_20230904011039.jpg\\"\\n\\t},\\n\\t\\"features\\": [\\n\\t\\t{\\n\\t\\t\\t\\"modelId\\": \\"ocid1.aivisionmodel.oc1.phx.<REDACTED>\\",\\n\\t\\t\\t\\"featureType\\": \\"OBJECT_DETECTION\\",\\n\\t\\t\\t\\"maxResults\\": 10\\n\\t\\t},\\n\\t\\t{\\n\\t\\t\\t\\"featureType\\": \\"TEXT_DETECTION\\",\\n\\t\\t\\t\\"maxResults\\": 10\\n\\t\\t}\\n\\t]\\n}\\n```\\n\\nThe `image` attribute describes the image source, including the bucket name, and unique object name. Next, the `features` attribute is an array of JSON objects describing the type(s) of image analysis to perform. The first feature is of type `OBJECT_DETECTION`. Specifying the `modelId` attribute containing the custom model\'s *OCID*, will instruct the endpoint to use the custom model, instead of the pre-trained object detection model. The second calls the pre-trained text detection model.\\n\\nA response for a valid claim would look like this (truncated for readability):\\n\\n```json\\n{\\n\\t\\"imageObjects\\": [\\n\\t\\t{\\n\\t\\t\\t\\"name\\": \\"Back\\",\\n\\t\\t\\t\\"confidence\\": 0.94813126,\\n\\t\\t\\t...\\n\\t\\t}\\n\\t],\\n\\t...\\n\\t\\"imageText\\": {\\n\\t\\t\\"words\\": [\\n\\t\\t\\t...\\n\\t\\t\\t{\\n\\t\\t\\t\\t\\"text\\": \\"A0035\\",\\n\\t\\t\\t\\t\\"confidence\\": 0.9957846,\\n\\t\\t\\t\\t...\\n\\t\\t\\t}\\n\\t\\t],\\n    ...\\n}\\n```\\n\\nThe `imageObjects` attribute returns an array of up to 10 (set in the `maxResults` attribute of the request body) objects detected. Each contains the label name and corresponding confidence score. Similarly, the `imageText` returns an array of words (and lines). Each containing the text extracted, and its confidence score. These findings are then used to make an initial assessment if the claim is valid.\\n\\n## Summary\\n\\nThe application is still very much in its infancy. So far, working on this project has not only introduced me to the world of OCI Vision custom model training, but also some newer Oracle APEX features that I have yet to use. These includes the Map region and Geocoded Address page item. Next, I\'d like to use the *Approvals Component* to implement a simple workflow that adds a human reviewer to make a final determination, and approve the claim.\\n\\n:::info\\n[Picture](https://commons.wikimedia.org/wiki/File:Black_Star_Line_Stock_Certificate.jpg) from the Wikimedia Commons.\\n:::"},{"id":"/revolutionising-language-learning","metadata":{"permalink":"/content/revolutionising-language-learning","source":"@site/posts/revolutionising-language-learning/index.md","title":"Revolutionising Language Learning: How AI and Oracle APEX Transform Everyday Challenges","description":"I\'m finally finding more time, post-conference, to catch up on my writing. In this latest post, I\'d like to talk about a presentation that I did at Kscope23, \\"Solving Everyday Problems with the Oracle Cloud\\". During the session, I had described and demonstrated how we can use the Oracle Cloud Infrastructure (OCI) AI services and Oracle APEX to build compelling solutions, even for personal life challenges.","date":"2023-08-20T17:00:00.000Z","formattedDate":"August 20, 2023","tags":[{"label":"artificial intelligence","permalink":"/content/tags/artificial-intelligence"},{"label":"machine learning","permalink":"/content/tags/machine-learning"},{"label":"speech recognition","permalink":"/content/tags/speech-recognition"},{"label":"language learning","permalink":"/content/tags/language-learning"},{"label":"oracle ai","permalink":"/content/tags/oracle-ai"},{"label":"oracle apex","permalink":"/content/tags/oracle-apex"},{"label":"ai applications","permalink":"/content/tags/ai-applications"},{"label":"cloud solutions","permalink":"/content/tags/cloud-solutions"},{"label":"technical workflow","permalink":"/content/tags/technical-workflow"},{"label":"innovative technology","permalink":"/content/tags/innovative-technology"}],"readingTime":10.49,"hasTruncateMarker":true,"authors":[{"name":"Adrian Png","title":"Senior Cloud Solutions Architect @ Insum","url":"https://github.com/fuzziebrain","imageURL":"https://github.com/fuzziebrain.png","key":"fuzziebrain"}],"frontMatter":{"title":"Revolutionising Language Learning: How AI and Oracle APEX Transform Everyday Challenges","tags":["artificial intelligence","machine learning","speech recognition","language learning","oracle ai","oracle apex","ai applications","cloud solutions","technical workflow","innovative technology"],"categories":["Technology"],"authors":["fuzziebrain"],"date":"2023-08-20T17:00:00.000Z"},"prevItem":{"title":"Train an AI Model to Recognize Oracle APEX Challenge Coins","permalink":"/content/train-an-ai-model-to-recognize-oracle-apex-challenge-coins"},"nextItem":{"title":"Say Hello From An Autonomous Database","permalink":"/content/say-hello-from-an-autonomous-database"}},"content":"<youtube youTubeId=\\"aYnYQu7MY2g\\" />\\n\\nI\'m finally finding more time, post-conference, to catch up on my writing. In this latest post, I\'d like to talk about a presentation that I did at [Kscope23](https://kscope23.odtug.com), \\"Solving Everyday Problems with the Oracle Cloud\\". During the session, I had described and demonstrated how we can use the [*Oracle Cloud Infrastructure*](https://www.oracle.com/cloud) (OCI) [*AI services*](https://www.oracle.com/artificial-intelligence/ai-services/) and [*Oracle APEX*](https://apex.oracle.com) to build compelling solutions, even for personal life challenges.\\n\\n\x3c!-- truncate --\x3e\\n\\n## Background\\n\\nWe are constantly bombarded by news about *Artificial Intelligence* (AI) and *Machine Learning* (ML), driving many of us seek to find a strong use case to take advantage of these technologies and use them in our work and personal projects. I am no exception.\\n\\nA few years ago, I started learning the Japanese language using [Duolingo](https://www.duolingo.com/), but had a lot of trouble remembering the Hiragana, Katakana, Kanji characters, same issues when I learned Mandarin in school. Back in the days, learning Mandarin was through rote learning, and that sometimes involved flashcards. In August 2021, I published an [article](https://developer.oracle.com/learn/technical-articles/learning-languages-with-oracle-cloud) through the Oracle Developer Relations team, about how I had used a [M5Stack](https://www.m5stack.com) IoT device and some OCI resources to build a self-updating electronic flashcard that I could place on the refrigerator door. However, constantly updating the content repository can be tedious.\\n\\nLess than a year later, two developments at Oracle made it easier for me to enhance the data entry process with AI-based technologies, specifically, *Natural Language Processing* (NLP). First, the Speech service was released on Feb 23, 2022. Later, in October 2022, a second version of the Language service was released with a new feature for performing language translation tasks. Both these added features from Oracle AI provided me an opportunity to \\"use AI\\" to improve my approach for entering new Japanese words into my online personal word bank.\\n\\nI had blogged about this project [previously](/content/id/2201), to celebrate the Joel Kallman Day last year. This follow-up article provides a technical deep dive to guide readers on how to implement as similar solution.\\n\\n## New AI-Enhanced Data Entry Workflow\\n\\n\\"A picture speaks a thousand words\\", so here\'s are two workflow diagrams that provides a crude overview of how new words are entered today.\\n\\n### Speech-to-Text\\n\\n```mermaid\\nflowchart TD\\n    A([Create a new entry]) --\x3e B[Click Record and say a word or sentence.]\\n    B --\x3e C{Has the recording stopped?}\\n    C --\x3e |Yes| DY[Upload Base64-encoded audio file to the input object storage location.]\\n    C --\x3e |No| DN[Continue waiting] --\x3e C\\n    DY --\x3e E[Create a Speech transcription job.]\\n    E --\x3e F{Is transcription job\'s status = SUCCEEDED}\\n    F --\x3e |Yes| GY[Retrieve transcription job details.]\\n    F --\x3e |No| FA{Is it CANCELED or FAILED?}\\n    FA --\x3e |Yes| FAY[/Display an error message./]\\n    FAY ----\x3e Z\\n    FA --\x3e |No| FAN[Continue waiting] --\x3e F\\n    GY --\x3e H[Parse the transcription task result and extract the output\'s object storage URL from the job details.]\\n    H --\x3e I[Retrieve the transcription task result using the object storage URL.]\\n    I --\x3e J[Parse the transcription task result and extract the transcribed text.]\\n    J --\x3e K[/Display the transcribed text in the target APEX page item./]\\n    K --\x3e Z([End])\\n```\\n\\nAs mentioned in my earlier post, when the speech service was first launched, it only supported the Waveform Audio File Format (WAV), and had strict rules on sample rates and channels. Today, the service now supports a much larger number of formats including *FLAC*, *OGG*, and *WEBM*. In the latest version of my application, only the [*MediaStream Recording API*](https://developer.mozilla.org/docs/Web/API/MediaStream_Recording_API) is used.\\n\\nThe audio capture and transcription job requests are performed through an Oracle APEX modal page shown below.\\n\\n![The Oracle APEX modal page for recording and transcribing audio.](./assets/record-speech-modal.png)\\n\\nTo start off, we will need some utility JavaScript functions, and for simplicity, I had declared this in the page\'s *Function and Global Variable Declaration*. Firstly, there\'s the `mediaRecorder` variable that stores the reference to the *MediaRecorder* object that we will instantiate at page load. It is followed by a simple utility functions that I had borrowed from [Nick Buytaert](https://apexplained.wordpress.com). Reference to his blog post is in the comments.\\n\\n```javascript\\nlet mediaRecorder;\\n\\nconst util = {\\n  // builds a js array from long string\\n  // credit: https://apexplained.wordpress.com/2016/09/12/chunked-multi-file-upload-with-ajax/\\n  clob2Array: function (clob, size) {\\n    let array = [];\\n    loopCount = Math.floor(clob.length / size) + 1;\\n    for (let i = 0; i < loopCount; i++) {\\n      array.push(clob.slice(size * i, size * (i + 1)));\\n    }\\n    return array;\\n  },\\n  // converts blob to base64 string\\n  blob2base64: function (blob) {\\n    return new Promise(function(resolve, reject){\\n      const fileReader = new FileReader();\\n      fileReader.onerror = reject;\\n      fileReader.onload = function() {\\n        const dataURI = fileReader.result;\\n        resolve(dataURI.substr(dataURI.indexOf(\',\') + 1));\\n      }\\n      fileReader.readAsDataURL(blob);\\n    });\\n  }\\n};\\n```\\n\\nNext, I added the following JavaScript code in the *Execute when Page Loads* that would instantiate the *MediaRecorder* object, and implement the callback function when recording is stopped. When that function is called, the recorded audio will be loaded into an embedded audio player, allowing the user to playback the recording. It will also call an AJAX callback (`TRANSCRIBE_AUDIO`) process that includes the audio MIME-type, and the recording as a base64-encoded string. That\'s where the `util` package comes in handy.\\n\\n```javascript\\nif (navigator.mediaDevices) {\\n  console.debug(\\"getUserMedia supported.\\");\\n  /**\\n   * While FLAC is preferred, it appears Firefox and Chrome currently do not\\n   * support this lossless audio format.\\n   **/\\n  let audioMimeType = \'audio/webm\';\\n\\n  console.debug(audioMimeType + \' is\' +\\n    (MediaRecorder.isTypeSupported(audioMimeType) ? \'\' : \' not\')\\n    + \' supported.\');\\n\\n  const constraints = { audio: true };\\n  let chunks = [];\\n\\n  navigator.mediaDevices\\n    .getUserMedia(constraints)\\n    .then((stream) => {\\n      mediaRecorder = new MediaRecorder(stream);\\n\\n      mediaRecorder.onstop = async (e) => {\\n        console.debug(\\"data available after MediaRecorder.stop() called.\\");\\n        var popup = apex.widget.waitPopup();\\n\\n        const blob = new Blob(chunks, { type: audioMimeType });\\n        chunks = [];\\n        const audioURL = URL.createObjectURL(blob);\\n        player.src = audioURL;\\n        console.debug(\\"recorder stopped\\");\\n\\n        const base64 = await util.blob2base64(blob);\\n\\n        var process = apex.server.process(\\n          \\"TRANSCRIBE_AUDIO\\",\\n          {\\n            x01: audioMimeType,\\n            f01: util.clob2Array(base64, 32000)\\n          }\\n        );\\n\\n        process.done(function(result) {\\n          apex.item(\\"P2_TRANSCRIBED_TEXT\\").setValue(result.transcribedText);\\n          apex.message.showPageSuccess(\\"Audio transcribed.\\");\\n        }).fail(function(jqXHR, textStatus, errorMessage) {\\n          apex.message.alert(\\"Failed to process audio.\\");\\n          console.error(\\"Failed to save. Error: \\" + errorMessage);\\n        }).always(function() {\\n          popup.remove();\\n        });\\n      };\\n\\n      mediaRecorder.ondataavailable = (e) => {\\n        chunks.push(e.data);\\n      };\\n    })\\n    .catch((err) => {\\n      console.error(`The following error occurred: ${err}`);\\n    });\\n}\\n```\\n\\nThe AJAX callback named `TRANSCRIBE_AUDIO` contains the following PL/SQL procedure:\\n\\n```sql\\ndeclare\\n  c_audio_mime_type constant apex_application.g_x01%type := apex_application.g_x01;\\n  l_clob_temp clob;\\n  l_blob_content blob;\\n  l_buffer varchar2(32767);\\n  l_filename varchar2(30) := \'sample-\' || to_char(systimestamp, \'YYYYMMSSHH24MISS\')\\n    || \'.\' || regexp_replace(c_audio_mime_type, \'^audio/(.+)$\', \'\\\\1\');\\n  l_transcribed_text varchar2(32767);\\nbegin\\n  dbms_lob.createtemporary(\\n    lob_loc => l_clob_temp\\n    , cache => false\\n    , dur => dbms_lob.session\\n  );\\n\\n  for i in 1..apex_application.g_f01.count loop\\n    l_buffer := apex_application.g_f01(i);\\n\\n    dbms_lob.writeappend(\\n      lob_loc => l_clob_temp\\n      , amount => length(l_buffer)\\n      , buffer => l_buffer\\n    );\\n  end loop;\\n\\n  l_blob_content := apex_web_service.clobbase642blob(l_clob_temp);\\n\\n  pkg_oci_os_util.p_upload_object(\\n    p_bucket_name => :G_INPUT_BUCKET_NAME\\n    , p_file_blob => l_blob_content\\n    , p_filename => l_filename\\n    , p_mime_type =>  c_audio_mime_type\\n  );\\n\\n  dbms_lob.freetemporary(l_clob_temp);\\n\\n  l_transcribed_text := pkg_oci_speech_util.f_transcribe_audio(\\n    p_input_bucket_name => :G_INPUT_BUCKET_NAME\\n    , p_output_bucket_name => :G_OUTPUT_BUCKET_NAME\\n    , p_filename => l_filename\\n  );\\n\\n  sys.htp.p(\'{ \\"transcribedText\\": \\"\' || l_transcribed_text || \'\\"}\');\\nend;\\n```\\n\\nThe callback process converts the base64-encoded string back to a BLOB, and then uploads it to the target Object Storage bucket defined by the APEX substitution string `G_INPUT_BUCKET_NAME`. Once the file has been uploaded successfully, the Speech service transcription job is then created using a facade. The PL/SQL function takes in the input and output bucket names, as well as the audio filename, and returns the transcribed text. The function also has an optional parameter `p_source_language_code` that lets the developer specify the language used in audio recording. The default value is `en-US`.\\n\\n> **NOTE**\\n>\\n> If you have not worked with the Object Storage service in APEX before, be sure to check out the LiveLabs [workshop](https://apexapps.oracle.com/pls/apex/r/dbpm/livelabs/view-workshop?wid=3558) I had prepared on this subject. I will also be presenting this workshop in-person at the upcoming [Oracle CloudWorld 2023](https://www.oracle.com/cloudworld/). The session number is [HOL2327](https://reg.rf.oracle.com/flow/oracle/cwoh23/catalog/page/catalog/session/1683745130623001qcie).\\n\\nThis function actually makes several OCI REST API calls. The first one creates the transcription job using standard `APEX_WEB_SERVICE` code shown in the snippet below:\\n\\n```sql\\napex_web_service.g_request_headers(1).name := \'Content-Type\';\\napex_web_service.g_request_headers(1).value := \'application/json\';\\n\\nl_response := apex_web_service.make_rest_request(\\n  p_url => l_request_url\\n  , p_http_method => \'POST\'\\n  , p_body => json_object(\\n      key \'compartmentId\' value pkg_oci_speech_util.gc_compartmentid\\n      , key \'inputLocation\' value json_object(\\n          key \'locationType\' value \'OBJECT_LIST_INLINE_INPUT_LOCATION\'\\n          , key \'objectLocations\' value json_array(\\n              json_object(\\n                key \'bucketName\' value p_input_bucket_name\\n                , key \'namespaceName\' value pkg_oci_speech_util.gc_namespace\\n                , key \'objectNames\' value json_array(p_filename)\\n              )\\n          )\\n      )\\n      , key \'outputLocation\' value json_object(\\n          key \'bucketName\' value p_output_bucket_name\\n          , key \'namespaceName\' value pkg_oci_speech_util.gc_namespace\\n          , key \'prefix\' value \'apex\'\\n      )\\n      , key \'normalization\' value json_object(\\n          key \'isPunctuationEnabled\' value true\\n      )\\n      , key \'modelDetails\' value json_object(\\n          key \'languageCode\' value p_source_language_code\\n      )\\n  )\\n  , p_credential_static_id => pkg_oci_speech_util.gc_credential_static_id\\n);\\n```\\n\\nThe *CreateTranscriptionJob* API calls are asynchronous. When the job was created, the API returns a response object containing details about the transcription job, including the *OCID*, and the job\'s status (the `id` and `lifecycleState` attribute respectively). Rarely will the job complete immediately, therefore, the code then loops, making a call to the *GetTranscriptionJob* endpoint to get the current status until either job has succeeded, failed, or was cancelled.\\n\\nIf the transcription job was successful, the function then retrieves the task details using the *GetTranscriptionTask* API. The response will contain the required object storage location of the transcription results written in a JSON file (see example below).\\n\\n```json\\n{\\n  \\"status\\": \\"SUCCESS\\",\\n  \\"timeCreated\\": \\"2023-08-20 23:32:46.96\\",\\n  \\"modelDetails\\": {\\n    \\"domain\\": \\"GENERIC\\",\\n    \\"languageCode\\": \\"en-US\\"\\n  },\\n  \\"audioFormatDetails\\": {\\n    \\"format\\": \\"WEBM\\",\\n    \\"numberOfChannels\\": 2,\\n    \\"encoding\\": \\"OPUS\\",\\n    \\"sampleRateInHz\\": 48000\\n  },\\n  \\"transcriptions\\": [\\n    {\\n\\t  \\"transcription\\": \\"Hello, how are you?\\",\\n\\t  \\"confidence\\": \\"0.9600\\",\\n\\t  \\"tokens\\": [\\n\\t    {\\n\\t\\t  \\"token\\": \\"This\\",\\n\\t\\t  \\"startTime\\": \\"0.624s\\",\\n\\t\\t  \\"endTime\\": \\"1.104s\\",\\n\\t\\t  \\"confidence\\": \\"0.9527\\",\\n\\t\\t  \\"type\\": \\"WORD\\"\\n\\t\\t},\\n\\t\\t...\\n\\t  ]\\n\\t}\\n  ]\\n}\\n```\\n\\nIn this file, you can find details about the source audio file, the transcription, the confidence value of the transcription, and details of individual tokens. The function retrieves this file from the object storage, parses it, and then returns the value of the `transcription` attribute.\\n\\nFinally, this page contains the following UI components:\\n\\n* An audio player to playback the recording. It can be embedded within a *Static Content* region using the source:\\n    ```html\\n    <audio id=\\"player\\" controls style=\\"width: 100%;\\"></audio>\\n    ```\\n* A textarea page item called `P2_TRANSCRIBED_TEXT`; and\\n* Three buttons *Record*, *Stop*, and *Accept*.\\n\\nThe first two buttons trigger the media recorder object to start and stop recording. Simply attach dynamic actions for mouse click events and execute the corresponding JavaScript code.\\n\\n```javascript\\n// For the start button.\\nif(mediaRecorder) {\\n  mediaRecorder.start();\\n}\\n\\n// For the stop button.\\nif(mediaRecorder) {\\n  mediaRecorder.stop();\\n}\\n```\\n\\nThe *Accept* button executes the *Close Dialog* process, returning the value of the page item `P2_TRANSCRIBED_TEXT` that is used to set the value of `P1_TEXT` in the parent page.\\n\\n### Translate Text\\n\\n```mermaid\\nflowchart TD\\n    A([Click the button Translate.]) --\x3e B[Call the Language service\'s REST API, specifying the source and target language codes.]\\n    B --\x3e C{Is the HTTP response status code = 200?}\\n    C --\x3e |Yes| CY[Parse the response and extract the translated text.]\\n    CY --\x3e D[Display the translated text in the target APEX page item.]\\n    C --\x3e |No| CN[/Display error message./]\\n    CN ---\x3e Z\\n    D --\x3e Z([End])\\n```\\n\\nThe Language service API is a lot simpler to consume. When the *Translate* button is clicked, the page is submitted, and a page submission process is called with to execute the following PL/SQL procedure:\\n\\n```sql\\ndeclare\\n  c_rest_url constant varchar2(200) := \'https://language.aiservice.us-phoenix-1.oci.oraclecloud.com/20221001/actions/batchLanguageTranslation\';\\n  c_from_lang constant varchar2(2) := \'en\';\\n  c_compartment_ocid varchar2(1024) := \'ocid1.compartment.oc1..*****\';\\n  c_credential_static_id constant varchar2(50) := \'OCI_CREDENTIALS\';\\n  c_number_of_text_to_translate pls_integer := 1;\\n\\n  l_request_body json_object_t;\\n  l_document json_object_t;\\n  l_documents json_array_t;\\n  l_response clob;\\nbegin\\n  l_request_body := json_object_t();\\n  l_documents := json_array_t();\\n\\n  /**\\n   * The Language service APIs process requests in batches. Even though we are\\n   * only submitting a single text to be translated, I have written a loop to\\n   * construct the request body to \\"future proof\\" the implementation. ;-)\\n   **/\\n  for i in 1..c_number_of_text_to_translate\\n  loop\\n    l_document := json_object_t();\\n    l_document.put(\'key\', to_char(i));\\n    l_document.put(\'text\', :P1_TEXT);\\n    l_document.put(\'languageCode\', c_from_lang);\\n    l_documents.append(l_document);\\n  end loop;\\n\\n  l_request_body.put(\'documents\', treat(l_documents as json_element_t));\\n  l_request_body.put(\'targetLanguageCode\', :P1_TO_LANG);\\n  l_request_body.put(\'compartmentId\', c_compartment_ocid);\\n\\n  apex_debug.info(l_request_body.to_string());\\n\\n  apex_web_service.g_request_headers(1).name := \'Content-Type\';\\n  apex_web_service.g_request_headers(1).value := \'application/json\';\\n\\n  l_response := apex_web_service.make_rest_request(\\n    p_url => c_rest_url\\n    , p_http_method => \'POST\'\\n    , p_body => l_request_body.to_string()\\n    , p_credential_static_id => c_credential_static_id\\n  );\\n\\n  if apex_web_service.g_status_code != 200 then\\n    apex_debug.error(\'HTTP Status Code: \' || apex_web_service.g_status_code);\\n    apex_debug.error(l_response);\\n    raise_application_error(-20002, \'Translation unsuccessful! HTTP Status Code: \'\\n      || apex_web_service.g_status_code);\\n  else\\n    apex_debug.info(l_response);\\n    l_documents := treat(json_object_t.parse(l_response).get(\'documents\') as json_array_t);\\n    for i in 0..(l_documents.get_size() - 1)\\n    loop\\n      l_document := treat(l_documents.get(0) as json_object_t);\\n      :P1_TRANSLATED_TEXT := l_document.get_string(\'translatedText\');\\n    end loop;\\n  end if;\\nend;\\n```\\n\\nThat\'s it!\\n\\n## Summary\\n\\nIn this article, we have delved into the remarkable synergy between AI and Oracle APEX. By harnessing cutting-edge AI features like speech-to-text transcription and language translation, we\'ve unveiled a world of possibilities for enhancing communication and learning. Whether you\'re a beginner or an expert in the realm of machine learning, I hope this article has provided you a head start and inspiration for creating your own innovative solutions to solving day-to-day challenges using low-code technologies."},{"id":"/say-hello-from-an-autonomous-database","metadata":{"permalink":"/content/say-hello-from-an-autonomous-database","source":"@site/posts/say-hello-from-an-autonomous-database/index.md","title":"Say Hello From An Autonomous Database","description":"New features announced for Oracle Autonomous Database","date":"2023-04-17T00:00:00.000Z","formattedDate":"April 17, 2023","tags":[{"label":"oracle autonomous database","permalink":"/content/tags/oracle-autonomous-database"},{"label":"oracle cloud","permalink":"/content/tags/oracle-cloud"},{"label":"email","permalink":"/content/tags/email"},{"label":"slack","permalink":"/content/tags/slack"},{"label":"microsoft teams","permalink":"/content/tags/microsoft-teams"},{"label":"msteams","permalink":"/content/tags/msteams"},{"label":"communications","permalink":"/content/tags/communications"},{"label":"orclapex","permalink":"/content/tags/orclapex"},{"label":"oracle apex","permalink":"/content/tags/oracle-apex"},{"label":"plsql","permalink":"/content/tags/plsql"},{"label":"dbms_cloud_notification","permalink":"/content/tags/dbms-cloud-notification"}],"readingTime":9.92,"hasTruncateMarker":true,"authors":[{"name":"Adrian Png","title":"Senior Cloud Solutions Architect @ Insum","url":"https://github.com/fuzziebrain","imageURL":"https://github.com/fuzziebrain.png","key":"fuzziebrain"}],"frontMatter":{"title":"Say Hello From An Autonomous Database","tags":["oracle autonomous database","oracle cloud","email","slack","microsoft teams","msteams","communications","orclapex","oracle apex","plsql","dbms_cloud_notification"],"authors":["fuzziebrain"],"date":"2023-04-17T00:00:00.000Z"},"prevItem":{"title":"Revolutionising Language Learning: How AI and Oracle APEX Transform Everyday Challenges","permalink":"/content/revolutionising-language-learning"},"nextItem":{"title":"I Am Obsolete","permalink":"/content/id/2301"}},"content":"![New features announced for Oracle Autonomous Database](./assets/adb-new-features-alert.png)\\n\\nIn case you missed it, Oracle recently announced the availability of a new feature for the [Autonomous Database](https://www.oracle.com/autonomous-database/) (ADB) that allows developers and database administrators to send messages using either emails, [Slack](https://slack.com), or [Microsoft Teams](https://www.microsoft.com/microsoft-teams) (Teams). At [Insum](https://insum.ca), we use teams for internal communications and collaboration, and thus, you can appreciate my excitement when I heard about this feature release. Here\'s an early guide to getting started with `DBMS_CLOUD_NOTIFICATION` for those using Teams at their organisation.\\n\\n\x3c!-- truncate --\x3e\\n\\n> Before we begin, it is worthwhile to note that for [Oracle APEX](https://apex.oracle.com), you can also send out emails and Slack/Teams using the `APEX_MAIL` and `APEX_WEB_SERVICE` PL/SQL APIs. This is an additional convenience feature that is provided with an ADB.\\n\\nSetting up `DBMS_CLOUD_NOTIFICATION` to send Teams messages and data requires a few preparatory steps. You may read the details [here](\\nhttps://docs.oracle.com/en/cloud/paas/autonomous-database/adbsa/send-msteams-notifications.html#GUID-80ACCB33-DF2D-42D7-AC01-23670B5B90E4) but this blog post would provide further guidance and issues I noted when working through the steps.\\n\\n## Create a Teams Application\\n\\nThe first task that you will need to complete is to create a Teams application using the Teams [developer portal](https://dev.teams.microsoft.com). Create a new application and provide the following details:\\n\\n* App names\\n    * Short name\\n* Descriptions\\n    * Short description\\n    * Long description\\n* Version\\n* Developer information\\n    * Developer or company name\\n    * Website\\n* App URLs\\n    * Privacy policy\\n    * Terms of use\\n\\n![Provide the basic information required for the new application.](./assets/teams-app-basic-information.png)\\n\\nOther than the *Application (client) ID*, you **must** enter the required information to ensure the application passes validation and can be published successfully.\\n\\nOnce you are satisfied with what you have entered, save the changes, and then return to the *Dashboard* to make sure there are no errors or warnings. Correct any if necessary.\\n\\n## Create a Bot and Configure the App Feature\\n\\nNext, create a *Bot*. You can either do this by navigating to *Tools* > *Bot management*, or from within the *Apps* interface, navigate to *App features* and then click *Bot*. There, you will find the link *Create a new bot*.\\n\\n![Create an app feature.](./assets/teams-app-features.png)\\n\\nCreating a bot is simple. You will only need to provide a name for the bot. After the bot is created, you will be redirected to the bot\'s details page. Navigate to the *Client secrets*, and then click the button *Add a secret client for your bot*. Store the generated client secret securely as it will only be displayed **once**.\\n\\n![Bot secret](./assets/bot-secret.png)\\n\\nReturn to the *Bots management* page and note the *Bot ID* as well.\\n\\n![Bot ID](./assets/bot-id.png)\\n\\nAfter the bot has been created, return to the bot configuration in the app\'s feature page. Select the bot that you have just created, select the actions that the bot can do, and the scope.\\n\\n![Bot app feature configuration](./assets/teams-app-bot-feature-configuration.png)\\n\\n## Publish the Application\\n\\nOn the app\'s details page, navigate to *Publish* > *Publish to org*. At the bottom of the page, there should be a button to submit the app for publication. Click it and inform the Teams administrator.\\n\\n## Tasks for Azure Active Directory Administrators\\n\\nThe next task involves defining the *API permissions* that the bot **must** have. If you do not have permission to access the *Azure Active Directory* (Azure AD) console, then please work with the administrator to complete this step.\\n\\nWhen the bot was created, the platform automatically creates an *App registration* in Azure AD. You may access the app registrations using either [Microsoft Entra](https://entra.microsoft.com) or the main [Azure portal](https://portal.azure.com). Locate the app registration for the bot and then navigate to the *API permissions* page. Add the following permissions:\\n\\n* `Files.ReadWrite.All`\\n* `Sites.ReadWrite.All`\\n* `ChannelSettings.Read.All`<sup>*</sup>\\n\\n![Required API permissions](./assets/azuread-api-permissions-and-grant-consent.png)\\n\\n> **IMPORTANT** The Oracle documentation does not include `ChannelSettings.Read.All`. I will discuss this in a later section.\\n\\nFinally, click the *Grant admin consent for {{TENANT}}* link to allow the bot to access the specified APIs.\\n\\n## Tasks for Teams Administrators\\n\\nIn this next set of tasks, again, you may have to work with the administrators for your *Microsoft 365* tenancy or designated Teams administrator to approve and publish the Teams application. Access the Teams admin console [here](https://admin.teams.microsoft.com).\\n\\nNavigate to *Teams apps* > *Manage apps*. The first thing you might notice is that there is an app that has been submitted and pending approval. In the table below, search and locate the application you have created.\\n\\n![Search for the Teams app.](./assets/teams-admin-manage-apps.png)\\n\\nClick on the app, and on the app\'s details page, click *Publish* to approve the application for use in Teams.\\n\\n![Approve and publish the application.](./assets/teams-admin-publish-app.png)\\n\\nIt takes a little while for the app to show up in Teams for it to appear in the Teams *Apps* store under *Built for your org* category and then add it to team. A quick workaround is to add it to the team immediately using the admin console as show in the figure below.\\n\\n![Add the app to a team.](./assets/teams-admin-add-app-to-team.png)\\n\\nIn the table of apps, select the application and then click the button *Add to team*. Select the team you wish to add the app to, and then click the button *Apply*. Back in Teams, you may validate by accessing the team\'s management page and then click the tab *Apps*. You should see that the app has been added to the team.\\n\\n![App successfully added to team.](./assets/teams-team-apps.png)\\n\\n## Sending Messages and Data to Teams\\n\\nNow that you have fulfilled the prerequisites, it is time to test this feature.\\n\\n### Create Credential\\n\\nIn the document, it is stated that the ADB\'s *ADMIN* should create the credential, but I have tested it to work even with the credential created by the user. However, the schema that you wish to use to create the credentials **must** have *execute* privilege on the `DBMS_CLOUD` package.\\n\\nCreate the credential using the bot ID and secret:\\n\\n```sql\\nbegin\\n  dbms_cloud.create_credential(\\n    credential_name => \'TEAMS_CRED\'\\n    , username => \'6b2c7*****f7a15\'\\n    , password => \'Abd8Q*****BSc~i\'\\n  );\\nend;\\n/\\n```\\n\\n### Team Channel Link\\n\\nIn order to send messages and data, you will need **three** parameters obtained from either the team\'s or channel\'s link. They look something like these:\\n* Team\'s link\\n```\\nhttps://teams.microsoft.com/l/team/19%3a94d8e*****d68aa%40thread.tacv2/conversations?groupId=23e1c*****ba568&tenantId=7d58a*****c60a0\\n```\\n* Team channel\'s link\\n```\\nhttps://teams.microsoft.com/l/channel/19%3aa94d8*****d68aa%40thread.tacv2/Monthly%2520Reports?groupId=23e1c*****ba568&tenantId=7d58a*****c60a0\\n```\\n\\nThe information you will need are the `{{TEAM}}`, `{{CHANNEL}}` and `{{TENANT}}` components as marked below:\\n* From the team\'s link\\n```\\nhttps://teams.microsoft.com/l/team/{{TEAM}}/conversations?groupId=23e1c*****ba568&tenantId={{TENANT}}\\n```\\n* From the team channel\'s link\\n```\\nhttps://teams.microsoft.com/l/channel/{{CHANNEL}}/General?groupId=23e1c*****ba568&tenantId={{TENANT}}\\n```\\n\\nThe values for the parameters are:\\n\\n* `tenant` => `{{TENANT}}`\\n* `team` => `{{TEAM}}`\\n* `channel` => `{{CHANNEL}}`\\n\\n> **IMPORTANT** The team ID derived from the team\'s link is the same as the ID of the team\'s *General* channel.\\n\\n### Send Message\\n\\nWith that information handy, you are now ready to send a message to the team. In this example, we will send a message to the channel *General* for the team *Sales and Marketing* using the following PL/SQL procedure:\\n\\n```sql\\nbegin\\n  dbms_cloud_notification.send_message(\\n    provider => \'msteams\'\\n    , credential_name => \'TEAMS_CRED\'\\n    , message => \'**SUCCESS!** message sent from \'\\n        || sys_context(\'USERENV\',\'DB_NAME\')\\n    , params => json_object(\'channel\' value \'19%3a94d8*****d68aa%40thread.tacv2\')\\n  );\\nend;\\n/\\n```\\n\\nYou should see the message appear in Teams shortly.\\n\\n![Message from the ADB in Teams.](./assets/teams-message-received.png)\\n\\nDid you see that? There\'s support for *Markdown*! Bot messages support plain-text, Markdown, and XML formats as documented [here](https://learn.microsoft.com/microsoftteams/platform/bots/how-to/format-your-bot-messages).\\n\\n### Send Data\\n\\nNext, we will attempt to query the sample data available in the ADB\'s `SH` schema, and then send it to the same channel in `CSV` format. Execute the following code:\\n\\n```sql\\ndeclare\\nl_sql clob := q\'[select\\n  p.prod_name\\n  , s.quantity_sold\\n  , s.amount_sold\\n  , t.day_number_in_month || \' \' || t.calendar_month_name || \' \'\\n      || t.calendar_year as sold_on\\nfrom sh.sales s\\njoin sh.products p on p.prod_id = s.prod_id\\njoin sh.times t on t.time_id = t.time_id\\nfetch first 10 rows only]\';\\nbegin\\n  dbms_cloud_notification.send_data(\\n    provider => \'msteams\'\\n    , credential_name => \'TEAMS_CRED\'\\n    , query => l_sql\\n    , params => json_object(\\n                  \'tenant\' value \'7d58a*****c60a0\'\\n                  , \'team\' value \'19%3a94d8*****d68aa%40thread.tacv2\'\\n                  , \'channel\' value \'19%3a94d8*****d68aa%40thread.tacv2\'\\n                  , \'title\' value \'daily_sales_report\'\\n                  , \'type\' value \'csv\'\\n                )\\n  );\\nend;\\n/\\n```\\n\\nIf successful, you should see the `CSV` document link in teams.\\n\\n![Notification and link in Teams.](./assets/teams-data-received.png)\\n\\nAnd when you click on the link, it should open the file in *Microsoft Excel* (Excel).\\n\\n![Results from the query in Excel.](./assets/received-data.png)\\n\\n## Notable Issues\\n\\n### Version\\n\\nIf you need to make changes your Teams app after it has been approved and published by the Teams administrator, you must do the following:\\n\\n1. Increment the version number in the app\'s basic information page and save it.\\n1. Submit the updated app for approval.\\n1. Have the Teams administrator repeat the approval process only. There should not be a need to add the app to the team again.\\n\\n### Error Messages\\n\\nIf you have worked with an Oracle Database for a while, then it wouldn\'t surprise you that the error messages from the database sometimes do not represent the underlying issue well. ;-)\\n\\n#### ORA-20010: Missing credential name\\n\\nI came across this error in two difference scenarios. They are for when I attempted to:\\n\\n* Send a message when the Teams app was not published successfully.\\n* Send a message when the Teams app was not added to the channel\'s team.\\n* Send data to a channel other than the team\'s *General* channel.\\n\\nIt appears this error is returned whenever there is an error making a REST call to the *Microsoft Graph* API.\\n\\nThe first two scenarios were easily resolved. However, I am still unable to send data to a channel, other than the team\'s *General* channel.\\n\\n#### ORA-20000: Cant Access Team, Check App has enough permission\\n\\nThis one baffled me for a long time when I was trying to send data. In the documentation, it was stated that the value for `team` should be the `teamID` component in the team\'s link. However, the example code showed a value that looks a lot more like the `groupid`. As it turns out, this isn\'t a *permissions* issue, but rather, I had supplied the wrong value for `team`, and thus the internal ID for the team could not be found when the *Microsoft Bot* API was queried.\\n\\n#### ORA-20000: Cant Access Channel, Check App has enough permission\\n\\nSimilar to the previous error message, you might get this error when an incorrect `channel` identifier is provided.\\n\\n## Summary\\n\\nThere are countless use cases where developers might want to send notifications or data to users, whether it is through emails or a collaboration platform like Slack and Microsoft Teams. For example, notifying users involved in a business workflow for follow up action, sending daily reports to the C-suites, and more. Again, in Oracle APEX, we already have these capabilities by using either `APEX_MAIL` to send emails, or `APEX_WEB_SERVICE` to call webhooks and REST APIs. However, when working in an ADB, this provides yet another convenience method to perform the same task.\\n\\nWith regards to sending messages, I had wondered about why the development team might have chosen to implement the messaging using a bot as oppose to the simpler webhook interface. I found this [article](https://learn.microsoft.com/microsoftteams/platform/webhooks-and-connectors/what-are-webhooks-and-connectors) that listed this as a comparison between the two approaches:\\n\\n> A notification bot works as a Teams application. You can define your business logic to process data and show data in a customized format.\\n\\nWith the added ability to generate and send data files, I can see why they opted to choose to implement the functionality using a bot. Further, wearing an \\"admin\\" hat, I would be more comfortable having a streamlined process for managing access to Teams and the associated resources in Microsoft 365 tenancy.\\n\\nThe team behind the Oracle Autonomous Database product is constantly working on new features to enhance the user/developer experience, and this is one of them! We can only hope that these \\"dbms_cloud\\"-type packages find their way quickly to the *Oracle Base Database* service and on-premise installations.\\n\\nI hope this article helps you to use this new feature and skip the few pain points that I had experienced getting it going!"},{"id":"id/2301","metadata":{"permalink":"/content/id/2301","source":"@site/posts/i-am-obsolete/index.md","title":"I Am Obsolete","description":"A cassette tape.","date":"2023-01-14T00:00:00.000Z","formattedDate":"January 14, 2023","tags":[{"label":"chatgpt","permalink":"/content/tags/chatgpt"},{"label":"oracle cloud","permalink":"/content/tags/oracle-cloud"},{"label":"artificial intelligence","permalink":"/content/tags/artificial-intelligence"},{"label":"ai","permalink":"/content/tags/ai"},{"label":"machine learning","permalink":"/content/tags/machine-learning"},{"label":"ml","permalink":"/content/tags/ml"},{"label":"transformers","permalink":"/content/tags/transformers"},{"label":"natural language processing","permalink":"/content/tags/natural-language-processing"},{"label":"nlp","permalink":"/content/tags/nlp"},{"label":"generative models","permalink":"/content/tags/generative-models"},{"label":"orclapex","permalink":"/content/tags/orclapex"},{"label":"oracle apex","permalink":"/content/tags/oracle-apex"},{"label":"plsql","permalink":"/content/tags/plsql"}],"readingTime":15.725,"hasTruncateMarker":true,"authors":[{"name":"Adrian Png","title":"Senior Cloud Solutions Architect @ Insum","url":"https://github.com/fuzziebrain","imageURL":"https://github.com/fuzziebrain.png","key":"fuzziebrain"}],"frontMatter":{"title":"I Am Obsolete","tags":["chatgpt","oracle cloud","artificial intelligence","ai","machine learning","ml","transformers","natural language processing","nlp","generative models","orclapex","oracle apex","plsql"],"slug":"id/2301","authors":["fuzziebrain"],"date":"2023-01-14T00:00:00.000Z"},"prevItem":{"title":"Say Hello From An Autonomous Database","permalink":"/content/say-hello-from-an-autonomous-database"},"nextItem":{"title":"Wrapping Up 2022","permalink":"/content/id/2202"}},"content":"![A cassette tape.](./assets/cassette-tape_1280x720.png)\\n\\nWhen I was first introduced to generative adversarial networks (GANs), I was impressed, but yet not impressed. The concept, performance, and outcomes of these advanced machine learning (ML) algorithms were truly jaw-dropping, but easily abused. And so, when everyone was excitedly chatting about [ChatGPT](https://chat.openai.com/), I was cynical and tried to tune out until I came across a [comment](https://www.linkedin.com/posts/sausheong_teachers-v-chatgpt-schools-face-new-challenge-activity-7015524158113533952-OClM) by an old acquaintance, following a news article on concerns about ChatGPT and potential plagiarism in school.\\n\\n> *It\'s like someone using a calculator in an arithmetic exam, ChatGPT is just another tool.*\\n>\\n> -- [Sau Sheong Chang](https://sausheong.com/)\\n\\nThat statement got me rethinking my stance on the technology, and curious about its utility in my day-to-day job.\\n\\n\x3c!--truncate--\x3e\\n\\nAnd I\'m glad I did. My own exploration of this subject started by reading Sau Sheong\'s [post](https://medium.com/sausheong/chatting-about-chatgpt-645783b72e6b) to get a general idea about ChatGPT and the underlying technology that drives its ability to generate text and images. After *intelligent agents* and *artificial neural Networks*, *natural language processing* (NLP) was the next machine learning (ML) topic that I was passionate about. There wasn\'t a full module on that subject back when I was doing my graduate school training in *knowledge engineering* techniques, so it was pretty much a touch and go. Though during the pandemic, in 2020, I did take up a *Coursera*/*DeepLearning.AI* course on *Tensorflow* that covered NLP in one of its modules. There I got my legs deeper in the water and learned more in-depth about *recurrent neural networks* (RNN), *long short-term memory* (LSTM), and *Bidirectional Encoder Representations from Transformers* (BERT). And the keyword *transformers* is what\'s driving algorithms such as the *Generative Pre-trained Transformer* (GPT) used by ChatGPT.\\n\\n*Transformers are a type of neural network architecture that are particularly well-suited for natural language processing tasks. They allow a model to effectively process input sequences of variable length and attend to certain parts of the input while generating output. In other words, they enable the model to understand the context and meaning of words and sentences. This is achieved by using self-attention mechanisms, which allow the model to weigh the importance of different parts of the input when making predictions. This has led to significant improvements in the performance of models on a wide range of NLP tasks such as machine translation, text summarization, and question answering. Overall, the transformer architecture has been a major step forward in the field of NLP, and has been widely adopted in state-of-the-art models.*\\n\\nI don\'t know enough yet to go any deeper than flashing our buzzwords, and a lot of this is still over my head. I found this video to be very clear and easy to understand these concepts quickly, so have a watch!\\n\\n<youtube youTubeId=\\"SZorAJ4I-sA\\" />\\n\\n## Writing Content and Code\\n\\nWhen I first started on this blog post, I thought about using OpenAI\'s [*DALL-E*](https://openai.com/blog/dall-e/) model to generate an image that I could use in the banner. Unlike models such as [*StyleGAN*](https://github.com/NVlabs/stylegan), DALL-E is a a transformer language model that uses text and images as inputs to infer and generate a new image based on text inputs.\\n\\nHere\'s what I did:\\n\\n1. Sign up for a free account to access OpenAI\'s [API](https://openai.com/api/).\\n1. Create an API key.\\n1. Setup [Conda](https://conda.io) and create a [Jupyter](https://jupyter.org/) notebook.\\n1. In the first cell, call the *Create image* API adapting from the example code in the documentation:\\n    ```python\\n    import openai\\n\\n    openai.organization = \\"org-abc.....def\\"\\n    openai.api_key = \\"averylongstringthatshouldbekeptasecret\\"\\n\\n    response = openai.Image.create(\\n        prompt = \\"A robot with a large speech bubble.\\"\\n        , n = 10\\n        , size = \\"1024x1024\\"\\n    )\\n\\n    print(\\"Done\\")\\n    ```\\n    The parameter `prompt` is the descriptive text about the image that I\'d like the model to generate. I am also requested for `10` images using the parameter `n`, and finally, chose the desired image size using the three available values: `256x256`, `512x512`, or `1024x1024`.\\n1. It\'s been a while since I played with image manipulation in Python and then displaying it in a notebook, so I enlisted ChatGPT\'s help and asked:\\n    ```text\\n    create code in python to display images loaded from a URL in a grid in a jupyter notebook\\n    ```\\n    And after thinking for a while it returned this:\\n    ```python\\n    from IPython.display import display\\n    from PIL import Image\\n    import requests\\n\\n    urls = [\'https://www.example.com/image1.jpg\', \'https://www.example.com/image2.jpg\',\\n            \'https://www.example.com/image3.jpg\', \'https://www.example.com/image4.jpg\']\\n\\n    images = []\\n\\n    for url in urls:\\n        response = requests.get(url)\\n        images.append(Image.open(response.content))\\n\\n    widths, heights = zip(*(i.size for i in images))\\n\\n    total_width = sum(widths)\\n    max_height = max(heights)\\n\\n    new_im = Image.new(\'RGB\', (total_width, max_height))\\n\\n    x_offset = 0\\n    for im in images:\\n        new_im.paste(im, (x_offset,0))\\n        x_offset += im.size[0]\\n\\n    display(new_im)\\n    ```\\n    Great, but that\'s not going to work out-of-the-box, so I adapted it to suite my needs:\\n    ```python\\n    from IPython.display import display\\n    from PIL import Image\\n    import requests\\n\\n    images = []\\n\\n    for suggestion in response[\\"data\\"]:\\n        _response = requests.get(suggestion[\\"url\\"], stream = True)\\n        images.append(Image.open(_response.raw))\\n\\n    widths, heights = zip(*(i.size for i in images))\\n\\n    total_width = sum(widths)\\n    max_height = max(heights)\\n\\n    new_im = Image.new(\'RGB\', (total_width, max_height))\\n\\n    x_offset = 0\\n    for im in images:\\n        new_im.paste(im, (x_offset,0))\\n        x_offset += im.size[0]\\n\\n    display(new_im)\\n    ```\\n    Running this cell generated a \\"thumbnail\\" gallery of size `10240x1024`.\\n    ![Gallery of 10 images generated by DALL-E.](./assets/cell-two-output_1024x102.jpg)\\n1. After identifying my choice, the first one, I ran the following code (that I got lazy and asked ChatGPT again for help) to save it on my workstation:\\n    ```python\\n    urllib.request.urlretrieve(response[\\"data\\"][0][\\"url\\"], \\"banner-image.jpg\\")\\n    ```\\n\\nHere\'s the result after cropping it to a ratio of 16:9 that I typically use for banner images:\\n\\n![An image of a robot generated with a speech bubble using DALL-E](./assets/banner-image.png)\\n\\nNeat huh?\\n\\n:::note\\nI\'ll publish a full walk-through on what I did to setup Conda and Jupyter in my new section [Tech TnT](/tech-tnt) soon.\\n:::\\n\\nAnd what about textual content? In case you did not notice earlier, the blurb describing what transformers are were deliberately italicized. That\'s because I got ChatGPT to write a few words. I borrowed the idea from yet another great [read](https://kozyrkov.medium.com/introducing-chatgpt-aa824ad89623) written by [Cassie Kozyrkov](https://kozyrkov.medium.com/).\\n\\nHere\'s the prompt I used:\\n\\n> Explain what are transformers in natural language processing using the style of Adrian Png.\\n\\nDid it express what I wanted to say? Perhaps, and here are some key aspects:\\n\\n* Transformers are a type of neural network, and used for natural language processing tasks.\\n* A new concept and meaning to the word \\"Attention\\", and how it\'s used in NLP.\\n* List the types of use cases.\\n\\nWhenever I have a writers\' block, I would pen my thoughts sparsely and randomly, and then reorder and pad them with the required sentence constructs. Let\'s try doing that, this time including the term *positional encoding*. Here\'s the updated prompt:\\n\\n```text\\nWrite a passage explaining what transformers are in natural language processing. Capture the following key points:\\n- Transformers are a type of neural network, and used for natural language processing tasks.\\n- Mention positional encoding.\\n- A new concept and meaning to the word \\"Attention\\", and how it\'s used in NLP.\\n- List the types of use cases.\\n```\\n\\n> Transformers are a type of neural network architecture that have revolutionized the field of natural language processing. These models are designed to effectively process input sequences of variable length and generate output by using self-attention mechanisms. This allows the model to weigh the importance of different parts of the input when making predictions, which is crucial for understanding the context and meaning of words and sentences. A key component of transformers is the use of positional encoding, which allows the model to understand the order of the words in a sentence, even when the input is processed in parallel. This was a new concept and meaning to the word \\"Attention\\" when it was first introduced in NLP.\\n>\\n> There are many use cases for transformers in NLP, including but not limited to machine translation, text summarization, question answering, and text-to-speech synthesis. The transformer architecture has been widely adopted in state-of-the-art models and has led to significant improvements in performance on a wide range of NLP tasks.\\n\\nI don\'t know if I could express it as well as it did, but hey, I like it. But do I like using it? Not really. I\'m still not fully comfortable with using the text as-is. I like my content to be authentically \\"me\\" with my language flaws included. However, how is this different from engaging a professional copywriter to write an article based on the ideas you provide?\\n\\n## Who Owns the Content\\n\\nI think this is a question that is often ask and debated upon. Who owns the generated code, images, or text? For DALL-E, their [content policy](https://labs.openai.com/policies/content-policy) and [FAQ](https://help.openai.com/en/articles/6468065-dall-e-content-policy-faq) seems to say that you are free to use the generated images, however, my immediate concern was about the work that it was derived upon. That appears to be explained [here](https://github.com/openai/DALL-E/blob/master/model_card.md). I\'m not good with legalese, and so, my level of comfort using the images generated is 4 on a scale of 1 to 10. And I\'d presume that the concerns about generated code and text are very similar.\\n\\n## The \\"CANIUSE\\" in Oracle APEX\\n\\nYes! For those new to [Oracle APEX](https://apex.oracle.com), this is a rapid application development platform that lets you create usable web applications very quickly. It won\'t take you long to create an intelligent web application that takes advantage of the OpenAI models. The models are exposed as REST endpoints, and Oracle APEX has the all-powerful `APEX_WEB_SERVICE` API that lets developers consume REST services.\\n\\nA quick example of how I\'d submit source code submitted through an Oracle APEX page and returned an explanation of what the code is attempting to do:\\n\\n```sql\\ndeclare\\n  l_prompt varchar2(32767) := q\'[---\\n {{CODE}}\\n ---\\n Here\'s what the code does:\']\';\\n  l_request_body json_object_t;\\n  l_response clob;\\nbegin\\n  l_request_body := json_object_t();\\n\\n  l_request_body.put(\'model\', \'code-davinci-002\');\\n  l_request_body.put(\'temperature\', 0.40);\\n  l_request_body.put(\'prompt\', apex_escape.json(replace(l_prompt, \'{{CODE}}\', :P2_SOURCE_CODE)));\\n  l_request_body.put(\'stop\', \'---\');\\n  l_request_body.put(\'max_tokens\', 250);\\n  apex_debug.info(apex_escape.json(replace(l_prompt, \'{{CODE}}\', :P2_SOURCE_CODE)));\\n\\n  apex_web_service.set_request_headers(\\n    p_name_01 => \'Authorization\'\\n    , p_value_01 => \'Bearer averylongstringthatshouldnotbehardcoded\'\\n    , p_name_02 => \'OpenAI-Organization\'\\n    , p_value_02 => \'org-donothardcodethisaswell\'\\n    , p_name_03 => \'Content-Type\'\\n    , p_value_03 => \'application/json\'\\n  );\\n\\n  l_response := apex_web_service.make_rest_request(\\n    p_url => \'https://api.openai.com/v1/completions\'\\n    , p_http_method => \'POST\'\\n    , p_body => l_request_body.to_string()\\n  );\\n\\n  -- Code to parse the JSON response.\\nend;\\n```\\n\\nWe can do a lot of cool stuff in terms of functionality. However, these OpenAI models are also capable of providing a quick review of the code. However, it seems like the Codex model is fairly limited when it comes to PL/SQL. I had better results running this through GPT-3 to get a security and performance review of this pipelined table function:\\n\\n```sql\\ncreate or replace type typ_animal_names is table of varchar2(50)\\n/\\n\\ncreate or replace function find_animals_by_species(p_species_name varchar2)\\nreturn typ_animal_names\\n  pipelined\\nis\\nbegin\\n  for animal in (\\n    select animal_name from animal where species = p_species_name\\n  ) loop\\n    pipe row (animal.animal_name);\\n  end loop;\\nend;\\n/\\n```\\n\\nAnd at the end, I provided this prompt:\\n\\n```sql\\n-- Suggest security and performance improvements to this code.\\n---\\n```\\n\\nWhat do you think it would say? Perhaps I should have masked the outputs and asked [Steven Feuerstein](https://www.stevenfeuerstein.com/) for his opinion. He did [write](https://blogs.oracle.com/connect/post/pipelined-table-functions) about this topic quite some time ago. Anyways, here\'s what I got from GPT-3 after a few tries.\\n\\n```sql\\n/**\\nSecurity:\\n1. Add a check to ensure that the input parameter is valid and does not contain any malicious code.\\n2. Add a check to ensure that the user has the necessary permissions to execute the function.\\n\\nPerformance:\\n1. Create an index on the species column of the animal table.\\n2. Use bind variables for the input parameter to ensure that the query is not re-parsed each time it is executed.\\n3. Use a cursor to fetch the results from the query instead of looping through the results.\\n4. Use bulk collect to fetch the results from the query instead of looping through the results.\\n5. Use a PL/SQL table instead of a pipelined function*/\\n```\\n\\nIf you\'re interested, here are the parameters I had set:\\n\\n* **Model** - `text-davinci-003`\\n* **Temperature** - `0.3`\\n* **Maximum Length** - `150`\\n\\nIt would be nice to automate this as ChatGPT suggested, but I find the responses are not consistent, and sometimes, a little odd. That said, it would still be a good first attempt at scrutinizing the code. Consider it a helper when paired with a more experienced developer.\\n\\n## Pitting Against OCI AI Services\\n\\nI asked ChatGPT to:\\n\\n```\\nList all possible uses of OpenAI transformers.\\n```\\n\\nAnd here\'s the response I received:\\n\\n> 1. Natural Language Processing (NLP) tasks such as language translation, text summarization, and sentiment analysis.\\n> 1. Chatbots and virtual assistants for customer service and support.\\n> 1. Text-to-speech and speech-to-text applications.\\n> 1. Image and video captioning.\\n> 1. Language generation for creative writing and content creation.\\n> 1. Automated question answering systems.\\n> 1. Automatic text classification and tagging.\\n> 1. Text completion and prediction.\\n> 1. Named Entity Recognition (NER) and part-of-speech tagging.\\n> 1. Text-based search and retrieval systems.\\n> 1. Semantic similarity and relatedness measurements.\\n> 1. Personalization and recommendation systems.\\n> 1. Language understanding for intelligent agents and robots.\\n> 1. Text-based summarization of news articles and other long-form content.\\n> 1. Text-based sentiment analysis for social media monitoring and marketing.\\n\\nThe final topic I\'d like to explore is how OpenAI\'s product offerings compare to the services provided by [Oracle AI](https://www.oracle.com/artificial-intelligence/). And to do that, let\'s first attempt to prune the list of 15 use cases into these buckets:\\n\\n* Question and answer\\n* Content (text, images, and code) generation\\n* Text-to-speech\\n* Speech-to-text\\n* Named entity recognition\\n* Sentiment analysis\\n* Image and video captioning\\n* Language translation\\n* Text/document classification\\n* Keyword extraction and tagging\\n* Text summarization\\n\\n| Functionality                  | Language                                           | Speech                                             | Vision                                             | Document Understanding                             |\\n| ------------------------------ | -------------------------------------------------- | -------------------------------------------------- | -------------------------------------------------- | -------------------------------------------------- |\\n| Question and answer            | <i class=\\"rating fa-solid fa-circle-question\\"></i> | <i class=\\"rating fa-solid fa-circle-xmark\\"></i>    | <i class=\\"rating fa-solid fa-circle-xmark\\"></i>    | <i class=\\"rating fa-solid fa-circle-xmark\\"></i>    |\\n| Content generation             | <i class=\\"rating fa-solid fa-circle-xmark\\"></i>    | <i class=\\"rating fa-solid fa-circle-xmark\\"></i>    | <i class=\\"rating fa-solid fa-circle-xmark\\"></i>    | <i class=\\"rating fa-solid fa-circle-xmark\\"></i>    |\\n| Text-to-speech                 | <i class=\\"rating fa-solid fa-circle-xmark\\"></i>    | <i class=\\"rating fa-solid fa-circle-xmark\\"></i>    | <i class=\\"rating fa-solid fa-circle-xmark\\"></i>    | <i class=\\"rating fa-solid fa-circle-xmark\\"></i>    |\\n| Speech-to-text                 | <i class=\\"rating fa-solid fa-circle-xmark\\"></i>    | <i class=\\"rating fa-solid fa-circle-check\\"></i>    | <i class=\\"rating fa-solid fa-circle-xmark\\"></i>    | <i class=\\"rating fa-solid fa-circle-xmark\\"></i>    |\\n| Named entity recognition       | <i class=\\"rating fa-solid fa-circle-check\\"></i>    | <i class=\\"rating fa-solid fa-circle-xmark\\"></i>    | <i class=\\"rating fa-solid fa-circle-xmark\\"></i>    | <i class=\\"rating fa-solid fa-circle-xmark\\"></i>    |\\n| Sentiment analysis             | <i class=\\"rating fa-solid fa-circle-check\\"></i>    | <i class=\\"rating fa-solid fa-circle-xmark\\"></i>    | <i class=\\"rating fa-solid fa-circle-xmark\\"></i>    | <i class=\\"rating fa-solid fa-circle-xmark\\"></i>    |\\n| Image and video captioning     | <i class=\\"rating fa-solid fa-circle-question\\"></i> | <i class=\\"rating fa-solid fa-circle-question\\"></i> | <i class=\\"rating fa-solid fa-circle-question\\"></i> | <i class=\\"rating fa-solid fa-circle-question\\"></i> |\\n| Language translation           | <i class=\\"rating fa-solid fa-circle-check\\"></i>    | <i class=\\"rating fa-solid fa-circle-xmark\\"></i>    | <i class=\\"rating fa-solid fa-circle-xmark\\"></i>    | <i class=\\"rating fa-solid fa-circle-xmark\\"></i>    |\\n| Text/document classification   | <i class=\\"rating fa-solid fa-circle-check\\"></i>    | <i class=\\"rating fa-solid fa-circle-xmark\\"></i>    | <i class=\\"rating fa-solid fa-circle-xmark\\"></i>    | <i class=\\"rating fa-solid fa-circle-check\\"></i>    |\\n| Keyword extraction and tagging | <i class=\\"rating fa-solid fa-circle-check\\"></i>    | <i class=\\"rating fa-solid fa-circle-xmark\\"></i>    | <i class=\\"rating fa-solid fa-circle-xmark\\"></i>    | <i class=\\"rating fa-solid fa-circle-xmark\\"></i>    |\\n| Text summarization             | <i class=\\"rating fa-solid fa-circle-xmark\\"></i>    | <i class=\\"rating fa-solid fa-circle-xmark\\"></i>    | <i class=\\"rating fa-solid fa-circle-xmark\\"></i>    | <i class=\\"rating fa-solid fa-circle-xmark\\"></i>    |\\n\\nThe areas where Oracle AI is clearly lacking are in content generation and text summarization. As to \\"text-to-speech\\", it does not appear to be in the list of APIs that OpenAI has, so I wonder where that suggestion came from. Perhaps ChatGPT is pulling a fast one?\\n\\nSo, I thought I\'d ask ChatGPT the same question that I\'m trying to answer:\\n\\n```text\\nHow does Oracle AI compare with ChatGPT?\\n```\\n\\nAnd this is what it said:\\n\\n> Oracle AI and ChatGPT are both natural language processing (NLP) technologies, but they have different capabilities and are used for different purposes.\\n>\\n> ChatGPT is a transformer-based language model developed by OpenAI. It is primarily used for generating human-like text, such as in chatbots, virtual assistants, and language generation tasks. ChatGPT is trained on a large dataset of text, allowing it to generate text that is often indistinguishable from human-written text.\\n>\\n> Oracle AI, on the other hand, is a collection of AI-based products and services offered by Oracle Corporation. It includes several NLP-based technologies, such as natural language understanding, text analytics, and chatbot building. Oracle AI is designed to be used in enterprise environments and can be integrated with other Oracle products and services.\\n>\\n> In summary, ChatGPT is more focused on language generation, while Oracle AI is a broader set of AI-based products and services that can include NLP capabilities. Oracle AI is more focused on enterprise and business use cases, while ChatGPT is focused more on generating human-like text.\\n\\nApart from the first paragraph, I\'d say that\'s mostly factual and accurate. Hence, I can only conclude that it\'s a rather difficult comparison to make. Duh! My main goal here was really to highlight how we could also use Oracle AI to provide solutions for some of these use cases.\\n\\n## Summary\\n\\nCool as this technology might be, there\'s a nagging fear that pretty soon, IT practitioners, like myself, would be replaced by AI. I think that fear is valid. However, as Sau Sheong aptly describes ChatGPT, these are tools and we should embrace and empower ourselves to do our jobs faster and better. It was only a few years ago when Oracle launched the Autonomous Database product. Back [then](https://www.forbes.com/sites/oracle/2018/12/19/2018-the-year-the-database-went-autonomous/), it was touted to be intelligent and sent shivers down the DBAs\' spine. After almost four years, DBAs are still needed even when a company\'s running only ADBs in their environment. What it has done was take away some of the mundane tasks like database patching and upgrading (an introduced new ones) as advertised. Freeing us up to perform other functions, be innovative, and allowed us to discover other efficiencies and approaches to improve business operations.\\n\\nI have used ChatGPT in the past few days to help me in certain tasks such as writing a French version of an email, and constructing some of parts of this blog post. I still had to validate and adapt the outputs, but more importantly, like in the case of writing this blog post, compose the flow and piece them altogether for the final content. I don\'t think ChatGPT is quite ready to replicate my thought processes, feelings/opinions, and validate the story I wished to tell.\\n\\nAnd my last words are to thank the brilliant brains at OpenAI, Google, and all over the world for their contributions to computer science so that I can write this blog post with a bit more flavour. But please try not to make Judgement Day a reality. Thank you!\\n\\n:::info\\nPhoto by <a href=\\"https://unsplash.com/es/@lunarts?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText\\">Volodymyr Hryshchenko</a> on <a href=\\"https://unsplash.com/photos/D5_cfqMAY0Y?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText\\">Unsplash</a>\\n:::"},{"id":"id/2202","metadata":{"permalink":"/content/id/2202","source":"@site/posts/wrapping-up-2022/index.md","title":"Wrapping Up 2022","description":"Set Your Sights to Greater Heights","date":"2022-12-18T00:00:00.000Z","formattedDate":"December 18, 2022","tags":[{"label":"oracle","permalink":"/content/tags/oracle"},{"label":"community","permalink":"/content/tags/community"},{"label":"oracleace","permalink":"/content/tags/oracleace"},{"label":"getinvolved","permalink":"/content/tags/getinvolved"},{"label":"bethelight","permalink":"/content/tags/bethelight"}],"readingTime":7.855,"hasTruncateMarker":true,"authors":[{"name":"Adrian Png","title":"Senior Cloud Solutions Architect @ Insum","url":"https://github.com/fuzziebrain","imageURL":"https://github.com/fuzziebrain.png","key":"fuzziebrain"}],"frontMatter":{"title":"Wrapping Up 2022","tags":["oracle","community","oracleace","getinvolved","bethelight"],"slug":"id/2202","categories":["Technology","Community"],"authors":["fuzziebrain"],"date":"2022-12-18T00:00:00.000Z"},"prevItem":{"title":"I Am Obsolete","permalink":"/content/id/2301"},"nextItem":{"title":"Speak My Language","permalink":"/content/id/2201"}},"content":"![Set Your Sights to Greater Heights](./assets/./northern-lights-the-twenty-four.jpg)\\n\\nA few weeks ago, I was fortunate to accompany a group of twenty-four exchange students from a local university. Some of them came from countries as far as Singapore. While up in the North, the *Aurora Borealis* might be a common sight, for tourists, it is still pretty much a calculated risk that it might be a fruitless journey. The night skies were overcast on the first night that they arrived, and it was bitterly cold for folks used to tropical weather, but they persisted. On their second night, there was a minor emergency that delayed a few of them from heading out to the remote site for viewing the *Northern Lights*, but again, they were determined. Within minutes of their arrival at the viewing site, there was a sudden break in the cloudy skies that coincided with a display of greens and purples that illuminated in the heavens. What a treat!\\n\\nAs we near the end of 2022, I\'d like to reflect on the year\'s challenges and milestones.\\n\\n\x3c!--truncate--\x3e\\n\\n## Published a Second Book\\n\\nIn 2021, I had started work and finished two chapters for a would be *Pro Oracle Application Express* along with a few other community members. However, that book never materialized and was cancelled by the publisher. Not willing to throw away the work that I had done, I did the unthinkable of asking the publisher if I could complete a book under a very different title. One that would cover different [*Oracle Cloud Infrastructure*](https://www.oracle.com/cloud) (OCI) services that could be used in an [*Oracle Application Express*](https://apex.oracle.com) (APEX) application.\\n\\nMidway through the book, I was told that the esteemed [Heli Helskyaho](https://fi.linkedin.com/in/helihelskyaho) would be the book\'s technical reviewer. And since I had some chapters on integrating APEX with OCI AI Service and Machine Learning planned, I figured, why not ask if she would be more interested on being a co-author instead. That was the best decision ever! Heli\'s a great person to work with. She\'s dedicated, hardworking, and always optimistic. I don\'t think I would have been able to complete the book on time had she not gotten involved.\\n\\n![Book Cover](./assets/978-1-4842-8170-3.jpg)\\n\\nWe published the book [*Extending Oracle Application Express with Oracle Cloud Features*](https://link.springer.com/book/10.1007/978-1-4842-8170-3) in September 2022. Book projects are very demanding, and more often than not, would fail without strong external support. I am first of all very grateful to Heli for sharing this burden with me. I also want to thank [Jonathan Gennick](https://www.linkedin.com/in/gennick), who has since moved from Apress to Manning. Jonathan\'s a great editor to work with and very experienced. He\'s been a great guide for both the books I published through Apress, and will miss working with him.\\n\\nAnyone who actively writes on the OCI without company support, and **was** not an [Oracle ACE](https://ace.oracle.com) Director, would know that it is tough justifying the monetary costs of experimenting with its services. Fortunately for us, we had some support through [Guillermo Ruiz](https://linktr.ee/iaasgeek) and [Bo English-Wiczling](https://www.linkedin.com/in/boenglish). Not just with resources, but the access to Oracle Product Managers and early access to OCI Services. Thank you \\"G\\" and Bo.\\n\\nFinally, much thanks to [Karen Cannell](https://www.linkedin.com/in/karen-cannell-thtech) for replacing Heli as the book\'s technical reviewer. Thanks for taking time to read through all our chapters thoroughly, and offering many value advices and suggestion to improve the quality of the publication.\\n\\n## Travels and Conferences\\n\\nIt was stressful, but refreshing, to be travelling to conferences again. This year, I was fortunate to be at two: [Kscope22](https://kscope22.odtug.com) and [Oracle CloudWorld 2022](https://www.oracle.com/cloudworld/) (OCW22).\\n\\n### Kscope22\\n\\n![Final Approach to Dallas Fort Worth Internal Airport](./assets/kscope22-aerial-approach.jpeg)\\n\\nIt was almost customary to attend one of the regions most important annual technical conferences of the year, until the pandemic. After a two-year hiatus, we were once again able to gather to share knowledge, and meet old friends and colleagues. Here\'s the scene where Oracle ACEs were assembled for their annual photo op!\\n\\n<youtube youTubeId=\\"F2OgM7XzZfg\\" />\\n\\nThis year, I delivered two presentations: *Build Intelligent APEX Applications Now!* and *Packing Your Bags for a Journey to the Metaverse*. Both are topics that I am extremely passionate about. I am very grateful for everyone who attended my sessions, and the very flattering evaluation scores. One of which resulted in this very, very cool prize!\\n\\n![Award for Best Speaker in the Emerging Technologies track at Kscope22](./assets/kscope22-et-best-speaker.jpeg)\\n\\nI was also extremely glad that this year, we had more attendees from the Faculty of Medicine\'s [Digital Solutions](https://mednet.med.ubc.ca/resources/it/) at the [University of British Columbia](https://www.ubc.ca), and even had one of them present at OpenMic Night!\\n\\n![A representative from MedIT presenting at OpenMic Night](./assets/kscope22-medit-at-openmic-night.jpg)\\n\\nI\'m glad that APEX is gaining more traction at the university, not just for research, but in education and operations.\\n\\n### OCW22\\n\\nThe last time I attended OpenWorld was in 2014, before I joined [Insum](https://www.insum.ca). I miss San Francisco, but Las Vegas can be just as fun (./assets/and expensive). I still reminisce the village atmosphere around the Moscone Center though and smell of seawater.\\n\\nThis year, I fulfilled a lifelong dream: to speak at the conference!\\n\\n![OCW22 Speaker Badge](./assets/ocw22-speaker-badge.jpeg)\\n\\nSpecial thanks to the Oracle DevRel [team](https://developer.oracle.com/community/meet-the-devrel-team.html) for inviting me to present on *Solving Everyday Problems with Oracle Cloud*. It was pleasure delivering this session to a few great friends in the community, and I hope everyone enjoyed listening as much as I did preparing for the presentation and demos.\\n\\nAs with all conferences, it was also a great opportunity to meet other Oracle ACES, but most importantly, my co-author and mentor!\\n\\n![Wefie with Heli](./assets/ocw22-oracle-ace-dinner.jpeg)\\n\\n## Singapore\\n\\nPerhaps the most important trip I had to make this year was the one to Singapore in late August.\\n\\nIn late 2021, I was delighted that the company was ready to expand to the Asia Pacific (APAC) market, with our regional base in my home country, Singapore. The primary reason for our trip was to officiate and sign a Memorandum of Understanding (./assets/MoU) between [Nanyang Polytechnic](https://www.nyp.edu.sg) (NYP) and Insum.\\n\\n![MoU signed between NYP and Insum](./assets/nyp-mou.jpeg)\\n\\nWe also took the opportunity to introduce APEX and OCI to the local community through a half-day seminar with speakers from Oracle, NYP, and Insum.\\n\\n![Introducing APEX and OCI](./assets/orclapex-in-singapore-seminar.jpeg)\\n\\nIt was also a great opportunity to meet the NYP interns that we have trained and continue to train. Here\'s a very touching virtual meeting between the team in Singapore, and the past and present CEOs of the company.\\n\\n![Virtual team meeting at a shopping mall](./assets/saying-hi-in-singapore.jpeg)\\n\\nThere was also great team-building activities for the Singapore and Canadian leadership teams. Check this out:\\n\\n<youtube youTubeId=\\"Zv_xHRgwsvM\\" />\\n\\nFinally, a little learning lesson by observing the [All Nippon Airways](https://www.ana.co.jp) (ANA) crew members prepare the flight for takeoff and landing. It begins with a preflight huddle, and then on board, a systematic approach including pointing at the obvious, to ensure that the plane and passengers are safe. We need this dedication to task and non-assuming attitude when readying applications and infrastructure for production release.\\n\\n![A preflight huddle for crew members](./assets/ana-dedication-to-tasks.jpg)\\n\\n## Oracle ACE Director Award\\n\\nTo cap off a great year, I was deeply honoured to be appointed as an Oracle ACE Director.\\n\\n![Oracle ACE Director award announcement, graphic courtesy of the Oracle ACE Program](./assets/aced-award.jpg)\\n\\nI wanted again to express my deep gratitude to:\\n\\n* Heli for nominating me for the award, but more importantly, her friendship and mentorship. You have inspired me since the day I received an autographed copy of your book *Oracle SQL Developer Data Modeler for Database Design Mastery*.\\n* The Oracle peeps who supported my nomination: [Monica Godoy](https://blogs.oracle.com/authors/monica-godoy), [Chaitanya Koratamaddi](https://blogs.oracle.com/authors/chaitanya-koratamaddi), and [Joshua Chua](https://sg.linkedin.com/in/joshua-b-chua). Thank you for your confidence in me, and I look forward to working with you more closely in 2023 to spark greater interest in APEX and the OCI, most especially in the Asia Pacific region.\\n* [Opal Alapat](https://www.linkedin.com/in/opalalapat/), for nominating and starting my journey in the Oracle ACE program as an associate. I cannot thank you enough for how joining the program has transformed my life in many ways.\\n* [Martin D\'Souza](https://www.linkedin.com/in/martingiffydsouza), for getting me involved in community a decade ago. Wow, time flies by very quickly! Thank you for your friendship, encouragement, and always challenging me to go further, and further.\\n\\nIn 2023, I hope to pay it forward. If you are an Oracle technologist interested in community work and looking to join the Oracle ACE program, please do not hesitate to [reach out to me](https://mastodon.social/@fuzziebrain). I\'d be happy to share my experience and guide you through this journey.\\n\\n## The Northern Lights\\n\\nThe banner image for this post was from a 360 photo. If you are interested, here\'s a [link](https://photos.app.goo.gl/58qyJZRPF2rHwUGm7) to view it in its original format.\\n\\nA little thought from a First Nations tour guide about the incredible natural phenomenon:\\n\\n*\\"In our indigenous culture, when the Aurora is moving very fast, my grandmother told me that somebody on the other side is sending us a message saying that you don\'t need to be sad anymore. We had some good times. Think of those good times. We are going to meet again someday, but right now, don\'t be sad because I\'m over here, [and] you\'re there. Continue to live a good life while you\'re there, and we\'ll meet again someday.\\"* ~ Joe Buffalo Child [Source: [Meet the North](https://www.youtube.com/watch?v=mp7ucghevOw)]\\n\\nJoel, I missed you at all the events I attended this year. Meeting you in person has always been the highlight of my trips to Kscope and OpenWorld. I hope I have made you proud."},{"id":"id/2201","metadata":{"permalink":"/content/id/2201","source":"@site/posts/speak-my-language/index.md","title":"Speak My Language","description":"\\"Welcome\\" in multiple languages.","date":"2022-10-11T00:00:00.000Z","formattedDate":"October 11, 2022","tags":[{"label":"oracle cloud","permalink":"/content/tags/oracle-cloud"},{"label":"artificial intelligence","permalink":"/content/tags/artificial-intelligence"},{"label":"machine learning","permalink":"/content/tags/machine-learning"},{"label":"orclapex","permalink":"/content/tags/orclapex"},{"label":"oci","permalink":"/content/tags/oci"},{"label":"natural language processing","permalink":"/content/tags/natural-language-processing"}],"readingTime":3.725,"hasTruncateMarker":true,"authors":[{"name":"Adrian Png","title":"Senior Cloud Solutions Architect @ Insum","url":"https://github.com/fuzziebrain","imageURL":"https://github.com/fuzziebrain.png","key":"fuzziebrain"}],"frontMatter":{"title":"Speak My Language","tags":["oracle cloud","artificial intelligence","machine learning","orclapex","oci","natural language processing"],"slug":"id/2201","categories":["Technology","Infrastructure","Oracle Cloud"],"authors":["fuzziebrain"],"date":"2022-10-11T00:00:00.000Z"},"prevItem":{"title":"Wrapping Up 2022","permalink":"/content/id/2202"},"nextItem":{"title":"Will You Love Me Tomorrow","permalink":"/content/id/2014"}},"content":"![\\"Welcome\\" in multiple languages.](./assets/welcome-g84fb3b5fb_1280_853.jpg)\\n\\nIt\'s been too long since I last posted anything on my blog. Thanks to [Tim Hall](https://twitter.com/oraclebase), I am writing again, after a long pause, to celebrate [Joel Kallman Day 2022](https://oracle-base.com/blog/2022/09/27/joel-kallman-day-2022-announcement/). My contribution today is about my deep appreciation for the team behind the [Oracle Cloud Infrastructure](https://oracle.com/cloud) (OCI) [AI Services](https://www.oracle.com/artificial-intelligence/ai-services/). Below is a quick scoop on how I have used these services to improve on my [OCI-powered e-flashcard](https://developer.oracle.com/tutorials/learning-languages-with-oracle-cloud/) for learning new words in a different language. I will be doing a quick demonstration (session LRN3673) of this work at the upcoming [Oracle CloudWorld 2022](https://www.oracle.com/cloudworld/).\\n\\n\x3c!-- truncate --\x3e\\n\\n## The E-Flashcard Solution\\n\\nA **TL;DR** for my initial article on the e-flashcard solution.\\n\\nOn my fridge, I have a [M5Paper](https://shop.m5stack.com/collections/m5-controllers/products/m5paper-esp32-development-kit-v1-1-960x540-4-7-eink-display-235-ppi) that pulls a random English word or phrase from an [Oracle Autonomous Database](https://www.oracle.com/autonomous-database/). The word/phrase and the Japanese translation are entered using an [Oracle Application Express](https://apex.oracle.com/) (APEX) and exposed as a REST service using [Oracle REST Data Services](https://www.oracle.com/database/technologies/appdev/rest.html). The translation task was performed manually using a tool like [Google Translate](https://translate.google.com).\\n\\nThen [Dimitri Gielis](https://twitter.com/dgielis) made this suggestion:\\n\\n![Source: https://twitter.com/dgielis/status/1358781075879845888](./assets/dgielis-tweet.png)\\n\\nThe second version of my implementation isn\'t quite driven by [Amazon Alexa](https://developer.amazon.com/alexa) yet, but at least for the web interface, I had added a voice input functionality that allowed me to speak a word and translate it to a language of my choosing.\\n\\n> Voice => APEX => Speech Service => Language Service => DB => ORDS => M5Paper\\n\\n## Speech Service\\n\\nReleased in early 2022, the [Speech Service](https://www.oracle.com/artificial-intelligence/speech/) is one of several OCI\'s AI Services that uses Automatic Speech Recognition (ASR) technology to transcribe audio content to text. Initially, the service was very limited in what audio formats it received, specifically, it had to be single channel at a sample rate of 16,000 Hz, encoded in [PCM](https://wikipedia.org/wiki/Pulse-code_modulation), and saved in the [WAV](https://wikipedia.org/wiki/WAV) format.\\n\\nThe strict requirements made it difficult to implement using a purely web-based interface. One would have to capture the audio using the [Media Recorder API](https://developer.mozilla.org/docs/Web/API/MediaRecorder) and any of its [supported codecs](https://developer.mozilla.org/en-US/docs/Web/Media/Formats/codecs_parameter), transcode it using [FFmpeg](https://ffmpeg.org/), executed using [Oracle Cloud Functions](https://www.oracle.com/cloud/cloud-native/functions/), and then submitted a Speech transcription job.\\n\\nI then looked at other solutions that used Web Assembly to perform the audio format purely in the browser, such as [Opus MediaRecorder](https://kbumsik.io/opus-media-recorder/) and [FFMPEG.WASM](https://ffmpegwasm.netlify.app/). Unfortunately, I ran out of time, got busy, and had to put this project on hold.\\n\\nThankfully, months later, a newer version of the service was [released](https://blogs.oracle.com/ai-and-datascience/post/oci-speech-now-supports-seven-new-languages-and-additional-input-formats). Apart from the new languages supported, it also allowed users to submit audio files in a other formats, several of which are supported by the Media Recorder API. Hurrah!\\n\\nI kept most of the initial prototype and updated the *Ajax Callback*.\\n\\n![Recording audio through an APEX modal page.](./assets/audio-record-ui.png)\\n\\nI will avoid making this post unreadable by not listing the source code of the PL/SQL procedure I wrote, but here\'s the pseudocode:\\n\\n1. Upload the recorded audio as a BLOB and store it in OCI [Object Storage](https://www.oracle.com/cloud/storage/object-storage/).\\n1. Create a transcription job.\\n1. If successful, query the transcription job using the job\'s [OCID](https://docs.oracle.com/iaas/Content/General/Concepts/identifiers.htm).\\n1. Loop until the job state is either `SUCCEEDED`, `FAILED`, or `CANCELED`.\\n1. If the state is `SUCCEEDED`, then get the transcription job\'s task and get the location of the results stored as an object in the Object Storage.\\n1. Retrieve the transcription results stored as JSON.\\n\\n## Language Service \\"V2\\"\\n\\nIn Chapter 4 of the book [\\"Extending Oracle Application Express with Oracle Cloud Features \\"](https://link.springer.com/book/10.1007/978-1-4842-8170-3), I describe how you can host and use a language translation service using a few OCI resources and an open sourced engine like [LibreTranslate](https://libretranslate.com/). Before getting limited access to the next generation of Language Service, my solution looked like this:\\n\\n![Initial approach to language translation.](./assets/translation-past.png)\\n\\nSince then, I have abandoned all of that for a simplified solution using the Language pre-trained models for text translation. As there is limited availability, so is the documentation. Fortunately, the service also comes with an OCI Console-based user interface to try out the service. With that, I was able to figure out what I needed to call the service in the APEX application through REST. Believe me, it\'s not difficult.\\n\\nTo the amazing people behind these products. Thank you very much.\\n\\n\u3053\u308c\u3089\u306e\u88fd\u54c1\u306e\u80cc\u5f8c\u306b\u3042\u308b\u7d20\u6674\u3089\u3057\u3044\u4eba\u3005\u3078\u3002\u672c\u5f53\u306b\u3042\u308a\u304c\u3068\u3046\u3002(*Transcribed and translated by OCI*)\\n\\n> <u>Photo Credits</u>\\n>\\n> This post\'s banner photo is by <a href=\\"https://pixabay.com/users/tumisu-148124/\\">Tumisu</a> on <a href=\\"https://pixabay.com/illustrations/welcome-words-greeting-language-905562/\\">Pixabay</a>."},{"id":"id/2014","metadata":{"permalink":"/content/id/2014","source":"@site/posts/will-you-love-me-tomorrow/index.md","title":"Will You Love Me Tomorrow","description":"Crystal ball insights into tomorrow?","date":"2021-10-17T00:00:00.000Z","formattedDate":"October 17, 2021","tags":[{"label":"odtug","permalink":"/content/tags/odtug"},{"label":"kscope","permalink":"/content/tags/kscope"},{"label":"kscope22","permalink":"/content/tags/kscope-22"},{"label":"oracle","permalink":"/content/tags/oracle"},{"label":"future","permalink":"/content/tags/future"},{"label":"abstract","permalink":"/content/tags/abstract"},{"label":"speak","permalink":"/content/tags/speak"},{"label":"presentation","permalink":"/content/tags/presentation"},{"label":"community","permalink":"/content/tags/community"},{"label":"emerging technology","permalink":"/content/tags/emerging-technology"},{"label":"digital transformation","permalink":"/content/tags/digital-transformation"},{"label":"lifelong learning","permalink":"/content/tags/lifelong-learning"}],"readingTime":2.15,"hasTruncateMarker":true,"authors":[{"name":"Adrian Png","title":"Senior Cloud Solutions Architect @ Insum","url":"https://github.com/fuzziebrain","imageURL":"https://github.com/fuzziebrain.png","key":"fuzziebrain"}],"frontMatter":{"title":"Will You Love Me Tomorrow","tags":["odtug","kscope","kscope22","oracle","future","abstract","speak","presentation","community","emerging technology","digital transformation","lifelong learning"],"slug":"id/2014","authors":["fuzziebrain"],"date":"2021-10-17T00:00:00.000Z","categories":["Technology","Conferences"]},"prevItem":{"title":"Speak My Language","permalink":"/content/id/2201"},"nextItem":{"title":"Secret Agent APEX","permalink":"/content/id/2013"}},"content":"![Crystal ball insights into tomorrow?](./assets/drew-beamer-xU5Mqq0Chck-1280x800.png)\\n\\n> Is this a lasting treasure, or just a moment\'s pleasure?\\n>\\n> ~ *Will You Love Me Tomorrow*. The Shirelles.\\n\\nAs a technologist, we are constantly bombarded with buzzwords and terms like blockchain, IoT, AI, machine learning, quantum computing, and so much more! I love buzzwords, and wholeheartedly enjoy deciphering the future of technology and work. Hence, for the last few years, I have been a proud member of the committee that peruses and selects abstracts for the Kscope Emerging Technology (ET) track. For next year\'s [Kscope22](https://kscope22.odtug.com), I am honoured to be leading the team that will help select a series of sessions that are visionary, and hopefully, give us guidance on what to expect in the years ahead.\\n\\n\x3c!-- truncate --\x3e\\n\\nFor the last few Kscopes, ET sessions generally fell under this selection criteria:\\n\\n1. About the following subject(s):\\n    * Artificial Intelligence and Machine Learning\\n    * Blockchain\\n    * Internet of Things (IoT)\\n    * Serverless Computing\\n    * Chatbots\\n1. Involve Oracle technology\\n\\nToday, many of these exciting technologies have gone mainstream, and so, we have decided to give the ET track a facelift. For next year\'s conference, we have decided to focus on new subject areas. They are:\\n* AI Engineering\\n* AIOps\\n* Digital Twins\\n* Distributed Cloud\\n* General Advancements in Technology\\n* Hyperautomation\\n* Internet of Behaviours\\n* Next-Generate Security\\n* Quantum Computing\\n* Sustainability\\n* Total Experience\\n\\nYou can find this list and a brief description of each theme [here](https://kscope22.odtug.com/page/kscope22-tracks#emergingtech).\\n\\nI should also emphasize that the use of Oracle technology is **NOT** required. Sure it would be nice that it does, but it is important that we look beyond our comfort zones. If you are not an Oracle professional but have something novel to share, then I strongly encourage you to submit an abstract. This is also a great opportunity for researchers, post-docs, and post-graduate students to share your research. Getting a talk accepted means a *FREE* ticket (air tickets and [luxurious accommodation](https://kscope22.odtug.com/page/kscope22-location) are not included) to attend one of the top tech conferences in the world. And for those of you who are, go through your address books and LinkedIn connections lists, and spread the word. Invite a luminary to join us in Texas next year! Or, if you prefer, you may send [me](https://twitter.com/fuzziebrain) suggestions *via* Twitter.\\n\\nStay healthy and enjoy this rendition of the song *Will You Love Me Tomorrow* by one of my favourite songwriters, [Carole King](https://wikipedia.org/wiki/Carole_King).\\n\\n<youtube youTubeId=\\"TxWrUtbzlLM\\" />\\n\\n\\n> <u>Photo Credits</u>\\n>\\n> This post\'s banner photo is by <a href=\\"https://unsplash.com/@drew_beamer?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText\\">Drew Beamer</a> on <a href=\\"https://unsplash.com/s/photos/future?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText\\">Unsplash</a>."},{"id":"id/2013","metadata":{"permalink":"/content/id/2013","source":"@site/posts/secret-agent-apex/index.md","title":"Secret Agent APEX","description":"Ssshhh! Do not tell anyone!","date":"2021-10-11T13:30:00.000Z","formattedDate":"October 11, 2021","tags":[{"label":"oracle","permalink":"/content/tags/oracle"},{"label":"database","permalink":"/content/tags/database"},{"label":"encryption","permalink":"/content/tags/encryption"},{"label":"signature","permalink":"/content/tags/signature"},{"label":"security","permalink":"/content/tags/security"},{"label":"communication","permalink":"/content/tags/communication"},{"label":"secret","permalink":"/content/tags/secret"},{"label":"plsql","permalink":"/content/tags/plsql"},{"label":"orclapex","permalink":"/content/tags/orclapex"}],"readingTime":5.3,"hasTruncateMarker":true,"authors":[{"name":"Adrian Png","title":"Senior Cloud Solutions Architect @ Insum","url":"https://github.com/fuzziebrain","imageURL":"https://github.com/fuzziebrain.png","key":"fuzziebrain"}],"frontMatter":{"title":"Secret Agent APEX","tags":["oracle","database","encryption","signature","security","communication","secret","plsql","orclapex"],"slug":"id/2013","authors":["fuzziebrain"],"categories":["Technology","Programming","Oracle Application Express"],"date":"2021-10-11T13:30:00.000Z"},"prevItem":{"title":"Will You Love Me Tomorrow","permalink":"/content/id/2014"},"nextItem":{"title":"Intelligent APEX Applications - Fact or Fiction?","permalink":"/content/id/2012"}},"content":"![Ssshhh! Do not tell anyone!](./assets/kristina-flour-BcjdbyKWquw-1280x800.png)\\n\\nFor a long time now, I have been eagerly awaiting for out-of-the-box [RSA encryption](https://wikipedia.org/wiki/RSA_(cryptosystem)) support with pure PL/SQL. Not so much for to sharing secrets, but rather, the ability to sign messages and verify signatures. I might have missed the announcements, but it looks like my wait is over support with asymmetric key operations added in 21c, and (perhaps backported) in 19c as-of release 19.9.\\n\\n\x3c!-- truncate --\x3e\\n\\n## #JoelKallman Day\\n\\nBefore we continue though, along with many other members of the Oracle community, I\'d like to ask that you take a one-minute pause to remember [Joel Kallman](https://twitter.com/joelkallman) on this day. Joel was a wonderful man who was not only passionate about his work, but was even more devoted to uplifting others. He was taken away from us too soon, but the best way to move forward, is to continue his work and mould ourselves with his legacy.\\n\\n## Secret Story Continued...\\n\\nBefore this release, supporting RSA encryption typically involved installing a Java package and writing some custom PL/SQL code. With Oracle Database 19.9 and 21.x, The updated `DBMS_CRYPTO` package now includes four new functions:\\n* `PKENCRYPT`\\n* `PKDECRYPT`\\n* `SIGN`\\n* `VERIFY`\\n\\nBy default, new schemas do not have the appropriate privileges to execute the `DBMS_CRYPTO` package, so please kindly request it from your DBA. The grant statement that you will need is:\\n\\n```sql\\ngrant execute on dbms_crypto to jenglish;\\n```\\n\\nTo demonstrate how to use these new PL/SQL APIs, I have created a simple [Oracle Application Express](https://apex.oracle.com) (APEX) application that users will:\\n\\n1. Load a private key.\\n1. Decrypt a message that was signed by the receiver\'s public key.\\n1. Load the sender\'s public key and a base64-encoded signature signed using the sender\'s private key. The sender\'s public key will be used to verify the signature.\\n\\n### Generate the Keys\\n\\nFor secure and non-repudiable communications, participants in the conversation would each generate a private-public key pair, and **only** exchange their public keys. To create the key pair, the common tool used is [OpenSSL](https://www.openssl.org/). These days, you can find the application already installed in Linux, Mac, and Windows. For me, Alice loves working with Bash and so here\'s the command that she can use to generate her private key:\\n\\n```bash\\nopenssl genrsa -out alice_key.pem 2048\\n```\\n\\nAnd then followed by a command to get her public key to be shared with her peers:\\n\\n```bash\\nopenssl rsa -in alice_key.pem -outform PEM -pubout -out alice_key_public.pem\\n```\\n\\nThe private key should look somewhat like this:\\n\\n```\\n-----BEGIN RSA PRIVATE KEY-----\\nMIIEogIBAAKCAQEAy/wiWejt+fKwrT7NHTTEeKuWrWqC4KwzpjWUSCK0zWh832xL\\njnrZuJc6Sl8gmMz6/p8p6T2NrJEBIda1EbjAD/W4G7IUykf5LrlICEJ3miJV33WL\\nwpFaFI6wmIas2PpodYhMqA0HngEMTP4oQS4AHnVuxn2naijzeRa0HbG55pAr2K1J\\n...\\n/bneFDA7jyv3bbOujCkk3DpaBjHXunqz5JmAJ6JqqMhzCQwRQ7c=\\n-----END RSA PRIVATE KEY-----\\n```\\n\\nAnd the public key:\\n\\n```\\n-----BEGIN PUBLIC KEY-----\\nMIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAy/wiWejt+fKwrT7NHTTE\\neKuWrWqC4KwzpjWUSCK0zWh832xLjnrZuJc6Sl8gmMz6/p8p6T2NrJEBIda1EbjA\\nD/W4G7IUykf5LrlICEJ3miJV33WLwpFaFI6wmIas2PpodYhMqA0HngEMTP4oQS4A\\nHnVuxn2naijzeRa0HbG55pAr2K1J+R/o5hOp460iEbP0HWM16QBwI2TTPfq0R+fz\\nKE2EA72g/gbttWSUdRZE/ZdDzG1ZE06RAfu7bNP7GQqGcyMTJ+bv3zWkYlMeDSQD\\n5foCzTHrcDomRL9H/snPSfMpnOzn8xwndZcwBE1ydy852NJatIRbPitglgNMKzfi\\n8QIDAQAB\\n-----END PUBLIC KEY-----\\n```\\n\\nDo the same for Charlie.\\n\\n### Stripping the Keys\\n\\nIn the early days of [Oracle Cloud Infrastructure](https://oracle.com/cloud/) [REST API](https://docs.cloud.oracle.com/iaas/api/) support in APEX, you might recall from this [article](https://blogs.oracle.com/oraclemagazine/post/better-file-storage-in-oracle-cloud), that we had to remove the enclosing tags and linefeeds. While this is no longer required when working with APEX Web Credentials, it\'s still necessary when working with the `DBMS_CRYPTO` API. Since we are going to need this a lot, I decided to follow the [DRY](https://wikipedia.org/wiki/Don%27t_repeat_yourself) principle and coded a function to strip these unwanted characters from any RSA private or public keys entered.\\n\\n```sql\\ncreate or replace function strip_rsa_key(p_rsa_key in varchar2)\\nreturn varchar2\\nas\\nbegin\\n  return regexp_replace(\\n    p_rsa_key\\n    ,\'(-+((BEGIN|END) (RSA )?(PUBLIC|PRIVATE) KEY)-+\\\\s?|\\\\s)\'\\n    , \'\');\\nend;\\n/\\n```\\n\\n### Secret Agent APEX\\n\\nI create a simple APEX application with two pages. Page 1 includes the workflow for steps 2 and 3. To avoid displaying the private key once set, a dedicated page 2 was created to provide that information.\\n\\n\\n### Load a Private Key\\n\\nAlice loves Bash, but she happens to own a Windows machine, so she often relies on Windows Subsystem for Linux v2 (WSL2). In Windows, there\'s also a helpful utility to copy text to the clipboard. The following command copies Alice\'s private key to her clipboard:\\n\\n```bash\\nclip.exe < alice_key.pem\\n```\\n\\nShe then pastes the clipboard contents into the textarea on page 2.\\n\\n![Load private key page](./assets/load-private-key.png)\\n\\nThis should activate the *Decrypt* button back on page 1.\\n\\n### Decrypt the Secret Message\\n\\nThe secret message that Charlie is sending to Alice will be encrypted using her public key. At the moment, the API appears to only support RSA Public Key Cryptosystem with PKCS1 and OAEP padding. So, if I am not mistaken, Alice should be able to generate the encrypted message using the following OpenSSL command:\\n\\n```bash\\necho -n \\"Bob has no keys\\" | openssl rsautl -pubin -inkey alice_key_public.pem \\\\\\n  -encrypt -oaep | openssl base64\\n```\\n\\nUnfortunately, she couldn\'t get it to work at the time of writing. If you had any pointers, please feel free to comment below. For now, to continue, Alice generated the encrypted message using the function `DBMS_CRYPTO.PKENCRYPT`:\\n\\n```sql\\ndeclare\\n  l_input_string varchar2(100) := \'Bob has no keys\';\\n\\n  l_public_key raw(4000);\\n  l_encrypted_output raw(4000);\\nbegin\\n  l_public_key :=\\n    utl_i18n.string_to_raw(strip_rsa_key(q\'[\\n-----BEGIN PUBLIC KEY-----\\nMIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAy/wiWejt+fKwrT7NHTTE\\neKuWrWqC4KwzpjWUSCK0zWh832xLjnrZuJc6Sl8gmMz6/p8p6T2NrJEBIda1EbjA\\nD/W4G7IUykf5LrlICEJ3miJV33WLwpFaFI6wmIas2PpodYhMqA0HngEMTP4oQS4A\\nHnVuxn2naijzeRa0HbG55pAr2K1J+R/o5hOp460iEbP0HWM16QBwI2TTPfq0R+fz\\nKE2EA72g/gbttWSUdRZE/ZdDzG1ZE06RAfu7bNP7GQqGcyMTJ+bv3zWkYlMeDSQD\\n5foCzTHrcDomRL9H/snPSfMpnOzn8xwndZcwBE1ydy852NJatIRbPitglgNMKzfi\\n8QIDAQAB\\n-----END PUBLIC KEY-----\\n]\'));\\n\\n  l_encrypted_output :=\\n    dbms_crypto.pkencrypt(\\n      src => utl_i18n.string_to_raw(l_input_string)\\n      , pub_key => l_public_key\\n      , pubkey_alg => dbms_crypto.key_type_rsa\\n      , enc_alg => dbms_crypto.pkencrypt_rsa_pkcs1_oaep\\n    );\\n\\n  dbms_output.put_line(utl_i18n.raw_to_char(utl_encode.base64_encode(l_encrypted_output)));\\nend;\\n/\\n```\\n\\nHere\'s an example of the base64-encoded, encrypted output:\\n\\n```\\nWb3KKyyu+8/MxRKn/kIoqAt4ZMsSvE7AZ/Gr+/3x+xHedtxvj504sy7pffIT1BTl\\nAbvVdjpcZ+VsxNpYCeN3Q/47R51YKPu/2P5u8FiY+nOLPu+sN5hQc4a6WdHVvRL/\\nwvXOhSx0fV6M4OtAO7KMuNrrRCqBlOCXPkIIHMeReAq1YVyVuRrofuDptznuEiOo\\nMKIt6s0j95n9RGCNWC3dI3Rsr+Nm64psjtWPVxCFjMUCSqh4fZcFXXW8xdCsHxYw\\n0zw43LrD1REnUbUCXAeNOhVXIV5f5TxRYwW9ibkjPTWeDBtViIy33XTvCleEmf7K\\n8kMkShUD3gfQt2HJ3FSmNQ==\\n```\\n\\nRunning this through the secret agent\'s cryptopad with Alice\'s private key loaded, it appears that she has successfully mission #2!\\n\\n![Message decrypted!](./assets/message-decrypted.png)\\n\\n### Verify the Sender\\n\\nBut how can Alice know that Charlie was truly the person who had sent this message. For that, we will rely on Charlie signing his message (in this case, the unencrypted text), and send the digital digest along with the message payload. Again, you can do this with either OpenSSL, `DBMS_CRYPTO.SIGN`, or any other programming language that supports public key encryption. Here\'s an example OpenSSL command that Charlie can use to generate the digital signature:\\n\\n```bash\\necho -n \\"Bob has no keys\\" | openssl dgst -sha256 -sign charlie_key.pem | \\\\\\n    openssl base64\\n```\\n\\nAlong with his Charlie, Alice can now verify that he was indeed the sender of the message \\"Bob has no keys\\".\\n\\n![Digital signature verified.](./assets/verified-signature.png)\\n\\n\\n### Summary\\n\\nI am very excited! There are more practical uses for this API, and you can definitely expect a follow up post. Stay tuned!\\n\\n> <u>Photo Credits</u>\\n>\\n> This post\'s banner photo is by <a href=\\"https://unsplash.com/@tinaflour?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText\\">Kristina Flour</a> on <a href=\\"https://unsplash.com/s/photos/secret-agent?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText\\">Unsplash</a>"},{"id":"id/2012","metadata":{"permalink":"/content/id/2012","source":"@site/posts/intelligent-apex-applications-fact-or-fiction/index.md","title":"Intelligent APEX Applications - Fact or Fiction?","description":"Neurons","date":"2021-07-04T16:45:00.000Z","formattedDate":"July 4, 2021","tags":[{"label":"oracle cloud","permalink":"/content/tags/oracle-cloud"},{"label":"orclapex","permalink":"/content/tags/orclapex"},{"label":"artificial intelligence","permalink":"/content/tags/artificial-intelligence"},{"label":"ai","permalink":"/content/tags/ai"},{"label":"machine learning","permalink":"/content/tags/machine-learning"},{"label":"ml","permalink":"/content/tags/ml"},{"label":"rest","permalink":"/content/tags/rest"}],"readingTime":8.17,"hasTruncateMarker":true,"authors":[{"name":"Adrian Png","title":"Senior Cloud Solutions Architect @ Insum","url":"https://github.com/fuzziebrain","imageURL":"https://github.com/fuzziebrain.png","key":"fuzziebrain"}],"frontMatter":{"title":"Intelligent APEX Applications - Fact or Fiction?","tags":["oracle cloud","orclapex","artificial intelligence","ai","machine learning","ml","rest"],"slug":"id/2012","authors":["fuzziebrain"],"categories":["Technology","Programming","Oracle Application Express"],"date":"2021-07-04T16:45:00.000Z"},"prevItem":{"title":"Secret Agent APEX","permalink":"/content/id/2013"},"nextItem":{"title":"ARM Your APEX Instance","permalink":"/content/id/2011"}},"content":"![Neurons](./assets/neurons-1773922_1280x800.jpg)\\n\\nDuring a recent [Kscope21](https://kscope21.odtug.com/) session, \\"[Predict Customer Churn Using Oracle Autonomous Database, Machine Learning, and APEX](https://kscope21.odtug.com/e/in/eid=42&req=info&s=3444&all=1)\\", I had shared that it has always been my personal desire to create web applications that are not only purposeful, but also have an element of \\"intelligence\\". With new features recently introduced in the [Oracle Machine Learning](https://www.oracle.com/data-science/machine-learning/) product offering, it is now relatively easy to introduce ML as a component of your [Oracle Application Express](https://apex.oracle.com) (APEX) applications. This is the first of several posts that I am writing as a follow up to the presentations I did at this year\'s (virtual) conference.\\n\\n\x3c!-- truncate --\x3e\\n\\nThe whys and how we created our first Machine Learning (ML) model using [AutoML UI](https://blogs.oracle.com/machinelearning/introducing-oml-automl-user-interface) is detailed in our presentation, so I highly encourage you to review the presentation to get the required background information. There are also several tutorials that Oracle has published to help you get going with the technology. In this blog post, the goal is to provide details on code needed to consume the ML model that we operationalised using OML Services. these are:\\n\\n* [How to authenticate with the REST services](#Authentication)\\n* [Identifying the REST service\'s inputs and outputs](#Inputs-and-Outputs); and finally\\n* [Using the Deployed Model](#using-the-Deployed-Model)\\n\\nAlso, it would be worthwhile pointing out that AutoML, OML Services, and many of the other Oracle technologies described in this post are only available on the [Oracle Autonomous Database](https://www.oracle.com/autonomous-database/) (ADB) at this time, including the two *Always Free* instances that you get with every Oracle Cloud account.\\n\\n## Authentication\\n\\nTo secure OML Service endpoints, clients must use an OML Service account and obtain an *access token*. You may create this user and enable access to OML Services using either the *Oracle ML User Administration* console or *Database Actions*.\\n\\nThe authentication element consists of:\\n1. Two application items `G_TOKEN` and `G_TOKEN_EXPIRY` that are used to store the access token and expiry measured in epochs.\\n1. An on page load (before header) application process with the following PL/SQL code:\\n    ```sql\\n    if :G_TOKEN is null\\n      or :G_TOKEN_EXPIRY is null\\n      or pkg_omlservices_util.f_convert_to_epoch(sysdate) > :G_TOKEN_EXPIRY\\n    then\\n      pkg_omlservices_util.p_authenticate(\\n        p_token_url => :G_TOKEN_URL\\n        , p_username => \'readmore\'\\n        , p_password => pkg_secrets.gc_password\\n        , p_token => :G_TOKEN\\n        , p_expiry_epoch => :G_TOKEN_EXPIRY\\n      );\\n    end if;\\n    ```\\n1. The application process calls a custom procedure `pkg_omlservices_util.p_authenticate` that embodies the following code:\\n    ```sql\\n    create or replace package body pkg_omlservices_util\\n    as\\n      ...\\n\\n      procedure p_authenticate(\\n        p_token_url in varchar2\\n        , p_username in varchar2\\n        , p_password in varchar2\\n        , p_token out varchar2\\n        , p_expiry_epoch out number\\n      )\\n      as\\n        l_response clob;\\n      begin\\n        apex_web_service.g_request_headers(1).name := \'Content-Type\';\\n        apex_web_service.g_request_headers(1).value := \'application/json\';\\n\\n        l_response := apex_web_service.make_rest_request(\\n          p_url => p_token_url\\n          , p_http_method => \'POST\'\\n          , p_body => json_object(\\n              \'grant_type\' value \'password\'\\n              , \'username\' value p_username\\n              , \'password\' value p_password\\n            )\\n        );\\n\\n        if apex_web_service.g_status_code = 200 then\\n          apex_debug.info(l_response);\\n          apex_json.parse(p_source => l_response);\\n\\n          p_token := apex_json.get_varchar2(p_path => \'accessToken\');\\n          p_expiry_epoch := pkg_omlservices_util.f_convert_to_epoch(sysdate)\\n            + apex_json.get_number(p_path => \'expiresIn\');\\n        else\\n          raise_application_error(-20001, \'Authentication request failed. \'\\n            || \'HTTP status code returned: \'\\n            || apex_web_service.g_status_code);\\n        end if;\\n      end p_authenticate;\\n    end pkg_omlservices_util;\\n    ```\\n\\nThe URL to direct the access token request to is set and passed using the APEX substitution string `G_TOKEN_URL`. This URL is constructed based on a few components unique to your environment. You will need the following information:\\n\\n* `REGION_CODE`. For example, `us-ashburn-1`.\\n* `TENANT_OCID`. Looks like this: `ocid1.tenancy.oc1..aaaaaaaa...`.\\n* `DATABASE_NAME`. The database name given to your ADB instance.\\n\\nThen, use the template below and replace the placeholders with the appropriate values.\\n\\n```\\nhttps://adb.<REGION_CODE>.oraclecloud.com/omlusers/tenants/<TENANT_OCID>/databases/<DATABASE_NAME>/api/oauth2/v1/token\\n```\\n\\nThis approach helps maintain a valid access token for as long as it is not expired, and will request a new one only when it does.\\n\\n## Inputs and Outputs\\n\\nDuring the presentation, I had demonstrated how you can use the OML AutoML User Interface (UI) to create ML models with no code. Once you are satisfied with the predicted model, you can then deploy the model thus exposing it through OML Services as a REST endpoint. To consume the REST service, there are three essential information that we will require:\\n\\n1. The REST endpoint URL.\\n1. The request\'s input parameters and how to transmit them.\\n1. The response\'s expected output format.\\n\\nYou can access this information from the metadata published after the models are deployed.\\n\\n![List of Deployed ML Models](./assets/inputs-and-outputs.png)\\n\\nThe endpoint URL can be derived from the *Open API Specification* for the service that you can access by clicking the link labelled \\"A\\". Below is shortened version of the JSON output to help highlight the important pieces of information.\\n\\n```json\\n{\\n  \\"openapi\\": \\"3.0.1\\",\\n  \\"info\\": {\\n    \\"title\\": \\"svml_9223d64159\\",\\n    \\"version\\": \\"1.0\\"\\n  },\\n  ...\\n\\n  \\"paths\\": {\\n    \\"/svml/score\\": {\\n      \\"post\\": {\\n        \\"operationId\\": \\"scoreModel\\",\\n        \\"requestBody\\": {\\n          \\"content\\": {\\n            \\"application/json\\": {\\n              \\"schema\\": {\\n                \\"$ref\\": \\"#/components/schemas/svml_9223d64159_INPUT_TYPE\\"\\n              }\\n            }\\n          },\\n          \\"required\\": true\\n        },\\n        \\"responses\\": {\\n          \\"200\\": {\\n            \\"description\\": \\"Successfully scored model svml.\\",\\n            \\"content\\": {\\n              \\"application/json\\": {\\n                \\"schema\\": {\\n                  \\"$ref\\": \\"#/components/schemas/svml_9223d64159_OUTPUT_TYPE\\"\\n                }\\n              }\\n            }\\n          },\\n          ...\\n        }\\n      }\\n    }\\n  },\\n  \\"components\\": {\\n    \\"schemas\\": {\\n      \\"svml_9223d64159_INPUT_TYPE\\": {\\n        \\"properties\\": {\\n          \\"inputRecords\\": {\\n            \\"type\\": \\"array\\",\\n            \\"items\\": {\\n              \\"$ref\\": \\"#/components/schemas/Attributes\\"\\n            }\\n          },\\n          \\"topN\\": {\\n            \\"type\\": \\"integer\\",\\n            \\"format\\": \\"int32\\"\\n          },\\n          \\"topNdetails\\": {\\n            \\"type\\": \\"integer\\",\\n            \\"format\\": \\"int32\\"\\n          }\\n        }\\n      },\\n      \\"Attributes\\": {\\n        \\"properties\\": {\\n          \\"AVG_TOTAL_GROUP_READING_TIME\\": {\\n            \\"type\\": \\"number\\",\\n            \\"format\\": \\"double\\"\\n          },\\n          \\"DASHBOARD_VISITS\\": {\\n            \\"type\\": \\"number\\",\\n            \\"format\\": \\"double\\"\\n          },\\n          \\"MAX_TOTAL_GROUP_READING_TIME\\": {\\n            \\"type\\": \\"number\\",\\n            \\"format\\": \\"double\\"\\n          },\\n          \\"MIN_TOTAL_GROUP_READING_TIME\\": {\\n            \\"type\\": \\"number\\",\\n            \\"format\\": \\"double\\"\\n          },\\n          \\"NUMBER_OF_KIDS_ENROLLED\\": {\\n            \\"type\\": \\"number\\",\\n            \\"format\\": \\"double\\"\\n          },\\n          \\"NUMBER_OF_REWARDS_CREATED\\": {\\n            \\"type\\": \\"number\\",\\n            \\"format\\": \\"double\\"\\n          },\\n          \\"NUMBER_OF_REWARDS_PURCHASED\\": {\\n            \\"type\\": \\"number\\",\\n            \\"format\\": \\"double\\"\\n          }\\n        }\\n      },\\n      \\"svml_9223d64159_OUTPUT_TYPE\\": {\\n        \\"properties\\": {\\n          \\"scoringResults\\": {\\n            \\"type\\": \\"array\\",\\n            \\"items\\": {\\n              \\"$ref\\": \\"#/components/schemas/ScoringResults\\"\\n            }\\n          }\\n        }\\n      },\\n      \\"ScoringResults\\": {\\n        \\"properties\\": {\\n          \\"classifications\\": {\\n            \\"type\\": \\"array\\",\\n            \\"items\\": {\\n              \\"$ref\\": \\"#/components/schemas/LabelProb\\"\\n            }\\n          },\\n          \\"details\\": {\\n            \\"type\\": \\"array\\",\\n            \\"items\\": {\\n              \\"$ref\\": \\"#/components/schemas/Detail\\"\\n            }\\n          }\\n        }\\n      },\\n      ...\\n  }\\n}\\n```\\n\\n### Endpoint URL\\n\\nThe template for the OML Service endpoint URLs is:\\n\\n```\\nhttps://adb.<REGION_CODE>.oraclecloud.com/omlmod/v1/deployment<PATH>\\n```\\n\\nThe `PATH` that we will use to perform the scoring operation is `/svml/score`, which you can find in the `paths` attribute. It accepts a HTTP `POST` request with a required body containing input parameters in JSON format.\\n\\n### Inputs\\n\\nFrom the input schema, you can glean that the endpoint requires an `inputRecords` array containing one or more objects, each with seven numerical input variables:\\n\\n* `AVG_TOTAL_GROUP_READING_TIME`\\n* `DASHBOARD_VISITS`\\n* `MAX_TOTAL_GROUP_READING_TIME`\\n* `MIN_TOTAL_GROUP_READING_TIME`\\n* `NUMBER_OF_KIDS_ENROLLED`\\n* `NUMBER_OF_REWARDS_CREATED`\\n* `NUMBER_OF_REWARDS_PURCHASED`\\n\\nThese are features that AutoML had selected during the training process. An example of the JSON payload included in a HTTP request is:\\n\\n```json\\n{\\n    \\"inputRecords\\": [\\n        {\\n            \\"AVG_TOTAL_GROUP_READING_TIME\\": 383,\\n            \\"DASHBOARD_VISITS\\": 2021,\\n            \\"MAX_TOTAL_GROUP_READING_TIME\\": 157,\\n            \\"MIN_TOTAL_GROUP_READING_TIME\\": 281,\\n            \\"NUMBER_OF_KIDS_ENROLLED\\": 114,\\n            \\"NUMBER_OF_REWARDS_CREATED\\": 20,\\n            \\"NUMBER_OF_REWARDS_PURCHASED\\": 45\\n        }\\n    ]\\n}\\n```\\n\\n### Outputs\\n\\nFinally, from the specifications, we know that the service will return the array `scoringResults` that should be of the same size as the `inputRecords` submitted in the request. Each element will contain an array of `classifications` (there are two in our binary classifier) objects with the attributes `label` and `probability`. Below is an example of prediction outputs returned after a successful call to the OML Service.\\n\\n```json\\n{\\n    \\"scoringResults\\": [\\n        {\\n            \\"classifications\\": [\\n                {\\n                    \\"label\\": \\"0\\",\\n                    \\"probability\\": 0.1977130504153105\\n                },\\n                {\\n                    \\"label\\": \\"1\\",\\n                    \\"probability\\": 0.8022869495846895\\n                }\\n            ]\\n        }\\n    ]\\n}\\n```\\n\\n## Using the Deployed Model\\n\\nArmed with the information [above](#Inputs-and-Outputs), we can now construct a wrapper PL/SQL procedure, for example:\\n\\n```sql\\ncreate or replace package body pkg_omlservices_predict\\nas\\n  c_svml_url constant varchar2(73) := \'https://.../omlmod/v1/deployment/svml/score\';\\n\\n  function f_active_in_six_months(\\n    p_token in varchar2\\n    , p_avg_total_group_reading_time in number\\n    , p_dashboard_visits in number\\n    , p_max_total_group_reading_time in number\\n    , p_min_total_group_reading_time in number\\n    , p_number_of_kids_enrolled in number\\n    , p_number_of_rewards_created in number\\n    , p_number_of_rewards_purchased in number\\n  ) return clob\\n  as\\n    l_results clob;\\n  begin\\n    apex_web_service.g_request_headers(1).name := \'Authorization\';\\n    apex_web_service.g_request_headers(1).value := \'Bearer \' || p_token;\\n\\n    apex_web_service.g_request_headers(2).name := \'Content-Type\';\\n    apex_web_service.g_request_headers(2).value := \'application/json\';\\n\\n    l_results := apex_web_service.make_rest_request(\\n      p_url => c_svml_url\\n      , p_http_method => \'POST\'\\n      , p_body => json_object(\\n          \'inputRecords\' value json_array(\\n            json_object(\\n              \'AVG_TOTAL_GROUP_READING_TIME\' value p_avg_total_group_reading_time\\n              , \'DASHBOARD_VISITS\' value p_dashboard_visits\\n              , \'MAX_TOTAL_GROUP_READING_TIME\' value p_max_total_group_reading_time\\n              , \'MIN_TOTAL_GROUP_READING_TIME\' value p_min_total_group_reading_time\\n              , \'NUMBER_OF_KIDS_ENROLLED\' value p_number_of_kids_enrolled\\n              , \'NUMBER_OF_REWARDS_CREATED\' value p_number_of_rewards_created\\n              , \'NUMBER_OF_REWARDS_PURCHASED\' value p_number_of_rewards_purchased\\n            )\\n          )\\n        )\\n      )\\n    );\\n\\n    if apex_web_service.g_status_code != 200 then\\n      raise_application_error(\\n        -20002\\n        , \'Error predicting results. HTTP status code: \' || apex_web_service.g_status_code)\\n      ;\\n    end if;\\n\\n    return l_results;\\n  exception\\n    when others then\\n      owa_util.status_line(\\n        nstatus => 500\\n        , creason => apex_web_service.g_status_code\\n      );\\n  end f_active_in_six_months;\\nend pkg_omlservices_predict;\\n```\\n\\nFor a simple demonstration on how to use the OML Service in APEX, I created a page that allows users to enter the seven required inputs, and then call the `pkg_omlservices_predict.f_active_in_six_months` function to obtain the predicted labels and their probabilities.\\n\\n```sql\\ndeclare\\n  l_results clob;\\nbegin\\n  apex_collection.create_or_truncate_collection(\'PREDICTION_RESULTS\');\\n\\n  l_results := pkg_omlservices_predict.f_active_in_six_months(\\n    p_token => :G_TOKEN\\n    , p_avg_total_group_reading_time => :P1_AVG_TOTAL_GROUP_READING_TIME\\n    , p_dashboard_visits => :P1_DASHBOARD_VISITS\\n    , p_max_total_group_reading_time => :P1_MAX_TOTAL_GROUP_READING_TIME\\n    , p_min_total_group_reading_time => :P1_MIN_TOTAL_GROUP_READING_TIME\\n    , p_number_of_kids_enrolled => :P1_NUMBER_OF_KIDS_ENROLLED\\n    , p_number_of_rewards_created => :P1_NUMBER_OF_REWARDS_CREATED\\n    , p_number_of_rewards_purchased => :P1_NUMBER_OF_REWARDS_PURCHASED\\n  );\\n\\n  apex_collection.add_member(\\n    p_collection_name => \'PREDICTION_RESULTS\'\\n    , p_clob001 => l_results\\n  );\\nend;\\n```\\n\\nI can then render the desired output after parsing the `scoringResults` JSON array returned by the OML Service, for example, using a classic report using the following SQL query:\\n\\n```sql\\nwith results as (\\n  select clob001 as json_data\\n  from apex_collections\\n  where collection_name = \'PREDICTION_RESULTS\'\\n  fetch first 1 rows only\\n)\\nselect\\n  j.outcome\\n  , j.probability\\nfrom results r\\n  , json_table(\\n    r.json_data, \'$.scoringResults\' columns (\\n      nested path \'$.classifications[*]\'\\n      columns (\\n        outcome path \'$.label\'\\n        , probability path \'$.probability\'\\n      )\\n    )\\n  ) j\\n```\\n\\nIt is worthwhile noting that developers can submit multiple rows of input data and obtain all the results at once. For example, if you wanted to cache the predicted outcomes in a local table, you can batch them in one HTTP request and then parsing multiple results, thereby reducing some overheads.\\n\\n## Closing\\n\\nThis is only the beginning and I am super excited with all the tools that Oracle has put at our disposal. Do more and do good. If you require additional help, or would like to collaborate on other APEX+ML adventures, please do not hesitate to contact me *via* [Twitter](https://twitter.com/fuzziebrain) or through [Insum](https://www.insum.ca/contact/).\\n\\n\\n> **Credits**\\n>\\n> Image by <a href=\\"https://pixabay.com/users/geralt-9301/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=1773922\\">Gerd Altmann</a> from <a href=\\"https://pixabay.com/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=1773922\\">Pixabay</a>"},{"id":"id/2011","metadata":{"permalink":"/content/id/2011","source":"@site/posts/arm-your-apex-instance/index.md","title":"ARM Your APEX Instance","description":"Embedded ARM chip","date":"2021-05-25T08:50:00.000Z","formattedDate":"May 25, 2021","tags":[{"label":"oracle cloud","permalink":"/content/tags/oracle-cloud"},{"label":"oci","permalink":"/content/tags/oci"},{"label":"arm","permalink":"/content/tags/arm"},{"label":"aarch64","permalink":"/content/tags/aarch-64"},{"label":"ampere","permalink":"/content/tags/ampere"},{"label":"orclapex","permalink":"/content/tags/orclapex"},{"label":"orclords","permalink":"/content/tags/orclords"},{"label":"autonomous database","permalink":"/content/tags/autonomous-database"},{"label":"podman","permalink":"/content/tags/podman"},{"label":"docker","permalink":"/content/tags/docker"},{"label":"open container initiative","permalink":"/content/tags/open-container-initiative"}],"readingTime":7.525,"hasTruncateMarker":true,"authors":[{"name":"Adrian Png","title":"Senior Cloud Solutions Architect @ Insum","url":"https://github.com/fuzziebrain","imageURL":"https://github.com/fuzziebrain.png","key":"fuzziebrain"}],"frontMatter":{"title":"ARM Your APEX Instance","tags":["oracle cloud","oci","arm","aarch64","ampere","orclapex","orclords","autonomous database","podman","docker","open container initiative"],"slug":"id/2011","authors":["fuzziebrain"],"categories":["Technology","Infrastructure","Oracle Cloud"],"date":"2021-05-25T08:50:00.000Z"},"prevItem":{"title":"Intelligent APEX Applications - Fact or Fiction?","permalink":"/content/id/2012"},"nextItem":{"title":"Making AI Accessible to APEX","permalink":"/content/id/2010"}},"content":"![Embedded ARM chip](./assets/arm-chip_1200x720.jpg)\\n\\nIn September 2020, Oracle first [announced](https://youtu.be/aVwLlkhERuQ?t=331) that it was working with [Ampere Computing](https://amperecomputing.com/) to provide ARM-based chips for its Compute instance offering on the [Oracle Cloud Infrastructure](https://oracle.com/cloud). They have delivered!\\n\\nARM chips are found in many low-powered devices like smartphones, single-board computers, and IoT devices. So, if you are an [Oracle Application Express](https://apex.oracle.com) (APEX) developer, you might be wondering, \\"What\'s in it for me?\\" Well, remember we [talked](/content/id/2004) about running a customer-managed [Oracle REST Data Services](https://www.oracle.com/database/technologies/appdev/rest.html) (ORDS) instance for an [Oracle Autonomous Database](https://www.oracle.com/autonomous-database/)?\\n\\n\x3c!-- truncate --\x3e\\n\\nThe OCI isn\'t the first platform to offer ARM-based compute resources. You will find a similar offering from AWS with their [Graviton](https://aws.amazon.com/ec2/graviton/) processors. Instead of custom-designing a processor, Oracle opted to use the [Ampere Altra Processor](https://amperecomputing.com/altra/) that has 80 cores running at a maximum frequency of 3.3 Ghz. Check out AnandTech\'s [review](https://www.anandtech.com/show/16315/the-ampere-altra-review) of the chip.\\n\\nIf you are interested to read about the performance comparisons between the ARM and x86 architectures, please check out this [report](https://www.infoq.com/articles/arm-vs-x86-cloud-performance/). What I understand is that for hosting web applications that are typically single threaded, ARM chips offer a better cost-to-performance ratio, not to mention, lower power consumption. Each processing core is also generally cheaper than its competitors, thus allowing cloud vendors to provision them at cheaper rates. We saw this happening when Oracle first launched the AMD EPYC-based compute shapes that were cheaper than Intel\'s at that time.\\n\\n## Let\'s Get Started\\n\\nAs with other processor types, OCI customers can opt to provision virtual machines or bare metal machines:\\n\\n* **VM.Standard.A1.Flex.** Like the AMD EPYC Flex shapes, users can create instances with a flexible number of OCPUs and memory. Servers can be provisioned with 1 to 80 OCPUs, and 1 to 512 GB memory.\\n* **BM.Standard.A1.160.** Only one shape for bare metal machines is available at this time. It comes with 160 OCPUs and 1 TB of memory.\\n\\nOracle has also provided a generous Oracle Cloud Free Tier allotment of Ampere compute instance. Users can provision an *Always Free* compute instance with up to 4 OCPUs and 24 GB of memory (see the [press release](https://www.oracle.com/news/announcement/oracle-unlocks-power-of-arm-processors-at-one-cent-per-core-hour-2021-05-25/)). The Always Free block volume allotment has also increased from 100 to 200 GB. Yes, there are no charges and no end dates for these resources. I will use this to host the customer-managed ORDS instance. Beyond that, according to the [cost estimator](https://www.oracle.com/cloud/cost-estimator.html), each additional OCPU will cost you only $10 per month, and $1 for every gigabyte of memory!\\n\\n> **Note.** These A1 cores are single-threaded. In x86-based shapes, 1 OCPU has 2 virtual CPUs.\\n\\n![Create an Ampere-shaped Compute instance.](./assets/create-compute-instance.png)\\n\\nThe downside of moving to an ARM chip is the availability of software support. For example, though you can install the [Oracle Instant Client](https://www.oracle.com/database/technologies/instant-client.html) on an ARM system, Oracle has not released AArch64 database binaries. Fortunately for us, there already exists a Java port for this architecture, so there should be no issues deploying ORDS or running it standalone. However, at the time of writing, the ORDS package is not available to install through the Oracle Yum repositories. I will instead deploy ORDS on a [Tomcat](https://tomcat.apache.org/) server using [Podman](https://podman.io/). The official [Docker](https://docker.com) image that I will use can be found [here](https://hub.docker.com/_/tomcat). Take note that you will have to ensure that you deploy a version of ORDS that\'s compatible to what the ADB is using.\\n\\n![Use Database Actions to check version of ORDS that the ADB is running.](./assets/checking-ords-version.png)\\n\\nAfter provisioning the Oracle Enterprise Linux 8 server, login as the user *opc* and install the following software:\\n\\n* OCI Command Line Interface (CLI)\\n* Podman\\n* [Optional] Support for emulating Docker CLI using podman\\n\\n```bash\\nsudo dnf install python36-oci-cli podman podman-docker\\n```\\n\\nCreate the base directory where we will deploy our files to.\\n\\n```bash\\nBASE_PATH=/opt/podman/ords && \\\\\\nmkdir -p $BASE_PATH\\n```\\n\\nStage the following installer files in the directory `$BASE_PATH/files`:\\n\\n* `apex_20.2.zip`\\n* `p32006852_2020_Generic.zip`\\n* `ords-21.1.1.116.2032.zip`\\n\\n> **Note:** At the time of writing, the ADB is still running APEX 20.2 with the latest Patch Set Bundle. To download the patch, you will require a qualified support contract.\\n\\nCreate the following *Dockerfile* in the directory specified by the environment variable `BASE_PATH`:\\n\\n```dockerfile\\nFROM tomcat:9.0-jdk8-openjdk-slim\\n\\nARG INSTALL_FILES_DIR=/tmp\\nARG ORDS_HOME=/opt/oracle/ords\\nARG ORDS_CONFIG_DIR=$ORDS_HOME/conf\\nARG ORDS_INSTALL_FILE=ords-21.1.1.116.2032.zip\\n\\nCOPY files/$ORDS_INSTALL_FILE $INSTALL_FILES_DIR/\\n\\nRUN apt update -y && \\\\\\n  apt install -y unzip && \\\\\\n  mkdir -p $ORDS_HOME $ORDS_CONFIG_DIR && \\\\\\n  unzip $INSTALL_FILES_DIR/$ORDS_INSTALL_FILE -d $ORDS_HOME && \\\\\\n  java -jar $ORDS_HOME/ords.war configdir $ORDS_CONFIG_DIR && \\\\\\n  cp $ORDS_HOME/ords.war /usr/local/tomcat/webapps/ && \\\\\\n  sed -i -r \'s/Connector port=\\\\\\"8080\\\\\\" protocol=\\\\\\"HTTP\\\\/1\\\\.1\\\\\\"/& scheme=\\\\\\"https\\\\\\"/\' \\\\\\n    /usr/local/tomcat/conf/server.xml && \\\\\\n  rm -f $INSTALL_FILES_DIR/$ORDS_INSTALL_FILE && \\\\\\n  apt clean -y\\n```\\n\\nAlternatively, run the following Bash script to create the Dockerfile:\\n\\n```bash\\ncd $BASE_PATH && \\\\\\ncat << EOF > Dockerfile\\nFROM tomcat:9.0-jdk8-openjdk-slim\\n\\nARG INSTALL_FILES_DIR=/tmp\\nARG ORDS_HOME=/opt/oracle/ords\\nARG ORDS_CONFIG_DIR=\\\\$ORDS_HOME/conf\\nARG ORDS_INSTALL_FILE=ords-21.1.1.116.2032.zip\\n\\nCOPY files/\\\\$ORDS_INSTALL_FILE \\\\$INSTALL_FILES_DIR/\\n\\nRUN apt update -y && \\\\\\\\\\n  apt install -y unzip && \\\\\\\\\\n  mkdir -p \\\\$ORDS_HOME \\\\$ORDS_CONFIG_DIR && \\\\\\\\\\n  unzip \\\\$INSTALL_FILES_DIR/\\\\$ORDS_INSTALL_FILE -d \\\\$ORDS_HOME && \\\\\\\\\\n  java -jar \\\\$ORDS_HOME/ords.war configdir \\\\$ORDS_CONFIG_DIR && \\\\\\\\\\n  cp \\\\$ORDS_HOME/ords.war /usr/local/tomcat/webapps/ && \\\\\\\\\\n  sed -i -r \'s/Connector port=\\\\\\\\\\"8080\\\\\\\\\\" protocol=\\\\\\\\\\"HTTP\\\\/1\\\\\\\\.1\\\\\\\\\\"/& scheme=\\\\\\\\\\"https\\\\\\\\\\"/\' \\\\\\\\\\n    /usr/local/tomcat/conf/server.xml && \\\\\\\\\\n  rm -f \\\\$INSTALL_FILES_DIR/\\\\$ORDS_INSTALL_FILE && \\\\\\\\\\n  apt clean -y\\nEOF\\n```\\n\\nBuild the container image.\\n\\n```bash\\ncd $BASE_PATH && \\\\\\npodman build -t ords .\\n```\\n\\nSet the following environment variables based on your tenancy, data region, and ADB:\\n\\n```bash\\nADB_OCID=ocid1.autonomousdatabase.oc1.phx.abyh...4pggq\\nADB_NAME=myadb\\nORDS_USER=ORDS_PUBLIC_USER2\\nORDS_PASSWORD=\\"avoidsimplepasswords\\"\\nWALLET_PASSWORD=\\"atalltimes\\"\\nSERVICE_NAME=${ADB_NAME}_low\\nWALLET_FILE_PATH=/tmp/wallet_${ADB_NAME}.zip\\nORDS_CONFIG_DIR=$BASE_PATH/conf\\n```\\n\\nUse the OCI CLI to generate the required ADB wallet, and then create the base64-encoded string needed for configuring ORDS to access the database.\\n\\n```bash\\nexport OCI_CLI_AUTH=instance_principal && \\\\\\noci db autonomous-database generate-wallet \\\\\\n  --autonomous-database-id $ADB_OCID \\\\\\n  --file $WALLET_FILE_PATH \\\\\\n  --password WALLET_PASSWORD && \\\\\\nWALLET_BASE64=`base64 -w 0 $WALLET_FILE_PATH` && \\\\\\nrm -f ${WALLET_FILE_PATH}\\n```\\n\\nCreate the database user named in the environment variable `ORDS_USER` and asssign the necessary privileges. Refer to the section *User Accounts* in the previous [post](/content/id/2004#User-Accounts) if you require help. Then create the ORDS configuration files:\\n\\n```bash\\nmkdir -p $ORDS_CONFIG_DIR/ords/conf && \\\\\\ncat << EOF > $ORDS_CONFIG_DIR/ords/conf/apex_pu.xml\\n<?xml version=\\"1.0\\" encoding=\\"UTF-8\\" standalone=\\"no\\"?>\\n<!DOCTYPE properties SYSTEM \\"http://java.sun.com/dtd/properties.dtd\\">\\n<properties>\\n  <entry key=\\"db.username\\">$ORDS_USER</entry>\\n  <entry key=\\"db.password\\">!$ORDS_PASSWORD</entry>\\n  <entry key=\\"db.wallet.zip.service\\">$SERVICE_NAME</entry>\\n  <entry key=\\"db.wallet.zip\\"><![CDATA[$WALLET_BASE64]]></entry>\\n</properties>\\nEOF\\n\\ncat << EOF > $ORDS_CONFIG_DIR/ords/defaults.xml\\n<?xml version=\\"1.0\\" encoding=\\"UTF-8\\" standalone=\\"no\\"?>\\n<!DOCTYPE properties SYSTEM \\"http://java.sun.com/dtd/properties.dtd\\">\\n<properties>\\n  <entry key=\\"plsql.gateway.enabled\\">true</entry>\\n  <entry key=\\"jdbc.InitialLimit\\">5</entry>\\n  <entry key=\\"jdbc.MaxLimit\\">10</entry>\\n</properties>\\nEOF\\n```\\n\\nPrepare to stage the APEX images files by first setting the following environment variables:\\n\\n```bash\\nINSTALL_FILES_DIR=$BASE_DIR/files\\nAPEX_INSTALL_FILE=apex_20.2.zip\\nAPEX_VERSION=20.2.0.00.20\\nAPEX_IMAGES_DIR=$BASE_PATH/apex_images\\n```\\n\\nExecute the snippet below to extract the necessary files from the APEX installation file and deploy them to the Tomcat container.\\n\\n```bash\\nmkdir -p $APEX_IMAGES_DIR && \\\\\\ntouch $APEX_IMAGES_DIR/index.html && \\\\\\nunzip $INSTALL_FILES_DIR/$APEX_INSTALL_FILE \\"apex/images/*\\" -d /tmp/ && \\\\\\nmv /tmp/apex/images $APEX_IMAGES_DIR/$APEX_VERSION && \\\\\\nrm -rf /tmp/apex\\n```\\n\\nIf your instance is running a version of APEX that has a patch available, then be sure to apply it. Again, you will need access to [Oracle Support](https://support.oracle.com) with the rights to download patches. Set the environment variables:\\n\\n```bash\\nPATCH_FILE=p32006852_2020_Generic.zip\\nPATCH_NUMBER=32006852\\n```\\n\\nThen run the snippet below to deploy the changes:\\n\\n```bash\\nmkdir -p /tmp/$APEX_VERSION && \\\\\\nunzip /tmp/$PATCH_FILE \\"$PATCH_NUMBER/images/*\\" -d /tmp/$APEX_VERSION && \\\\\\nyes | cp -R /tmp/$APEX_VERSION/$PATCH_NUMBER/images/* $APEX_IMAGES_DIR/$APEX_VERSION && \\\\\\nrm -rf /tmp/$APEX_VERSION\\n```\\n\\nCreate a utility `run.sh` script to run the container. The script allows one argument to set the port number to map to, and also used to set the container\'s name.\\n\\n```bash\\ncd $BASE_PATH && \\\\\\ncat << EOF > run.sh\\n#!/usr/bin/env bash\\n\\nPORT_NUMBER=\\\\${1:-8080}\\n\\npodman run -d --name=ords_port_\\\\${PORT_NUMBER} \\\\\\\\\\n  -p \\\\$PORT_NUMBER:8080 \\\\\\\\\\n  -v \\\\$PWD/conf:/opt/oracle/ords/conf:z \\\\\\\\\\n  -v $APEX_IMAGES_DIR:/usr/local/tomcat/webapps/i \\\\\\\\\\n  --restart unless-stopped \\\\\\\\\\n  ords\\nEOF\\n```\\n\\nFinally, since we are running this as a rootless container, the firewall rules must be updated to allow communication on port `8080`. Execute the following commands:\\n\\n```bash\\nsudo firewall-cmd --zone=public --add-port 8080/tcp --permanent && \\\\\\nsudo firewall-cmd --reload\\n```\\n\\nRun the container that\'s listening on port `8080`.\\n\\n```bash\\ncd $BASE_PATH && bash run.sh 8080\\n```\\n\\nFinally, setup a load balancer for the customer-managed ORDS instance. This was discussed in an earlier [post](/content/id/2005).\\n\\n## Bonus Task\\n\\nIf you are also interested in testing out some \\"redundancy\\", you might like to run a second container:\\n\\n```bash\\nPORT_NUMBER=8081 && \\\\\\nsudo firewall-cmd --zone=public --add-port $PORT_NUMBER/tcp --permanent && \\\\\\nsudo firewall-cmd --reload && \\\\\\ncd $BASE_PATH && bash run.sh $PORT_NUMBER\\n```\\n\\nAnd then add this as a backend server that\'s listening on port `8081` to the set. Just be really careful about exceeding the maximum number of concurrent database sessions if this is an Always Free ADB.\\n\\n![Add a backend server to simulate redundancy.](./assets/extra-backend-server.png)\\n\\nTo be clear, this is really for experimentation and simulation use only. As a best practice, the second container should be deployed on a separate host, preferably in a different availability domain.\\n\\n## Summary\\n\\nThis is a great addition to the family of OCI resources that Oracle provides.  Besides deploying customer-managed ORDS to complement the Autonomous Database, APEX developers can also use them to host other workloads to supercharge web applications. For example, running a [WebSocket](https://developer.mozilla.org/docs/Web/API/WebSockets_API) server using [Socket.IO](https://socket.io/) to display realtime updates, or hosting a [REST service using Spring Boot](https://spring.io/guides/tutorials/rest/). A choice of provisioning low-cost and highly-efficient servers, and the expanded Always Free limits. What\'s there not to love?"},{"id":"id/2010","metadata":{"permalink":"/content/id/2010","source":"@site/posts/making-ai-accessible-to-apex/index.md","title":"Making AI Accessible to APEX","description":"Figuring out sentiments.","date":"2021-05-08T11:24:17.000Z","formattedDate":"May 8, 2021","tags":[{"label":"oracle cloud","permalink":"/content/tags/oracle-cloud"},{"label":"artificial intelligence","permalink":"/content/tags/artificial-intelligence"},{"label":"machine learning","permalink":"/content/tags/machine-learning"},{"label":"orclapex","permalink":"/content/tags/orclapex"},{"label":"oci","permalink":"/content/tags/oci"},{"label":"natural language processing","permalink":"/content/tags/natural-language-processing"}],"readingTime":5.3,"hasTruncateMarker":true,"authors":[{"name":"Adrian Png","title":"Senior Cloud Solutions Architect @ Insum","url":"https://github.com/fuzziebrain","imageURL":"https://github.com/fuzziebrain.png","key":"fuzziebrain"}],"frontMatter":{"title":"Making AI Accessible to APEX","tags":["oracle cloud","artificial intelligence","machine learning","orclapex","oci","natural language processing"],"slug":"id/2010","authors":["fuzziebrain"],"categories":["Technology","Infrastructure","Oracle Cloud"],"date":"2021-05-08T11:24:17.000Z"},"prevItem":{"title":"ARM Your APEX Instance","permalink":"/content/id/2011"},"nextItem":{"title":"Manipulating the Oracle Cloud with PL/SQL","permalink":"/content/id/2009"}},"content":"![Figuring out sentiments.](./assets/emoji-1200x800.jpg)\\n\\nThe [*Oracle Cloud Infrastructure*](https://oracle.com/cloud/) (OCI) is a great platform in many ways. However, as much as I love it, I have long griped about the lack of out-of-the-box support for AI services that many of Oracle\'s competitors have. Well, as of May 6, 2021, I am glad to see that things are starting to turn around with the [launch](https://docs.oracle.com/iaas/releasenotes/changes/f801b1f5-b40d-49a5-bbb1-e2b2001ce2cf/) of the OCI *Language* service.\\n\\n\x3c!-- truncate --\x3e\\n\\nToday, there are different ways to \\"AI-enable\\" an [*Oracle Application Express*](https://apex.oracle.com) (APEX). For a long time now, Oracle has had several PL/SQL packages under the Oracle Enterprise Database\'s Machine Learning option, or what used to be known as Oracle Advanced Analytics. This option is in fact, [no longer](https://blogs.oracle.com/database/machine-learning,-spatial-and-graph-no-license-required-v2) an option and is included in all editions of the database.\\n\\nSpecifically, [*Oracle Machine Learning for SQL*](https://www.oracle.com/database/technologies/datawarehouse-bigdata/oml4sql.html) (OML4SQL) allows the PL/SQL developer to train and apply ML models without the data leaving the database. It supports a variety of ML algorithms, and generally, does not require developers to have advanced theoretical knowledge of ML. And because it relies only on PL/SQL and SQL, it can be easily integrated into APEX. However, developers are limited to ML algorithms that have been implemented by Oracle.\\n\\nAbout a month ago, Oracle announced the general availability of *AutoML* and *OML Services* on the *Autonomous Database* (ADB). Developers who are not too ML-savvy can use the AutoML UI (user interface), available through the Oracle Machine Learning Notebooks that is included with every ADB. AutoMl automates the process of selecting, training, testing, and evaluating ML models. The generated models can then be exposed through OML Services\' REST APIs.\\n\\nHowever, sometimes, all we need is a ready-to-use AI/ML model that performs a rather generic task. Very common these days is the need to perform *Sentiment Analysis* on a body of text. Microsoft Azure\'s Cognitive Services has Text Analytics and AWS has Amazon Comprehend. What does Oracle have? At least now we can say, \\"OCI Language Services.\\" It took a while, but better late than never!\\n\\n## Identity and Policy\\n\\nAs a best practice, we should create an OCI user specifically for interacting with the OCI REST API on behalf of the APEX application. The user should have limited OCI capabilities and should only be allowed to have API access and nothing more. You will need to generate the API keys and can easily do this from the user\'s details page in the OCI console.\\n\\n![Generate and download the API key pair from OCI console.](./assets/generate-download-api-keys.png)\\n\\nYou should also download the keys in PEM format. Also, gather the information (click the *View Configuration File* link on the user\'s details page) that you will need to setup the APEX Web Credentials for OCI, specifically:\\n\\n* User OCID\\n* Tenant OCID; and\\n* The API key\'s fingerprint\\n\\n![Gather the OCI information that you will need.](./assets/gather-information.png)\\n\\nOpen the private key that you had downloaded from the OCI console and remove (a) the enclosing tags `-----BEGIN PRIVATE KEY-----` and `-----END PRIVATE KEY-----`; and (b) **all** newline characters. The end result should be a very long string of characters. This is the *OCI Private Key* that you will need to provide in the APEX Web Credentials. Copy the text, but **do not** save the PEM file.\\n\\nCreate the Web Credentials with the required information about your user and tenancy.\\n\\n![Setup APEX Web Credentials.](./assets/web-credentials.png)\\n\\nNext, create an IAM Group, e.g., *AiLanguageUsers*, and assign the user to the group.\\n\\nTo use the Language Services, we will need to create a policy to allow users access. We have the option of creating a blanket policy with the following statement:\\n\\n```\\nallow any-user to use ai-service-language-family in tenancy\\n```\\n\\n**OR**\\n\\n```\\nallow group AiLanguageUsers to use ai-service-language-family in tenancy\\n```\\n\\nI recommend the latter.\\n\\n![Create a policy to allow a group access to the Language Services.](./assets/policy.png)\\n\\n## Demo Application\\n\\nCreate a simple APEX application to demonstrate the utility of the Language Service through its REST API.\\n\\n![](./assets/demo-page.png)\\n\\nThe key items are:\\n\\n* Two textarea page items\\n* A button that triggers a dynamic action\\n* A dynamic action that sets one item using a PL/SQL function with the code below:\\n\\n```sql\\ndeclare\\n  c_base_url constant varchar2(59) := \'https://language.aiservice.us-phoenix-1.oci.oraclecloud.com\';\\n  c_endpoint_path constant varchar2(50) := \'/20210101/actions/detectLanguageSentiments\';\\n  c_web_credential constant varchar2(30) := \'DG_ADBAGENT01\';\\n\\n  l_response clob;\\nbegin\\n  apex_web_service.g_request_headers(1).name := \'Content-Type\';\\n  apex_web_service.g_request_headers(1).value := \'application/json\';\\n\\n  l_response := apex_web_service.make_rest_request(\\n    p_url => c_base_url || c_endpoint_path\\n      , p_http_method => \'POST\'\\n      , p_body => \'{ \\"text\\": \\"\' || apex_escape.json(:P1_INPUT) || \'\\" }\'\\n      , p_credential_static_id => c_web_credential\\n    );\\n\\n  return l_response;\\nend;\\n```\\n\\nThis DA simply takes the input text and makes a POST request to the appropriate endpoint for performing sentiment analysis. The expected output is a JSON containing the model\'s prediction.\\n\\n![We made it!](./assets/apex-we-got-there.png)\\n\\n~~Unfortunately, it doesn\'t seem to work right now and I\'m in touch with [Carsten](https://twitter.com/cczarski) to figure this out.~~\\n\\n> ~~[TODO] I will update the post as soon as there\'s a positive outcome.~~\\n\\n[UPDATE, 2021-05-10] A noob error! I should have checked the debugger logs. As it turns out, I was missing the HTTP header `Content-Type`. All systems go! The above code snippet and screenshot have been updated to reflect the success! Special thanks to [Carsten](https://twitter.com/cczarski) who helped validate that this REST call is supported and should have been successful.\\n\\nMeanwhile, I\'ve tried looking at the OCI PL/SQL SDK that I had discussed in the [previous post](/content/id/2009), but it doesn\'t appear to have been updated on my ADB.\\n\\n> [TODO] I will also update this post as that becomes available.\\n\\nWhat\'s working though is the OCI CLI, but you will need to upgrade it to access the new commands. Here\'s an example and the output that we should have obtained through the example APEX application.\\n\\n```bash\\n$ oci ai language detect-sentiments --text \\"I feel great with the launch of OCI AI Language Services!\\"\\n{\\n  \\"data\\": {\\n    \\"aspects\\": [\\n      {\\n        \\"length\\": 6,\\n        \\"offset\\": 22,\\n        \\"scores\\": {\\n          \\"Negative\\": 0.0004437448224052787,\\n          \\"Neutral\\": 0.0,\\n          \\"Positive\\": 0.9995562434196472\\n        },\\n        \\"sentiment\\": \\"Positive\\",\\n        \\"text\\": \\"launch\\"\\n      },\\n      {\\n        \\"length\\": 24,\\n        \\"offset\\": 32,\\n        \\"scores\\": {\\n          \\"Negative\\": 0.00011140753485960886,\\n          \\"Neutral\\": 0.0,\\n          \\"Positive\\": 0.99988853931427\\n        },\\n        \\"sentiment\\": \\"Positive\\",\\n        \\"text\\": \\"OCI AI Language Services\\"\\n      }\\n]\\n  }\\n}\\n```\\n\\nExciting times! As I have repeatedly said, the OCI keeps getting better. I\'m certainly looking forward to resolving the issues I have encountered, and hopefully, more AI services to be launched in the near future!\\n\\n> **Credits**\\n>\\n> Photo by <a href=\\"https://unsplash.com/@markuswinkler?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText\\">Markus Winkler</a> on <a href=\\"https://unsplash.com/s/photos/emoji?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText\\">Unsplash</a>"},{"id":"id/2009","metadata":{"permalink":"/content/id/2009","source":"@site/posts/manipulating-oci-with-pl-sql/index.md","title":"Manipulating the Oracle Cloud with PL/SQL","description":"Slide on OCI REST APIs updated","date":"2020-10-15T23:06:16.000Z","formattedDate":"October 15, 2020","tags":[{"label":"oracle","permalink":"/content/tags/oracle"},{"label":"cloud","permalink":"/content/tags/cloud"},{"label":"plsql","permalink":"/content/tags/plsql"},{"label":"rest","permalink":"/content/tags/rest"},{"label":"orclapex","permalink":"/content/tags/orclapex"}],"readingTime":4.04,"hasTruncateMarker":true,"authors":[{"name":"Adrian Png","title":"Senior Cloud Solutions Architect @ Insum","url":"https://github.com/fuzziebrain","imageURL":"https://github.com/fuzziebrain.png","key":"fuzziebrain"}],"frontMatter":{"title":"Manipulating the Oracle Cloud with PL/SQL","tags":["oracle","cloud","plsql","rest","orclapex"],"slug":"id/2009","authors":["fuzziebrain"],"categories":["Technology","Infrastructure","Oracle Cloud"],"date":"2020-10-15T23:06:16.000Z"},"prevItem":{"title":"Making AI Accessible to APEX","permalink":"/content/id/2010"},"nextItem":{"title":"New Book on Getting the Most Out of the Oracle Cloud Free Tier","permalink":"/content/id/2008"}},"content":"![Slide on OCI REST APIs updated](./assets/slide-updated.png)\\n\\nIt was only days ago that I had finalised my slides for my upcoming Oracle Groundbreakers APAC Virtual Tour [talk](https://oracle-groundbreakers-apac-virtual-tour-2020.heysummit.com/talks/store-your-treasures-in-the-cloud-1/) on using [Oracle Cloud Infrastructure](https://oracle.com/cloud) (OCI) Object Storage for storing large files in [Oracle Application Express](https://apex.oracle.com) (APEX). Today, I found out with a lot of excitement, that one of my slides was now factually wrong!\\n\\n\x3c!-- truncate --\x3e\\n\\nFor a while now, Oracle has provided SDKs (Software Development Kits) for a wide range of programming languages, but strangely, that didn\'t include PL/SQL. Thankfuly, the wait and bewilderment is now finally over! Well, at least for folks who are running their applications on the [Oracle Autonomous Database](https://www.oracle.com/autonomous-database/) (ADB). And yes, even if you\'re on an [*Always Free*](https://oracle.com/cloud/free) instance.\\n\\nDon\'t believe me? Use your favourite database client and login to your ADB instance using the *admin* account. Run the query below to get a list of available packages that line up with the APIs listed [here](https://docs.cloud.oracle.com/iaas/pl-sql-sdk/index.html).\\n\\n```sql\\nselect object_name\\nfrom dba_objects\\nwhere object_name like \'DBMS_CLOUD_OCI_%\'\\n  and object_type = \'PACKAGE\'\\norder by object_name;\\n```\\n\\nYou should see a long list of 70 packages (at the time of writing), something like this:\\n\\n```\\nOBJECT_NAME\\n----------------------------------------\\nDBMS_CLOUD_OCI_AD_AUDIT\\nDBMS_CLOUD_OCI_AG_DEPLOYMENT\\nDBMS_CLOUD_OCI_AG_GATEWAY\\nDBMS_CLOUD_OCI_AG_WORK_REQUESTS\\nDBMS_CLOUD_OCI_AM_APPLICATION_MIGRATION\\nDBMS_CLOUD_OCI_AN_ANALYTICS\\nDBMS_CLOUD_OCI_ASC_AUTO_SCALING\\nDBMS_CLOUD_OCI_AS_ANNOUNCEMENT\\nDBMS_CLOUD_OCI_BC_BLOCKCHAIN_PLATFORM\\nDBMS_CLOUD_OCI_BDS_BDS\\nDBMS_CLOUD_OCI_BD_BUDGET\\n...\\nDBMS_CLOUD_OCI_WS_WAAS\\n\\n70 rows selected.\\n```\\n\\n## Required Database Privileges\\n\\nTo use them with your application schema, e.g. *myapp*, you will need to do the following tasks. Begin with the *admin* account.\\n\\n1. Allow *myapp* to create the required OCI Cloud Services credentials (OCI credentials). We will discuss what this is in greater detail later. The procedures that you will need are in the package `DBMS_CLOUD`.\\n    ```sql\\n    grant execute on dbms_cloud to myapp;\\n    ```\\n1. Identify the API and corresponding PL/SQL package that you wish to use. For example, to access the Object Storage, I will require privileges to the package `DBMS_CLOUD_OCI_OBS_OBJECT_STORAGE`.\\n    ```sql\\n    grant execute on dbms_cloud_oci_obs_object_storage to myapp;\\n    ```\\n1. Check the API reference and identify database types that you might need to use and grant these privileges as well. For example, in my code, I will be declaring a variable of the type `dbms_cloud_oci_obs_object_storage_list_buckets_response_t`.\\n    ```sql\\n    grant execute on dbms_cloud_oci_obs_object_storage_list_buckets_response_t to myapp;\\n    ```\\n\\n## OCI Setup\\n\\nSetup the Object Storage bucket, user, group and policy using the steps described in my Oracle Magazine article [Better File Storage in Oracle Cloud](https://blogs.oracle.com/oraclemagazine/better-file-storage-in-oracle-cloud), specifically:\\n\\n* Identity Setup; and\\n* Bucket Setup\\n\\nBe sure to generate the API keypair and associate it with the OCI user that will act on your application\'s behalf. Also ensure that this user has the necessary policies applied. Finally, please note the following information to setup the required OCI credentials:\\n\\n* User\'s ocid\\n* Tenancy ocid\\n* Private key\\n* Fingerprint\\n\\nNext, connect to the database as the application user/schema. You will need to execute the procedure `DBMS_CLOUD.CREATE_CREDENTIAL`. There are two overloaded procedures and you **must** execute the procedure for the *OCI Signing Key Credentials*.\\n\\n```sql\\nbegin\\n  dbms_cloud.create_credential(\\n    credential_name => \'MY_OCI_CRED\'\\n    , user_ocid => \'ocid1.user.oc1..aaaaaaaa**********\'\\n    , tenancy_ocid => \'ocid1.tenancy.oc1..aaaaaaaa**********\'\\n    , private_key => \'MIIE**********=\'\\n    , fingerprint => \'**:**:**:**:**:**:**:**:**:**:**:**:**:**:**:**\'\\n  );\\nend;\\n/\\n```\\n\\n> **NOTE**\\n>\\n> The private key is to be prepared the same way we do when setting up the *Web Credentials* in APEX for OCI. Remove the header, footer and any newlines in the PEM-formatted contents.\\n\\nCheck that the OCI credentials were created successfully using the following query:\\n\\n```sql\\nselect credential_name, username from user_credentials;\\n```\\n\\nNow run the following sample code to get the list of Object Storage buckets in the compartment defined in the parameter `compartment_id`. You will need to also specify your tenancy\'s Object Storage namespace and region.\\n\\n```sql\\ndeclare\\n  where_are_my_buckets dbms_cloud_oci_obs_object_storage_list_buckets_response_t;\\nbegin\\n  where_are_my_buckets :=\\n      dbms_cloud_oci_obs_object_storage.list_buckets(\\n        namespace_name => \'mynamespace\'\\n        , compartment_id => \'ocid1.compartment.oc1..aaaaaaaa**********\'\\n        , region => \'us-ashburn-1\'\\n        , credential_name => \'MY_OCI_CRED\'\\n      );\\n  dbms_output.put_line(\'status code: \' || where_are_my_buckets.status_code);\\n  dbms_output.put_line(\'bucket count \' || where_are_my_buckets.response_body.count);\\n  for i in 1..where_are_my_buckets.response_body.count\\n  loop\\n    dbms_output.put_line(\'bucket: \' || where_are_my_buckets.response_body(i).name);\\n  end loop;\\nend;\\n/\\n```\\n\\nYou should get an output similar to this:\\n\\n```\\nstatus code: 200\\nbucket count 2\\nbucket: apex_file_storage\\nbucket: data-import\\n```\\n\\n## Closing\\n\\nI have barely scratched the surface of what we can do with this new PL/SQL SDK for OCI REST APIs, especially for OCI adminsitrators and APEX developers looking to [integrate Cloud Native](https://insum.ca/oracle-cloud-native-and-oracle-application-express/) features in their applications. The APEX team did us developers a huge service when the web credentials for OCI was introduced in version 19.2. It greatly simplified how we accessed the OCI REST services. However, not every endpoint worked out-of-the-box and I have found problems accessing some endpoints. We also had to write our own code for constructing the request payload and parsing the responses. This SDK handles and shields us from all the low-level marshalling tasks, and provides a structured approach for retrieving information from the API responses.\\n\\nThe power of the [Converged Database](https://blogs.oracle.com/database/what-is-a-converged-database) is here. One database to rule them all! [#ThinkAutonomous](https://twitter.com/hashtag/thinkautonomous)"},{"id":"id/2008","metadata":{"permalink":"/content/id/2008","source":"@site/posts/new-book-on-getting-the-most-out-of-the-oracle-cloud-free-tier/index.md","title":"New Book on Getting the Most Out of the Oracle Cloud Free Tier","description":"Getting Started with Oracle Cloud Free Tier Book Cover","date":"2020-09-27T23:56:39.000Z","formattedDate":"September 27, 2020","tags":[{"label":"oracle","permalink":"/content/tags/oracle"},{"label":"cloud","permalink":"/content/tags/cloud"},{"label":"free-tier","permalink":"/content/tags/free-tier"},{"label":"always-free","permalink":"/content/tags/always-free"},{"label":"orclapex","permalink":"/content/tags/orclapex"},{"label":"python","permalink":"/content/tags/python"},{"label":"nodejs","permalink":"/content/tags/nodejs"},{"label":"javascript","permalink":"/content/tags/javascript"},{"label":"reactjs","permalink":"/content/tags/reactjs"},{"label":"web","permalink":"/content/tags/web"},{"label":"applications","permalink":"/content/tags/applications"},{"label":"architecture","permalink":"/content/tags/architecture"},{"label":"book","permalink":"/content/tags/book"}],"readingTime":2.42,"hasTruncateMarker":true,"authors":[{"name":"Adrian Png","title":"Senior Cloud Solutions Architect @ Insum","url":"https://github.com/fuzziebrain","imageURL":"https://github.com/fuzziebrain.png","key":"fuzziebrain"}],"frontMatter":{"title":"New Book on Getting the Most Out of the Oracle Cloud Free Tier","tags":["oracle","cloud","free-tier","always-free","orclapex","python","nodejs","javascript","reactjs","web","applications","architecture","book"],"slug":"id/2008","authors":["fuzziebrain"],"categories":["Technology","Infrastructure","Oracle Cloud"],"date":"2020-09-27T23:56:39.000Z"},"prevItem":{"title":"Manipulating the Oracle Cloud with PL/SQL","permalink":"/content/id/2009"},"nextItem":{"title":"Update Free SSL Certificates for the OCI Load Balancer","permalink":"/content/id/2007"}},"content":"![Getting Started with Oracle Cloud Free Tier Book Cover](./assets/9781484260104-book-cover.png)\\n\\nI won\'t deny that it has been a challenging and tough journey, but today, I\'m glad to be able to declare that the book \\"[Getting Started with Oracle Cloud Free Tier](https://www.apress.com/book/9781484260104)\\" has been published and available to purchase!\\n\\n\x3c!-- truncate --\x3e\\n\\nHow time flies!\\n\\nLooking back, it\'s been almost a year since [Jonathan Gennick](https://twitter.com/JonathanGennick) at [Apress](https://apress.com) first approached me to see if I was keen to write a book on the then recently-announced [Oracle Cloud Free Tier](https://oracle.com/cloud/free). I was excited about what Oracle was doing with this new offering, and even more eager to share what I knew about the Oracle Cloud, so I quickly said, \\"Yes!\\"\\n\\nThere was a lot of ground to cover and we had aimed to publish the book ahead of [Oracle OpenWorld](https://www.oracle.com/openworld/) (OOW) 2020. Fortunately, I was able to rope in [Luc Demanche](https://twitter.com/lucdemanche) as a co-author. Luc focused on the infrastucture-related topics, while I tackled areas closer to my heart, software applications, including the longest chapter written for readers who are totally new to [Oracle Application Express](https://apex.oracle.com). It was tough compressing as much information as I could about such a great platform in only one chapter.\\n\\nIt was also particularly daunting to venture into other web application development platforms like [Flask](https://flask.palletsprojects.com/) for Python developers, and [React](https://reactjs.org/) for JavaScript developers. I wanted to demonstrate that the Oracle Cloud wasn\'t just for the APEX geeks. There\'s something for everyone!\\n\\nMore than just being knowledgeable about the Oracle Cloud, Luc was also a seasoned author. This is his second book, and his experience and cheer-leading helped contribute greatly to the successful completion of this new adventure. I am greatly indebted to him. I would also like to take the opportunity to thank Jonathon again for the opportunity, and to his team of editors and project managers, who helped guide us along the editorial process. And last but not least, a special thank you to [Christoph Ruepprich](https://twitter.com/CRuepprich), who sacrificed his time to help ensure that what we wrote was technically sound.\\n\\n## Oracle Groundbreakers APAC Virtual Tour 2020\\n\\nIn other news, I\'m honoured and glad to be participating in the upcoming [Oracle Groundbreakers APAC Virtual Tour](https://www.apacgbt.com/) happening next month, October 2020. I will be presenting my session \\"Store Your Treasures in the Cloud\\", that will provide a detailed walkthrough on integrating Oracle Cloud [Object Storage](https://www.oracle.com/cloud/storage/object-storage.html) with APEX application. And yes, you can certainly do this with *Always Free*-only resources.\\n\\nBorn and bred in [Singapore](https://www.google.com/maps/place/Singapore/), there\'s a warm fuzzy feeling knowing that I\'ll be speaking in a region closer to home. Especially so this year with COVID-19 disrupting my travel plans. As one of my favourite singers [\u963f\u5170](https://wikipedia.org/wiki/Alan_Dawa_Dolma) would say, \\"\u65e0\u8bba\u6211\u4eec\u98de\u5f97\u6709\u591a\u9ad8\uff0c\u6709\u591a\u8fdc\uff0c\u6c38\u8fdc\u4e0d\u8981\u5fd8\u8bb0\u81ea\u5df1\u7684\u5bb6\u4e61\uff0c\u4e0d\u8981\u5fd8\u8bb0\u81ea\u5df1\u7684\u6897\u3002\\""},{"id":"id/2007","metadata":{"permalink":"/content/id/2007","source":"@site/posts/update-free-ssl-certificates-for-the-oci-load-balancer/index.md","title":"Update Free SSL Certificates for the OCI Load Balancer","description":"Keep Security Up to Date","date":"2020-09-14T21:59:11.000Z","formattedDate":"September 14, 2020","tags":[{"label":"letsencrypt","permalink":"/content/tags/letsencrypt"},{"label":"ssl","permalink":"/content/tags/ssl"},{"label":"security","permalink":"/content/tags/security"},{"label":"certificates","permalink":"/content/tags/certificates"},{"label":"load balancer","permalink":"/content/tags/load-balancer"},{"label":"generate","permalink":"/content/tags/generate"},{"label":"renew","permalink":"/content/tags/renew"}],"readingTime":1.97,"hasTruncateMarker":true,"authors":[{"name":"Adrian Png","title":"Senior Cloud Solutions Architect @ Insum","url":"https://github.com/fuzziebrain","imageURL":"https://github.com/fuzziebrain.png","key":"fuzziebrain"}],"frontMatter":{"title":"Update Free SSL Certificates for the OCI Load Balancer","tags":["letsencrypt","ssl","security","certificates","load balancer","generate","renew"],"slug":"id/2007","authors":["fuzziebrain"],"categories":["Technology","Infrastructure","Oracle Cloud"],"date":"2020-09-14T21:59:11.000Z"},"prevItem":{"title":"New Book on Getting the Most Out of the Oracle Cloud Free Tier","permalink":"/content/id/2008"},"nextItem":{"title":"Keep Your APEX Images Updated","permalink":"/content/id/2006"}},"content":"![Keep Security Up to Date](./assets/house-keys-4516178_1280.jpg)\\n\\nIn my previous blog post on [Running ORDS Against the Autonomous Database - Part 2](/content/id/2005/), we had looked at how to add an existing [Let\'s Encrypt](https://letsencrypt.org/) (LE) SSL certificate to an [Oracle Cloud Infrastructure](https://oracle.com/cloud) (OCI) Load Balancer. You\'d typically wwant to host your websites or [Oracle Application Express](https://apex.oracle.com) applications over a secure transport layer, especially when sensitive data is communicated between the browser and server.\\n\\n\x3c!-- truncate --\x3e\\n\\nTypically, I\'d run scripts on the web server to generate and renew certificates using [Certbot](https://certbot.eff.org/), since it would be easy enough to point the web server configuration to use the generated artifacts, as well as to perform service restarts. However, with the load balancer, this becomes more challenging. It doesn\'t help that LE certificates expire every 90 days.\\n\\nI\'ve found it most reliable to run certbot from within a [Docker](https://docker.com) to avoid any dependency issues and you can quite easily get the published image on [Docker Hub](https://hub.docker.com/) including some with plugins to work directly with DNS services like [Cloudflare](https://cloudflare.com/). These are important if you wanted to obtain wildcard certificates that requires a different challenge process.\\n\\nOnce the certificates have been generated, they would have to be deployed to the OCI load balancer and assigned to the active listener that supports HTTPS requests. You could do this manually through the OCI console, but it is a mundane and routine task that is prone to neglect, so I had to figure out a way to automate this part of the process.\\n\\nOn the web server, Certbot is usually called by a cron job once a day. It checks if a certificate is expiring and will initiate the renewal process if it is. It also has hooks that allow you to execute external scripts. For example, after a certificate is successfully generated or renewed, a *post-hook* could be used to restart a web server.\\n\\nUsing the Certbot hooks and the [OCI CLI](https://docs.cloud.oracle.com/iaas/Content/API/Concepts/cliconcepts.htm), I created a Docker image that encapsulates both pieces of software, and then have them work together to perform both tasks in concert. You can find the code [here](https://github.com/fuzziebrain/oci-le-cert-manager). The documentation is a little rough and lengthy, so I will leave you to read the steps described in the [README](https://github.com/fuzziebrain/oci-le-cert-manager/blob/master/README.md) file.\\n\\nHope this helps you maintain and secure your web applications!\\n\\n> <u>Photo Credits</u>\\n>\\n> Photo by [Schluesseldienst](https://pixabay.com/users/Schluesseldienst-13794927/)\\n> License: [Pixabay](https://pixabay.com/service/license/)"},{"id":"id/2006","metadata":{"permalink":"/content/id/2006","source":"@site/posts/keep-your-apex-images-updated/index.md","title":"Keep Your APEX Images Updated","description":"Spider Web","date":"2020-07-25T17:03:59.000Z","formattedDate":"July 25, 2020","tags":[{"label":"orclapex","permalink":"/content/tags/orclapex"},{"label":"autonomous","permalink":"/content/tags/autonomous"},{"label":"database","permalink":"/content/tags/database"},{"label":"ords","permalink":"/content/tags/ords"}],"readingTime":1.525,"hasTruncateMarker":true,"authors":[{"name":"Adrian Png","title":"Senior Cloud Solutions Architect @ Insum","url":"https://github.com/fuzziebrain","imageURL":"https://github.com/fuzziebrain.png","key":"fuzziebrain"}],"frontMatter":{"title":"Keep Your APEX Images Updated","tags":["orclapex","autonomous","database","ords"],"slug":"id/2006","authors":["fuzziebrain"],"categories":["Technology","Infrastructure","Oracle Cloud"],"date":"2020-07-25T17:03:59.000Z"},"prevItem":{"title":"Update Free SSL Certificates for the OCI Load Balancer","permalink":"/content/id/2007"},"nextItem":{"title":"Running ORDS Against the Autonomous Database - Part 2","permalink":"/content/id/2005"}},"content":"![Spider Web](./assets/spider-223982_1280x800.jpg)\\n\\nAs Uncle Ben would say, \\"Remember, with great power, comes great responsibility.\\" With the release of [Oracle REST Data Services](https://www.oracle.com/database/technologies/appdev/rest.html) (ORDS) version 19.4.6, many of us were eager to deploy a customer-managed ORDS solution in order to support vanity URLs with [Oracle Application Express](https://apex.oracle.com) (APEX), myself [included](/content/id/2004). It has worked well for me until a few days ago, I noticed that my [Oracle JET](https://oracle.com/jet) charts weren\'t loading any more.\\n\\n\x3c!-- truncate --\x3e\\n\\nA quick check showed multiple [HTTP 404](https://www.w3.org/Protocols/rfc2616/rfc2616-sec10.html#sec10.4.5) errors were triggered. APEX was referencing a newer Oracle JET version, 8.3.0 insted of the stock 8.0.0.\\n\\nOne of the key benefits of running APEX on an [Oracle Autonomous Database](https://www.oracle.com/database/what-is-autonomous-database.html) is auto-patching, and clearly something got updated in the recent days. True enough, an updated Patch Set Bundle (PSE) was released on July 20, 2020 (Source: https://apex.oracle.com/download).\\n\\nOn the [Known Issues](https://www.oracle.com/tools/downloads/apex-downloads/apex-201-known-issues.html) page, it states:\\n\\n> With 20.1, we integrated the following third party libraries: Oracle JET 8.0.0, jQuery 3.4.1, jQuery Migrate 3.0.1, and jQuery FullCalendar 3.10.0. Upgraded versions of these libraries have been made available: Oracle JET 8.3.0, jQuery 3.5.1, jQuery Migrate 3.3.0, and jQuery FullCalendar 3.10.2.\\n\\nHere\'s what I did to get my server updated.\\n\\n1. Login to [Oracle Support](https://support.oracle.com) and search `30990551`, or simply click [here](https://support.oracle.com/epmos/faces/PatchDetail?patchId=30990551)\\n1. Download the patch. The file name should be `p30990551_2010_Generic.zip`.\\n    > Important\\n    >\\n    > You will need an active support contract in order to download the PSE.\\n1. Upload the patch to the Compute instance hosting ORDS. Place the file in the `/tmp` directory.\\n1. Login to the server as the user `opc`, using SSH.\\n1. Set the environment variables:\\n    ```bash\\n    RELEASE=20.1.0.00.13\\n    PATCHNUM=30990551\\n    PATCHFILE=p30990551_2010_Generic.zip\\n    ```\\n1. Execute the following:\\n    ```bash\\n    mkdir -p /tmp/$RELEASE\\n    unzip /tmp/$PATCHFILE \\"$PATCHNUM/images/*\\" -d /tmp/$RELEASE\\n    cp -R /tmp/$RELEASE/$PATCHNUM/images/* /opt/oracle/apex/images/$RELEASE\\n    rm -rf /tmp/$RELEASE\\n    ```\\n\\nThat\'s it!\\n\\n> <u>Photo Credits</u>\\n>\\n> Photo by [ArtTower](https://pixabay.com/users/arttower-5337/)"},{"id":"id/2005","metadata":{"permalink":"/content/id/2005","source":"@site/posts/running-ords-against-the-autonomous-database-part-2/index.md","title":"Running ORDS Against the Autonomous Database - Part 2","description":"Old pipes and valves","date":"2020-07-01T20:46:45.000Z","formattedDate":"July 1, 2020","tags":[{"label":"ords","permalink":"/content/tags/ords"},{"label":"rest","permalink":"/content/tags/rest"},{"label":"orclapex","permalink":"/content/tags/orclapex"},{"label":"autonomous","permalink":"/content/tags/autonomous"},{"label":"database","permalink":"/content/tags/database"},{"label":"load balancer","permalink":"/content/tags/load-balancer"}],"readingTime":6.265,"hasTruncateMarker":true,"authors":[{"name":"Adrian Png","title":"Senior Cloud Solutions Architect @ Insum","url":"https://github.com/fuzziebrain","imageURL":"https://github.com/fuzziebrain.png","key":"fuzziebrain"}],"frontMatter":{"title":"Running ORDS Against the Autonomous Database - Part 2","tags":["ords","rest","orclapex","autonomous","database","load balancer"],"slug":"id/2005","authors":["fuzziebrain"],"categories":["Technology","Infrastructure","Oracle Cloud"],"date":"2020-07-01T20:46:45.000Z"},"prevItem":{"title":"Keep Your APEX Images Updated","permalink":"/content/id/2006"},"nextItem":{"title":"Running ORDS Against the Autonomous Database","permalink":"/content/id/2004"}},"content":"![Old pipes and valves](./assets/pipes-5146458_1280x800.jpg)\\n\\nIn my [previous post](/content/id/2004/), we looked at installing and configuring [Oracle REST Data Services](https://www.oracle.com/ca-en/database/technologies/appdev/rest.html) (ORDS) and have it work with [Oracle Application Express](https://apex.oracle.com) (APEX) that comes preinstalled with every [Oracle Autonomous Database](https://www.oracle.com/database/what-is-autonomous-database.html) (ADB). Like APEX, ORDS is also included as a feature of the ADB. However, there are certain situations where you might like to manage your own instance of ORDS.\\n\\n\x3c!-- truncate --\x3e\\n\\nThree key reasons stand out for me:\\n\\n1. Using my own domain for my APEX applications.\\n1. Managing the number of database connections consumed by ORDS for my Always Free ADB that comes with a limit of 20 concurrent database sessions.\\n1. Restricting \\"direct\\" access to the ADB, for example, to connect to the database using external tools like [SQL Developer and SQLcl](https://www.oracle.com/database/technologies/appdev/sqldeveloper-landing.html).\\n\\nThe figure below depicts the topology that I will build for my APEX applications.\\n\\n![Planned topology](./assets/topology.png)\\n\\nNote that I chose a single subnet for the Load Balancer and Compute. The latter may be deployed in a separate subnet for further isolation.\\n\\n## Load Balancers\\n\\nUnder the [Oracle Cloud Free Tier](https://oracle.com/cloud/free) offering, users are provided with one *Always Free* load balancer. It has a special bandwidth size called *Micro* and is limited to a maximum of 10 Mbps.\\n\\nGo to the Load Balancers page. You will find it under the Networks menu item. Click on the blue *Create Load Balancer* button to begin.\\n\\n![Create Load Balancer Wizard](./assets/screen01.png)\\n\\nIn the first step, do the following:\\n\\n* Provide a suitable name.\\n* Set the visibility type to *Public*.\\n* Set the total bandwidth to *Micro*. Note that this size is not selectable if you have already created one, or are not in your Oracle Cloud Infrastructure (OCI) home region.\\n* Select the appropriate Virtual Cloud Network (VCN) and subnet that the load balancer should be placed in. Make sure that it is setup as a public network, an Internet Gateway and network route rules for bidirectional Internet traffic.\\n\\nIn the next step, leave the default settings. However, on the third and final step, change the listener to listen for HTTP traffic.\\n\\n![Create Load Balancer Wizard - Final Step](./assets/screen02.png)\\n\\nClick on *Submit* to create the load balancer. A Listener and Backend Set is also created in the process, but we should delete them both once the load balancer is up.\\n\\nOnce created, the Load Balancer Details page will display the Reserved Public IP address it has been assigned with. Use this to update your DNS records.\\n\\n![Load Balancer Details page](./assets/screen03.png)\\n\\n> Remember to delete the listener and backend set created by default.\\n\\n## Backend Sets\\n\\nNext, create the backend set that includes the Compute instance hosting ORDS. On the left side of the Load Balancer Details page, you will find a list of Resources that can be managed. Click on *Backend Sets* and then click on the blue *Create Backend Set* button to launch the wizard.\\n\\nProvide a suitable name and then leave the default values for *Traffic Distribution Policy* and *Session Persistence*. Under Health Check, set the following:\\n\\n* **Protocol** - HTTP\\n* **Port** - 8080\\n* **URL Path** - /i/19.2.0.00.18/apex_version.txt\\n\\nAfter the backend set is created successfully, click on its name and add backends. You can access the list of backends from the Resources menu on the left of the Backend Set Information page.\\n\\nTo begin, click the blue *Add Backends* button and then select the Compute instance that is hosting ORDS. Be sure to change the port number to `8080`.\\n\\n![Add Backends Modal Dialog](./assets/screen04.png)\\n\\nFor traffic to flow between the load balancer and backends, appropriate ingress and egress security list rules are required. You may do this manually, but for convenience, you can have the wizard perform these tasks automaticallly.\\n\\nClick the *Add* button to complete the process.\\n\\n## Hostnames\\n\\nThis is an optional resource but is simple to create. Simply provide a name and the URL that will be associated with the listener.\\n\\n![Add Hostname Modal Dialog](./assets/screen05.png)\\n\\n## SSL/TLS Certificates\\n\\nWe will want to secure our access to the APEX instance. I will not go into details now on how to obtain a valid SSL/TLS certificate. To create this resource, you should have the following:\\n\\n1. SSL/TLS public certificate\\n1. CA certificate (while optional, I have found issues with REST clients validating with [Let\'s Encrypt](https://letsencrypt.org/) certificates)\\n1. Private key\\n\\n![Add Certificate Modal Dialog](./assets/screen06.png)\\n\\n> **All** certificates and key must be in the PEM format.\\n\\n> *Update [Sep 14, 2020]*\\n>\\n> I have made available a [project](https://github.com/fuzziebrain/oci-le-cert-manager) that helps perform some of these tasks. You can read more about it in this follow-up [post](/content/id/2007).\\n\\n## Listeners\\n\\nCreate two listeners, one each for listening to secure and non-secure traffic.\\n\\nThe HTTP listener should have the following attributes:\\n\\n* A suitable name\\n* An optional hostname\\n* Listen using the HTTP protocol\\n* Listen on port 80\\n* Use SSL unchecked\\n* The backend defined for the ORDS server selected\\n* Default timeout of 60 seconds (or leave it blank)\\n\\n![Create Listener Modal Dialog for HTTP Protocol](./assets/screen07.png)\\n\\nThe HTTPS listener should have the following attributes:\\n\\n* A suitable name\\n* An optional hostname\\n* Listen using the **HTTP** protocol\\n* Listen on port 443\\n* Use SSL checked\\n* The associated certificate selected\\n* Verify peer certificate unchecked\\n* The backend defined for the ORDS server selected\\n* Idle timeout set to 1800 seconds\\n\\n![Create Listener Modal Dialog for HTTPS Protocol](./assets/screen08.png)\\n\\n> The idle timeout I have suggested can be tweaked to your requirements. I have found 60 seconds to be too short for a functional APEX application. For example, file operations sometimes require a longer length of time to be completed successfully.\\n\\n## Other Resources\\n\\nThe instructions provided in the blog post gets you a basic setup to support a standard customer-managed ORDS instance. More often than not, you will have other requirements. For example, the [APEX Challenge Coin](https://apeks.app/sponsor-a-coin) website uses a path alias that links to the APEX application for submitting nominations. In web servers, these are usually addressed by creating URL rewrite rules that redirect you to the correct address. For this purpose, you may use a *Rule Set* and assign it to a listener. Here\'s a full list of rule types that you can create:\\n\\n* Access Control\\n* Access Method\\n* URL Redirect\\n* Request Header\\n* Response Header\\n* HTTP Header\\n\\n![Create Rule Set Modal Dialog](./assets/screen09.png)\\n\\nYou might also serve other resources hosted on other backend servers. For example, I host some static content on the apeks.app website. For that, I run an Apache Web Server that hosts a single static HTML page. I can create Path Route Sets that will redirect the browser to the appropriate backend server based on the supplied path. Any path that starts with `/i` or `/ords`, which is used by APEX and ORDS, hands off to the backend running ORDS. The default, and so every other path, will lead to the other web server.\\n\\n> *Update [Sep 14, 2020]*\\n>\\n> Thanks to [Robert Barton](https://disqus.com/by/disqus_RgMBmO0DI7/) for pointing out in the comments. I forgot to mention that once the path route set has been created, assign it to the target listener. In my case, this would be the listener with SSL enabled.\\n\\n![Create Path Route Set Modal Dialog](./assets/screen10.png)\\n\\n## Final Step\\n\\nLogin to the server hosting ORDS as the *opc* user and edit the *ORDS Service Script Configuration File*:\\n\\n```bash\\nsudo vi /etc/ords/ords.conf\\n```\\n\\nUncomment the line containing:\\n\\n```bash\\nJAVA_OPTIONS=-Dsecurity.forceHTTPS=true\\n```\\n\\nRestart the ORDS service:\\n\\n```bash\\nsudo systemctl restart ords\\n```\\n\\nThat\'s it!"},{"id":"id/2004","metadata":{"permalink":"/content/id/2004","source":"@site/posts/running-ords-against-the-autonomous-database/index.md","title":"Running ORDS Against the Autonomous Database","description":"Man repairing the engine of a car","date":"2020-07-01T00:49:05.000Z","formattedDate":"July 1, 2020","tags":[{"label":"ords","permalink":"/content/tags/ords"},{"label":"rest","permalink":"/content/tags/rest"},{"label":"orclapex","permalink":"/content/tags/orclapex"},{"label":"autonomous","permalink":"/content/tags/autonomous"},{"label":"database","permalink":"/content/tags/database"}],"readingTime":7.3,"hasTruncateMarker":true,"authors":[{"name":"Adrian Png","title":"Senior Cloud Solutions Architect @ Insum","url":"https://github.com/fuzziebrain","imageURL":"https://github.com/fuzziebrain.png","key":"fuzziebrain"}],"frontMatter":{"title":"Running ORDS Against the Autonomous Database","tags":["ords","rest","orclapex","autonomous","database"],"slug":"id/2004","authors":["fuzziebrain"],"categories":["Technology","Infrastructure","Oracle Cloud"],"date":"2020-07-01T00:49:05.000Z"},"prevItem":{"title":"Running ORDS Against the Autonomous Database - Part 2","permalink":"/content/id/2005"},"nextItem":{"title":"Happy 25th Anniversary Java","permalink":"/content/id/2003"}},"content":"![Man repairing the engine of a car](./assets/auto-repair-3691962_1280x720.jpg)\\n\\nThe [Oracle Autonomous Database](https://www.oracle.com/database/what-is-autonomous-database.html) (ADB) is Oracle\'s current flagship database product and rightly so! Each instance comes with self-tuning, self-patching and several Oracle Database Options that customer would otherwise have to purchase separate licenses for. Database developers will also be pleased to know that there are a suite of tools at your disposal including [Oracle Application Express](https://apex.oracle.com/) (APEX), [Oracle REST Data Services](https://www.oracle.com/ca-en/database/technologies/appdev/rest.html) (ORDS) and [Oracle Machine Learning Notebooks](https://www.oracle.com/database/technologies/datawarehouse-bigdata/oml-notebooks.html).\\n\\nThe minimum instance you can create will have 1 CPU and 1 TB of storage and with so much features, doesn\'t come cheap. Here\'s the good news though, Oracle provides **everyone** two *Always Free* ADBs through the [Oracle Cloud Free Tier](https://oracle.com/cloud/free) offering. Yes, that\'s a great deal for anyone wanting to host a simple web application on the Internet. It is however, not without limits.\\n\\n\x3c!-- truncate --\x3e\\n\\nUnlike the paid Autonomous Database, a free instance is limited to 1 CPU with no autoscaling, 8 GB of RAM and 20 GB of storage. It will also be shutdown after 7 days of inactivity, and even terminated if left in that state for too long. Another important limitation is the 20 concurrent database session limit imposed on the instance. These are crucial considerations that you will need to take into account when running production applications on the free instance.\\n\\nIf you can run your applications successfully within these limits, then ADBs are a great choice for deploying APEX applications for several reasons. The most obvious benefit is that you do not have to be an expert database administrator (DBA) to install, tune and manage the database. These are all handled automatically by Oracle. Upgrading and patching the database and APEX are either performed incrementally in the background or in some cases, initiated by the user with a simple button click.\\n\\nTypically, after installing APEX on the database, users will also have to install, configure and deploy ORDS. The software can either run in a standalone mode or deployed on a Java Enterprise Edition (JEE) container. For the ADB though, ORDS comes preinstalled on a separate, robust and scalable hardware infrastructure.\\n\\nNot surprisingly, that convenience comes with some restrictions. When the ADB is provisioned, you are assigned a cryptic URL that Oracle assigns and till today (June 30, 2020), *vanity URLs* are still not supported. That means, you may not assign a domain name to your APEX instance, e.g. http://example.com/orcl/apex.\\n\\nWhile you can use a web server and appropriate proxy rules, mask the APEX instance, it does have inherent security risks. I have also found it challenging to get Social Sign-In (OAuth2) working with APEX applications because of how URLs used in the authentication workflow are generated.\\n\\nAccess to the database and its tools are managed together. It is my understanding that it is not possible to assign different access rules. For example, perhaps we want users to access the APEX instance from the web, but only connect to the database using SQL Developer while on the corporate network. That\'s not possible at least for now.\\n\\nThankfully, with the release of ORDS version 19.4.6, it is now possible to install your own instance of ORDS alongside the ADB. You can find the feature description [here](https://www.oracle.com/tools/ords/ords-relnotes-1946.html).\\n\\n> BUG 30957867 - Implement feature to support alternative runtime user (ORDS_PUBLIC_USER)\\n\\nWith a customer-managed ORDS, we can create different entry points to the ADB. This ORDS instance can be placed in an Internet-facing subnet, while access to the ADB can be limited to the new ORDS instance and restricted networks by either using an Access Control List (ACL), or in the case of paid instances, assigned a private endpoint address.\\n\\nYou can find the full-length instructions from Oracle [here](https://docs.oracle.com/en/database/oracle/oracle-rest-data-services/19.4/aelig/installing-and-configuring-customer-managed-ords-autonomous-database.html#GUID-24F4D51F-0BAF-4D3B-A714-492ED02D1212), on how to set this up. I have however, summarised the steps here, as well as noted the issues I encountered and the solutions.\\n\\nThese instructions assume that you have the following provisioned:\\n\\n1. **Autonomous Database.** Any workload type is fine, and it can be either paid or Always Free.\\n2. **Always Free Micro-shape Compute Instance.** The Compute may be of any shape, but the assumption is that it runs Oracle Linux 7.8.\\n\\n> You may attempt to use Oracle Autonomous Linux 7.8, however, many of the packages required are installed using [YUM](https://wikipedia.org/wiki/Yum_(software)), and this prebuilt OS image has limited repositories available. While there\'s a workaround, you are advised in the documentation not to mess with the configuration. If you\'re interested to know what these are, please check with me privately.\\n\\n## Tasklist for ADB\\n\\n### User Accounts\\n\\nInstalling and configuring ORDS usually requires the following accounts:\\n\\n* APEX_PUBLIC_USER\\n* APEX_LISTENER\\n* APEX_REST_PUBLIC_USER\\n* ORDS_PUBLIC_USER\\n\\nThe first three are created in a typical APEX installation. The last one is created when we install ORDS. However, in ADB, these accounts are already created and we do not have the passwords, so clearly, a different approach is needed.\\n\\nIn this new ORDS installation method, we will use a different database user. `ORDS_PUBLIC_USER2` is the recommended name, but you don\'t have to name it so. You must however, as the *admin* user, assign the necessary privilege by executing the following commands in the database:\\n\\n```sql\\n-- 1. Create the database user.\\ncreate user ords_public_user2 identified by \\"duntellanyone\\";\\n\\n-- 2. Allow the user to connect to the database.\\ngrant connect to ords_public_user2;\\n\\n-- 3. Perform some magic.\\nbegin\\n    ords_admin.provision_runtime_role(\\n        p_user => \'ORDS_PUBLIC_USER2\'\\n        , p_proxy_enabled_schemas => true\\n    );\\nend;\\n/\\n```\\n\\n### Client Credentials\\n\\nDownload the wallet necessary for connecting to the ADB and then upload it securely, to the Compute instance\'s `/tmp` directory.\\n\\n## Tasklist for Compute\\n\\nAs an Oracle Cloud Infrastructure (OCI) customer, you will have access to software like Oracle Instant Client, Java runtimes and ORDS through a private YUM repository. Login to the server as the `opc` user using SSH and then enable the repository:\\n\\n```bash\\nsudo yum-config-manager --enable ol7_oci_included\\n```\\n\\nInstall ORDS using YUM, which will automatically install its dependencies include a supported Java Runtime Engine (JRE):\\n\\n```bash\\nsudo yum install -y ords\\n```\\n\\nThe package installation also creates and `oracle` OS user that will be used to configure and run the standalone server. Switch to that user:\\n\\n```bash\\nsudo su - oracle\\n```\\n\\nSet the `ORDS_CONFIG_DIR` variable to the base directory where ORDS configuration files will be stored, and then create the necessary directory structure:\\n\\n```bash\\nORDS_CONFIG_DIR=/opt/oracle/ords/config && mkdir -p $ORDS_CONFIG_DIR/ords/conf\\n```\\n\\nSet the ORDS configuration directory using the command `ords`:\\n\\n```bash\\nords configdir $ORDS_CONFIG_DIR\\n```\\n\\nIf you are running this on an Always Free Compute instance, chances are, you may encounter this error:\\n\\n```\\nCaused by: java.lang.OutOfMemoryError: Java heap space\\n```\\n\\nTry setting the `JAVA_OPTIONS` variable with an appropriate maximum heap size before reattempting to set the ORDS configuration directory. Run the command:\\n\\n```bash\\nexport JAVA_OPTIONS=-Xmx512M\\n```\\n\\nNext, set the following environment variables:\\n\\n```bash\\nORDS_USER=ORDS_PUBLIC_USER2\\nORDS_PASSWORD=duntellanyone\\nSERVICE_NAME=myadb_low\\nWALLET_BASE64=`base64 -w 0 /tmp/wallet_MYADB.zip`\\n```\\n\\nThen run the following script to create the required ORDS configuration files:\\n\\n```bash\\ncat << EOF > $ORDS_CONFIG_DIR/ords/conf/apex_pu.xml\\n<?xml version=\\"1.0\\" encoding=\\"UTF-8\\" standalone=\\"no\\"?>\\n<!DOCTYPE properties SYSTEM \\"http://java.sun.com/dtd/properties.dtd\\">\\n<properties>\\n  <entry key=\\"db.username\\">$ORDS_USER</entry>\\n  <entry key=\\"db.password\\">!$ORDS_PASSWORD</entry>\\n  <entry key=\\"db.wallet.zip.service\\">$SERVICE_NAME</entry>\\n  <entry key=\\"db.wallet.zip\\"><![CDATA[$WALLET_BASE64]]></entry>\\n</properties>\\nEOF\\n\\ncat << EOF > $ORDS_CONFIG_DIR/ords/defaults.xml\\n<?xml version=\\"1.0\\" encoding=\\"UTF-8\\" standalone=\\"no\\"?>\\n<!DOCTYPE properties SYSTEM \\"http://java.sun.com/dtd/properties.dtd\\">\\n<properties>\\n  <entry key=\\"plsql.gateway.enabled\\">true</entry>\\n  <entry key=\\"jdbc.InitialLimit\\">5</entry>\\n  <entry key=\\"jdbc.MaxLimit\\">10</entry>\\n</properties>\\nEOF\\n```\\n\\nOnce the configuration is done, logout and return to the OS as the `opc` user. Start the ORDS service using the following command:\\n\\n```bash\\nsudo systemctl start ords\\n```\\n\\nMake sure you have and OCI Security List rule that has port `8080` opened and accessible to your client machine. On the Compute instance, you will also need to ensure that the firewall port is opened. Run the following command:\\n\\n```bash\\nsudo firewall-cmd --zone=public --add-port 8080/tcp\\n```\\n\\nCheck that ORDS is accessible from your browser, e.g. http://example.com:8080/ords/apex. If so, save the OS firewall rule:\\n\\n```bash\\nsudo firewall-cmd --zone=public --add-port 8080/tcp --permanent\\n```\\n\\nEnable the ORDS service to start automatically on boot:\\n\\n```bash\\nsudo systemctl enable ords\\n```\\n\\n### APEX Image Files\\n\\nLogin as the `oracle` user again to  deploy the APEX image files.\\n\\nDownload the compatible APEX installer and extract the contents of the images directory to `/opt/oracle/apex/images/$RELEASE`, where the `RELEASE` must be set to the current release number in full.\\n\\nBased on what\'s current, set the following values:\\n\\n```bash\\nRELEASE=20.1.0.00.13\\nAPEX_FILE=/tmp/apex_20.1.zip\\n```\\n\\nThen execute the following:\\n\\n```bash\\nmkdir -p /opt/oracle/apex/images/$RELEASE\\nunzip $APEX_FILE -d /tmp/\\ncp -R /tmp/apex/images/* /opt/oracle/apex/images/$RELEASE\\nrm -rf /tmp/apex\\n```\\n\\nAfter starting ORDS the first time, the standalone properties file should be created. Edit the file `/opt/oracle/ords/config/ords/standalone/standalone.properties` and add the following two lines:\\n\\n```\\nstandalone.static.context.path=/i\\nstandalone.static.path=/opt/oracle/apex/images\\n```\\n\\nFinally reboot the server and make sure everything is running as expected.\\n\\nThat\'s it?\\n\\nNo it\'s not. Checkout my [next post](/content/id/2005/) on how to setup an Always Free Load Balancer to front the ORDS server you have just created."},{"id":"id/2003","metadata":{"permalink":"/content/id/2003","source":"@site/posts/happy-25th-anniversary-java/index.md","title":"Happy 25th Anniversary Java","description":"Java Challenge Coin","date":"2020-05-20T00:00:00.000Z","formattedDate":"May 20, 2020","tags":[{"label":"java","permalink":"/content/tags/java"},{"label":"challenge coin","permalink":"/content/tags/challenge-coin"},{"label":"award","permalink":"/content/tags/award"},{"label":"appreciation","permalink":"/content/tags/appreciation"},{"label":"community","permalink":"/content/tags/community"}],"readingTime":1.835,"hasTruncateMarker":true,"authors":[{"name":"Adrian Png","title":"Senior Cloud Solutions Architect @ Insum","url":"https://github.com/fuzziebrain","imageURL":"https://github.com/fuzziebrain.png","key":"fuzziebrain"}],"frontMatter":{"title":"Happy 25th Anniversary Java","tags":["java","challenge coin","award","appreciation","community"],"slug":"id/2003","authors":["fuzziebrain"],"categories":["Technology","Community","Java"],"date":"2020-05-20T00:00:00.000Z"},"prevItem":{"title":"Running ORDS Against the Autonomous Database","permalink":"/content/id/2004"},"nextItem":{"title":"A Call to Aid","permalink":"/content/id/2002"}},"content":"![Java Challenge Coin](./assets/duke-live-to-code.jpg)\\n\\nMy personal journey with programming started more than two decades ago. Back then, the one language that I was very passionate and determined to master was Java. Early in my career, I would find any opportunity to attend a Sun Microsystems event like the Sun Tech Days, where I had the opportunity to learn about the constantly evolving platform that went from desktop, to enterprise and then on mobile devices. I can still remember watching the cool things you could do with Lego Mindstorms and an embedded Java runtime, and the vision that [JXTA](https://wikipedia.org/wiki/JXTA) was going to revolutionise the Internet of Things with its state-of-the-art, peer-to-peer technology.\\n\\n\x3c!-- truncate --\x3e\\n\\nMore importantly though, I was inspired and even as a noob, I got involved in the community through the Singapore Java Users Group. While there have been many incarnations of JUGs in Singapore, SJUG was the group I grew up with. I am grateful to have been involved and the opportunity to get to know luminaries like Dennis Lee (a.k.a. smallfry), [Chew Boon Hong](https://twitter.com/chewboonhong) and [Chang Sau Sheong](https://twitter.com/sausheong).\\n\\nThen there were the Sun Evangelists who travelled to the lesser known place where I spent my growing years. As I watched them on stage, I admired, and perhaps envied them. And it became my dream that some day, I too could be travelling around talking about what I am passionate about. Always being at the cutting edge of technology, trying out cool new toys and then proudly showing everyone else what you can do it. These were my notable few:\\n\\n* Sang Shin a.k.a. [javapassion](https://twitter.com/javapassion)\\n* [Chuk-Mun Lee](https://www.linkedin.com/in/chukmunnlee)\\n* [Simon Ritter](https://twitter.com/speakjava)\\n* [Rags Srinivas](https://twitter.com/ragss)\\n\\nI never got to be a Sun Evangelist, but I did find a similar path and earned my spades!\\n\\nAs we commemorate the 25th anniversary of a technology that helped shape the Internet, I wanted to honour the people who inspired me and were in many ways, a role model to me. I will attempt to reach out to you and hopefully be able to send you a Java Challenge Coin as a little token of appreciation.\\n\\nFrom the bottom of my heart, **THANK YOU** for sharing your knowledge and passion."},{"id":"id/2002","metadata":{"permalink":"/content/id/2002","source":"@site/posts/a-call-to-aid/index.md","title":"A Call to Aid","description":"Singapore National Day Parade - March Past","date":"2020-03-21T10:19:35.000Z","formattedDate":"March 21, 2020","tags":[{"label":"orclapex","permalink":"/content/tags/orclapex"},{"label":"crisis","permalink":"/content/tags/crisis"},{"label":"covid-19","permalink":"/content/tags/covid-19"},{"label":"coronavirus","permalink":"/content/tags/coronavirus"},{"label":"pandemic","permalink":"/content/tags/pandemic"},{"label":"support","permalink":"/content/tags/support"},{"label":"technology","permalink":"/content/tags/technology"},{"label":"community","permalink":"/content/tags/community"},{"label":"techgivesback","permalink":"/content/tags/techgivesback"}],"readingTime":3.76,"hasTruncateMarker":true,"authors":[{"name":"Adrian Png","title":"Senior Cloud Solutions Architect @ Insum","url":"https://github.com/fuzziebrain","imageURL":"https://github.com/fuzziebrain.png","key":"fuzziebrain"}],"frontMatter":{"title":"A Call to Aid","tags":["orclapex","crisis","covid-19","coronavirus","pandemic","support","technology","community","techgivesback"],"slug":"id/2002","categories":["Technology","Community","Oracle Application Express"],"authors":["fuzziebrain"],"date":"2020-03-21T10:19:35.000Z"},"prevItem":{"title":"Happy 25th Anniversary Java","permalink":"/content/id/2003"},"nextItem":{"title":"APEX Challenge Coin 2020 Edition","permalink":"/content/id/2001"}},"content":"![Singapore National Day Parade - March Past](./assets/marching.jpg)\\n\\nEvery Singaporean Son grows up to respond to a codeword alongside an image of a soldier on guard.\\n\\n![](https://pbs.twimg.com/media/EKB4rNcUYAMn2FI?format=jpg&name=medium)\\n\\nWhen your codeword is flashed on the screen, you wrap up whatever you are doing, head home, pack, bid your goodbyes and then report to camp. Hence, when I heard this [call to help](https://joelkallman.blogspot.com/2020/03/seek-where-you-can-help-now.html), instinct kicked in and I had to respond.\\n\\n\x3c!-- truncate --\x3e\\n\\nI am blessed to have trained, worked and experienced first-hand what [Oracle Application Express](https://apex.oracle.com) (APEX) can do for organisations, especially in times of urgency and need. My own journey with APEX began with a weekend project that saved our human resource specialist hours of work collecting **54** spreadsheets weekly for **two** months; each time tallying up step counts and reporting the competition standings to the group.\\n\\nToday, I see many APEX developers sharing on social media, different ways they have used the technology to help their organisations navigate these challenging times. Others have used their skills and knowledge to help support operational activities at civic organisations and local governments. If you are one of them, please feel free to share your story by commenting below.\\n\\nIf you are looking to build rapid solutions with APEX, then here are useful resources that you might find useful.\\n\\n## Environment\\n\\nOrganisations already licensed to run an Oracle Database, good news, APEX can be installed on your database *free of charge*. If you do not already own one, then here are some options to consider.\\n\\n### Option A\\n\\nAPEX available out-of-the-box on the *Always Free* Oracle Autonomous Database (ADB) is the best way to get started. Begin by [signing up](http://oracle.com/cloud/free) for an Oracle Cloud Free Tier.\\n\\n### Option B\\n\\nIf your organisation is staffed with IT staff familiar with [Docker](https://docker.com) or [Vagrant](http://vagrantup.com/), there are many projects that will help stand up an APEX with a *free* [Oracle Database 18c Express Edition](http://oracle.com/xe). Here are some examples:\\n\\n* [docker-apex-stack](https://github.com/oracle/docker-images)\\n* [vagrant](https://github.com/oraclebase/vagrant) and [dockerfiles](https://github.com/oraclebase/dockerfiles) by [Tim Hall](https://twitter.com/oraclebase)\\n* [docker-db-apex-dev](https://github.com/Dani3lSun/docker-db-apex-dev) by [Daniel Hochleitner](https://twitter.com/Dani3lSun)\\n\\n### Option C\\n\\nLast but not least, you may [sign up](https://apex.oracle.com/pls/apex/f?p=4700&p_lang=en) for a *free* workspace on [apex.oracle.com](https://apex.oracle.com). While this is the easiest path to obtain an APEX development environment, it is not for production use. It also does not provide you direct access to the database that is sometimes needed in a software development workflow.\\n\\n## Learning\\n\\nThere are a wealth of videos that demonstrate the varioous features of APEX. I strongly recommend the APEX development team\'s YouTube [channel](https://www.youtube.com/channel/UCEpIXFjcQIztReQNLymvYrQ) for starters.\\n\\nThe open sourced APEX [curriculum](https://blogs.oracle.com/apex/oracle-apex-192-free-curriculum-now-available) is also available for those looking for a more structured training experience. Direct links to access these resources are as follows:\\n\\n* [Curriculum materials](http://www.oracle.com/technetwork/developer-tools/apex/learnmore/apex-curriculum-4490003.html)\\n* [Hands on labs on Oracle Learning Library](https://github.com/oracle/learning-library/tree/master/workshops/apex-en)\\n\\nFor getting started with the *Always Free* ADB and APEX, I highly suggest reading [Joel Kallman\'s](https://twitter.com/joelkallman) [blog post](https://blogs.oracle.com/apex/the-quick-guide-for-getting-started-with-apex-on-autonomous-database-serverless), or follow [Dimitri Gielis\'](https://twitter.com/dgielis) [series](http://dgielis.blogspot.com/2019/09/best-and-cheapest-oracle-apex-hosting.html) for a more comprehensive setup.\\n\\n## The Pledge\\n\\nIn response to Joel\'s call for APEX developers to step up, I am making myself available, *pro bono*, to the following organisations in Metro Vancouver, British Columbia, and East Asia:\\n\\n* Academia and research\\n* Municipal governments; and\\n* Small businesses\\n\\nIf you need a *remote* hand in getting started, e.g. install APEX, \\"applicationify\\" a spreadsheet, or just needed a conversation to help make a decision, please feel free to contact me at the following email address:\\n\\n![](./assets/email.png)\\n\\n## Closing\\n\\nWhen there\'s a need, the APEX community always responds. Looking back to 2018, it was amazing how a small team of Oracle tech professionals were able to band together quickly to help folks affected by *Hurrican Florence* get vital information on road closures and where to get gas.\\n\\n{% twitter https://twitter.com/NevianaZhgaba/status/1043934483479695360 %}\\n\\nToday, I am proud to see not only individual APEX developers stepping up, but businesses as well. Here\'s a list of examples I have come across so far:\\n\\n* [iAdvise](https://iadvise.eu/) in the Netherlands - https://www.linkedin.com/pulse/iadvise-biedt-hulp-strijd-tegen-covid-19-jonathan-van-vianen-/\\n* [Database Consulting](https://datacons.co.uk/) in the United Kingdom - https://apex.oracle.com/pls/apex/dataconsulting/r/gielda_pomocy\\n\\nLast but not least, I want to give a special shoutout to my [Insum](https://insum.ca) family. You can read more about Insum\'s COVID-19 response [here](https://insum.ca/covid19). If you require a more involved response including infrastructure resources support, please contact the company\'s Rapid Response Coordinator at [1-866-887-1670](tel:+18668871670) *ext.* 250 or email [covid19@insum.ca](mailto:covid19@insum.ca).\\n\\nWe will get there. Stay strong, remain healthy and be socially responsible.\\n\\n\u4e16\u754c\u52a0\u6cb9\uff01\\n\\n> <u>Photo Credits</u>\\n>\\n> Photo by [Brian Jeffery Beggerly](https://www.flickr.com/photos/beggs/)\\n> License: [CC BY 2.0](https://creativecommons.org/licenses/by/2.0/)\\n>\\n> Image was cropped. The original immage can be found [here](https://www.flickr.com/photos/beggs/6080505442/)"},{"id":"id/2001","metadata":{"permalink":"/content/id/2001","source":"@site/posts/apex-challenge-coin-2020-edition/index.md","title":"APEX Challenge Coin 2020 Edition","description":"Happy New Year!","date":"2020-01-22T19:21:23.000Z","formattedDate":"January 22, 2020","tags":[{"label":"orclapex","permalink":"/content/tags/orclapex"},{"label":"challenge coin","permalink":"/content/tags/challenge-coin"},{"label":"gratitude","permalink":"/content/tags/gratitude"},{"label":"memorabilia","permalink":"/content/tags/memorabilia"}],"readingTime":3.37,"hasTruncateMarker":true,"authors":[{"name":"Adrian Png","title":"Senior Cloud Solutions Architect @ Insum","url":"https://github.com/fuzziebrain","imageURL":"https://github.com/fuzziebrain.png","key":"fuzziebrain"}],"frontMatter":{"title":"APEX Challenge Coin 2020 Edition","tags":["orclapex","challenge coin","gratitude","memorabilia"],"slug":"id/2001","authors":["fuzziebrain"],"categories":["Technology","Community","Oracle Application Express"],"date":"2020-01-22T19:21:23.000Z"},"prevItem":{"title":"A Call to Aid","permalink":"/content/id/2002"},"nextItem":{"title":"Watch Your Weight","permalink":"/content/id/1922"}},"content":"![Happy New Year!](./assets/new-years-day-4649609_1920.jpg)\\n\\nProbably too late for New Year greetings, but not if you also celebrate the Lunar New Year that\'s coming really soon. For readers who, like me, celebrate this festival, I\'d like to take the opportunity to wish you \u65b0\u5e74\u5feb\u4e50\uff0c\u4e07\u4e8b\u5982\u610f!\\n\\nIt\'s now been almost a year since the [APEX Challenge Coin](/content/id/1727/) was released, and I must say, it\'s made 2019 a year to remember!\\n\\n\x3c!-- truncate --\x3e\\n\\nMany people have thanked me for initiating this project, but in my heart, I feel it is everyone who participated - from the givers to the receivers - that I should thank. The project outcome was more than I could imagine. Words aren\'t even enough to describe the feeling of knowing how much community members were touching each others hearts with simple thoughts and gestures. This world is a better place because of all of you, so thank you!\\n\\nI had intended to end the project when the last coin was given out in December 2019. However, when news broke that [Oracle Application Express (APEX)](https://apex.oracle.com) made the Gartner\'s Peer Insights [Customers\' Choice for Enterprise Low-Code Application Platforms](https://www.gartner.com/reviews/customers-choice/enterprise-low-code-application-platform), I knew at the moment, that there are and will be more APEX developers to show our appreciation too. I am thereby announcing the **launch of the 2020 Edition of the APEX Challenge Coin**!\\n\\nWhile the rules on how someone can earn a coin have not changed, there are a few small tweaks to how this year\'s coins will be distributed. Firstly, to help me keep my sanity, I am moving away from spreadsheets and will be using APEX to help me maintain records of whom these coins are going to. I\'ve got to eat my own dog food eh?\\n\\nThen there\'s the request/nomination process. Rather than rely solely on Twitter for accepting requests, this year, I have decided that the process should also be \\"APEX\'ified\\". If you have someone in mind that you\'d like to show your appreciation to, please go to this [website](https://apeks.app/sponsor-a-coin) to submit your nomination.\\n\\nMany have wondered about how the coins are serialised. I will make no statements to either confirm or deny those allegations, but suffice to say, the answer should be very obvious to a programmer. While I had issued the coins pretty much randomly last year, there were exceptions. For example, when I learned that the number \\"42\\" would actually help [Shakeeb Rahmen](https://twitter.com/shakeeb) find the meaning of life, I had no choice but to put it into the reserve pile. This year, I have decided to let sponsors select the number(s) they\'d like to gift and you can do so when submitting a nomination. Do note though that the coin\'s serial number is a primary key, duh!\\n\\nThere were about 90 coins distributed last year, and it will likely be the same number this year. I had imposed a limit on the number of coins a person could sponsor, and that was to help ensure a fair distribution of coins. That practice will continue going into 2020. What does that mean for folks who sponsored coins last year? For every coin that you had sponsored but have yet to submit a name, then that coin counts against your 2020 limit. For example, if you had sponsored three coins and submitted two names (and hopefully pictures) of the happy owners, then you\'ll have the opportunity to sponsor four this year. Submit that last name from 2019 and you\'ll fully restore your \\"credits\\" with me.\\n\\nLast but not least, I am planning to add a feature on the Sponsor a Coin website to map the global distribution of the coins. This was requested by [Oliver Lemm](https://twitter.com/OliverLemm) quite a while ago and I hope to have it done in the coming months. Stay tune for that, but meanwhile, think about that colleague, friend, customer or even family member, whom has made an impact on you or your company through their APEX development skills. Consider sponsoring them a coin. Believe me, the experience and result is magical!"},{"id":"id/1922","metadata":{"permalink":"/content/id/1922","source":"@site/posts/watch-your-weight/index.md","title":"Watch Your Weight","description":"Horizontal Old Weights","date":"2019-10-12T00:00:00.000Z","formattedDate":"October 12, 2019","tags":[{"label":"oracle","permalink":"/content/tags/oracle"},{"label":"cloud","permalink":"/content/tags/cloud"},{"label":"oci","permalink":"/content/tags/oci"},{"label":"orclapex","permalink":"/content/tags/orclapex"},{"label":"autonomous","permalink":"/content/tags/autonomous"},{"label":"alwaysfree","permalink":"/content/tags/alwaysfree"},{"label":"limits","permalink":"/content/tags/limits"}],"readingTime":4.99,"hasTruncateMarker":true,"authors":[{"name":"Adrian Png","title":"Senior Cloud Solutions Architect @ Insum","url":"https://github.com/fuzziebrain","imageURL":"https://github.com/fuzziebrain.png","key":"fuzziebrain"}],"frontMatter":{"title":"Watch Your Weight","tags":["oracle","cloud","oci","orclapex","autonomous","alwaysfree","limits"],"slug":"id/1922","authors":["fuzziebrain"],"categories":["Technology","Infrastructure","Oracle Cloud"],"date":"2019-10-12T00:00:00.000Z"},"prevItem":{"title":"APEX Challenge Coin 2020 Edition","permalink":"/content/id/2001"},"nextItem":{"title":"OGB Appreciation Day: Challenge Coin Sponsors","permalink":"/content/id/1921"}},"content":"![Horizontal Old Weights](./assets/horizontal-930716_1280x720.jpg)\\n\\nA reader left an interesting comment/question in my previous post on [Making the Move to the Autonomous Database](/content/id/1920/):\\n\\n> \\"Would You explain how can i check my on-premise DB to satisfy ADB size limitation 20G? What is the size 20G of?\\"\\n\\nThis is an excellent question to ask before embarking on any [Autonomous Database](https://www.oracle.com/database/autonomous-database.html) (ADB) migration plans and so I dug a bit deeper. \\n\\n\x3c!-- truncate --\x3e\\n\\nFolks moving applications and data from an [Oracle Database Express Edition](https://oracle.com/xe) (XE) instance, version 11gR2 or 18c, will probably not have problems fitting into a Free Tier Autonomous Database (ADB) since database sizes are capped at 11 and 12 GB respectively. You are allowed a maximum of 20 GB user for each Free Tier ADB provisioned. \\n\\nTo estimate the data size to be migrated, execute the following query as *SYS* on the source database:\\n\\n```sql\\nselect\\n  case grouping(owner) when 1 then \'Total\' else owner end as owner\\n  , round(sum(bytes) / power(2,10), 2) as space_usage_kb\\n  , round(sum(bytes) / power(2,20), 2) as space_usage_mb\\n  , round(sum(bytes) / power(2,30), 2) as space_usage_gb\\nfrom dba_segments\\nwhere owner in (\'CONTOSO_APP\', \'CONTOSO_WEB\', \'LOGGER\')\\ngroup by rollup(owner);\\n```\\n\\nOutput:\\n\\n```\\nOWNER                SPACE_USAGE_KB SPACE_USAGE_MB SPACE_USAGE_GB\\n-------------------- -------------- -------------- --------------\\nCONTOSO_APP               32,896.00          32.13           0.03\\nCONTOSO_WEB                6,720.00           6.56           0.01\\nLOGGER                     1,280.00           1.25           0.00\\nTotal                     40,896.00          39.94           0.04\\n```\\n\\nThat\'s great! My toy application only requires about 40 MB of storage, but after importing the data into the ADB, I get this on the Service Console?\\n\\n![ADB Service Console](./assets/adb-service-console-overview.png)\\n\\nAs it turns out, the total amount of storage used includes quite a few more components than anticipated. The following query should yield values that tally with the amount displayed on the Service Console.\\n\\n```sql\\nselect\\n  case grouping(tablespace_name) when 1 then \'Total\' else tablespace_name end as tablespace_name\\n  , round(sum(bytes) / power(2, 30), 2) as total_size_gb\\n  , round(sum(maxbytes) / power(2, 30), 2) as max_size_gb\\nfrom (\\n  select tablespace_name, bytes, maxbytes\\n  from dba_data_files\\n  where tablespace_name != \'SAMPLESCHEMA\'\\n  union all\\n  select \'TEMP\' as tablespace_name, bytes, maxbytes\\n  from dba_temp_files\\n)\\ngroup by rollup(tablespace_name);\\n```\\n\\nOutput:\\n\\n```\\nTABLESPACE_NAME                TOTAL_SIZE_GB   MAX_SIZE_GB\\n------------------------------ ------------- -------------\\nDATA                                    0.10     32,768.00\\nDBFS_DATA                               0.10     32,768.00\\nSYSAUX                                  2.07     32,768.00\\nSYSTEM                                  0.74     32,768.00\\nTEMP                                    2.03          6.00\\nUNDOTBS1                                0.42          1.00\\nUNDO_2                                  0.42          1.00\\nTotal                                   5.89    131,080.00\\n\\n8 rows selected.\\n```\\n\\nBased on the above output, it is clear that a more accurate measure of the ADB\'s storage footprint requires us to consider other \\"non-application\\" tablespaces, e.g. `SYSTEM`, `SYSAUX`, `TEMP`, `UNDO` and more! \\n\\nThe tablespace `DATA` is the default tablespace for schema users (think `USERS` tablespace) and is where your application data really lives. Given that `TEMP`, `UNDOTBS1` and perhaps `UNDO_2` have a max of 8 GB, it looks like application data really needs to remain within the confines of 12 GB. And even so, more needs to be understood about how the other tablespaces might grow as the database is used, before we can fully appreciate how much room we have to maneuver.\\n\\n## Oracle Advanced Compression\\n\\nIf you have used [Oracle Advanced Compression](https://www.oracle.com/database/technologies/advanced-compression.html), [available](https://docs.oracle.com/en/database/oracle/oracle-database/18/xelic/licensing-information.html#GUID-3BD43E8F-53C3-42F0-BBBD-B743FD41F951__VLDBDATAWAREHOUSINGANDBUSINESSINTEL-6DD7F365) in 18c XE, then your tables should remain compressed. While an Automonous Transaction Processing (ATP) database does not compress tables by default, it will do so if specified. For Autonomous Data Warehouse (ADW), compression is enabled by default.\\n\\n> **NOTE:**\\n>\\n> If you wanted to get some data to try out the following steps, use the `CUSTOMERS` table in schema `SH` that should be available to all ADBs:\\n> \\n> ```sql\\n  create table customers_sample as\\n  select * from SH.customers where rownum < 10000;\\n  ```\\n\\nFirst, check if the table already has compression enabled. Execute this query as the schema user:\\n\\n```sql\\nselect table_name, compression, compress_for\\nfrom user_tables\\nwhere table_name = \'CUSTOMERS_SAMPLE\';\\n```\\n\\nOutput:\\n\\n```\\nTABLE_NAME                     COMPRESS COMPRESS_FOR                  \\n------------------------------ -------- ------------------------------\\nCUSTOMERS_SAMPLE               DISABLED\\n```\\n\\nNext, estimate the amount of storage used by this table:\\n\\n```sql\\nselect \\n  segment_name\\n  , bytes / power(2, 10) as size_kb\\n  , bytes / power(2, 20) as size_mb\\n  , bytes / power(2, 30) as size_gb\\nfrom user_segments\\nwhere 1 = 1\\n  and segment_name = \'CUSTOMERS_SAMPLE\' \\n  and segment_type = \'TABLE\'\\n;\\n```\\n\\nOutput:\\n\\n```\\nSEGMENT_NAME                        SIZE_KB      SIZE_MB      SIZE_GB\\n------------------------------ ------------ ------------ ------------\\nCUSTOMERS_SAMPLE                    3072.00         3.00         0.00\\n```\\n\\nThe `DBMS_COMPRESS.GET_COMPRESSION_RATIO` is a built-in proceedure that could be used to estimate the amount of space savings. The code below checks the compression ratio for table `CUSTOMERS_SAMPLE` with Advanced Row Compression.\\n\\n```\\ndeclare\\n  c_schema_name constant varchar2(20) := \'CONTOSO_WEB\';\\n  c_object_name constant varchar2(20) := \'CUSTOMERS_SAMPLE\';\\n\\n  l_blkcnt_cmp pls_integer;\\n  l_blkcnt_uncmp pls_integer;\\n  l_row_cmp pls_integer;\\n  l_row_uncmp pls_integer;\\n  l_cmp_ratio number;\\n  l_comptype_str varchar2(32767);\\nbegin\\n  dbms_compression.get_compression_ratio(\\n    scratchtbsname => \'DATA\'\\n    , ownname => c_schema_name\\n    , objname => c_object_name\\n    , subobjname => null\\n    , comptype => dbms_compression.comp_advanced\\n    , blkcnt_cmp => l_blkcnt_cmp\\n    , blkcnt_uncmp => l_blkcnt_uncmp\\n    , row_cmp => l_row_cmp\\n    , row_uncmp => l_row_uncmp\\n    , cmp_ratio => l_cmp_ratio\\n    , comptype_str => l_comptype_str\\n  );\\n  \\n  dbms_output.put_line(\'Compression ratio = \' || l_cmp_ratio);\\nend;\\n/\\n```\\n\\nNow add compression to the `CUSTOMERS_SAMPLE` table:\\n\\n```sql\\nalter table customers_sample \\nmove compress for all operations;\\n```\\n\\nThe `MOVE` clause allows you to encrypt existing data in the table, but bear in mind that there is sufficient amount of space for the transition. Don\'t perform this operation if you are already running out of space!\\n\\nCheck the table properties and storage sizes again:\\n\\n```sql\\nSQL> select table_name, compression, compress_for\\n  2  from user_tables\\n  3  where table_name = \'CUSTOMERS_SAMPLE\';\\n\\nTABLE_NAME                     COMPRESS COMPRESS_FOR                  \\n------------------------------ -------- ------------------------------\\nCUSTOMERS_SAMPLE               ENABLED  ADVANCED\\n\\nSQL> select \\n  2    segment_name\\n  3    , bytes / power(2, 10) as size_kb\\n  4    , bytes / power(2, 20) as size_mb\\n  5    , bytes / power(2, 30) as size_gb\\n  6  from user_segments\\n  7  where 1 = 1\\n  8    and segment_name = \'CUSTOMERS_SAMPLE\' \\n  9*   and segment_type = \'TABLE\';\\n\\nSEGMENT_NAME                        SIZE_KB      SIZE_MB      SIZE_GB\\n------------------------------ ------------ ------------ ------------\\nCUSTOMERS_SAMPLE                    2048.00         2.00         0.00\\n```\\n\\nLooks like I didn\'t achieve the amount of compression that was anticipated, but something is definitely better than nothing. There are other compression options I could use to achieve higher amounts of space savings, but deciding on the right approach is obviously more involved. The examples here are meant to demonstrate what is possible with ADBs."},{"id":"id/1921","metadata":{"permalink":"/content/id/1921","source":"@site/posts/ogb-appreciation-day-challenge-coin-sponsors/index.md","title":"OGB Appreciation Day: Challenge Coin Sponsors","description":"Happy toddler clapping","date":"2019-10-10T00:34:39.000Z","formattedDate":"October 10, 2019","tags":[{"label":"orclapex","permalink":"/content/tags/orclapex"},{"label":"oracleiaas","permalink":"/content/tags/oracleiaas"},{"label":"oci","permalink":"/content/tags/oci"},{"label":"administrators","permalink":"/content/tags/administrators"},{"label":"challenge coin","permalink":"/content/tags/challenge-coin"},{"label":"ThanksOGB","permalink":"/content/tags/thanks-ogb"}],"readingTime":0.75,"hasTruncateMarker":false,"authors":[{"name":"Adrian Png","title":"Senior Cloud Solutions Architect @ Insum","url":"https://github.com/fuzziebrain","imageURL":"https://github.com/fuzziebrain.png","key":"fuzziebrain"}],"frontMatter":{"title":"OGB Appreciation Day: Challenge Coin Sponsors","tags":["orclapex","oracleiaas","oci","administrators","challenge coin","ThanksOGB"],"categories":["Technology","Others"],"slug":"id/1921","authors":["fuzziebrain"],"date":"2019-10-10T00:34:39.000Z"},"prevItem":{"title":"Watch Your Weight","permalink":"/content/id/1922"},"nextItem":{"title":"Making the Move to the Autonomous Database","permalink":"/content/id/1920"}},"content":"![Happy toddler clapping](./assets/baby-2659226_1280x719.jpg)\\n\\nMy [#ThanksOGB](https://twitter.com/hashtag/ThanksOGB) will be short and sweet!\\n\\nMost in the community probably don\'t know, but I am a sucker for military stories and even more crazy about hard-to-find memorabilia. Earlier this year, I took a gamble and started a little [social experiment](/content/id/1727) and minted a small batch of APEX Challenge Coins.\\n\\nThe coins demonstrated our deep appreciation for APEX developers who have impacted our lives. Through the process of distributing them worldwide, I was blessed to have basked in the warmth of givers and the happiness of receivers. Words cannot describe the emotions and joy I felt each time someone opens his or her little mail packet to find a gift of gratitude.\\n\\nSo... for my #ThanksOGB appreciation post this year, I wanted to express again, my heartfelt gratitude to all APEX Challenge Coin sponsors for making this project a success. Thank you for supporting APEX developers!"},{"id":"id/1920","metadata":{"permalink":"/content/id/1920","source":"@site/posts/making-the-move-to-the-autonomous-database/index.md","title":"Making the Move to the Autonomous Database","description":"Packing boxes","date":"2019-09-26T18:29:30.000Z","formattedDate":"September 26, 2019","tags":[{"label":"oracle","permalink":"/content/tags/oracle"},{"label":"cloud","permalink":"/content/tags/cloud"},{"label":"oci","permalink":"/content/tags/oci"},{"label":"orclapex","permalink":"/content/tags/orclapex"},{"label":"autonomous","permalink":"/content/tags/autonomous"},{"label":"alwaysfree","permalink":"/content/tags/alwaysfree"},{"label":"data migration","permalink":"/content/tags/data-migration"}],"readingTime":7.025,"hasTruncateMarker":true,"authors":[{"name":"Adrian Png","title":"Senior Cloud Solutions Architect @ Insum","url":"https://github.com/fuzziebrain","imageURL":"https://github.com/fuzziebrain.png","key":"fuzziebrain"}],"frontMatter":{"title":"Making the Move to the Autonomous Database","tags":["oracle","cloud","oci","orclapex","autonomous","alwaysfree","data migration"],"slug":"id/1920","authors":["fuzziebrain"],"categories":["Technology","Infrastructure","Oracle Cloud"],"date":"2019-09-26T18:29:30.000Z"},"prevItem":{"title":"OGB Appreciation Day: Challenge Coin Sponsors","permalink":"/content/id/1921"},"nextItem":{"title":"Autonomous DB - The Developers\' Playground","permalink":"/content/id/1919"}},"content":"![Packing boxes](./assets/boxes-2719166_1280x720.jpg)\\n\\nIn my [previous post](/content/id/1919), I mentioned that I have created my [Oracle Cloud Free Tier](https://oracle.com/cloud/free/) account and provisioned an [Autonomous Database](https://www.oracle.com/database/what-is-autonomous-database.html) (ADB). Now, I\'m ready to move my existing [Oracle Application Express](https://apex.oracle.com/) (APEX) to this new environment. Next, I\'ll describe the steps I took to move (1) workspace, (2) application and (3) data, from the old to new database.\\n\\n\x3c!-- truncate --\x3e\\n\\n## Scope\\n\\nTo better illustrate the move process, let\'s create some fictious information about what I am about to move.\\n\\n| Property | Value |\\n|-|-|\\n| Source server hostname | dev.contoso.com |\\n| Source database name | XE |\\n| Database dump directory on source server | `/tmp/cloud-migration` |\\n| Dumpfile filename | `cloud-migration.dmp` |\\n| Source data pump target directory | `CLOUD_MIGRATION_DIR` |\\n| Target database name | ATP01 |\\n| Oracle Cloud Region | Ashburn, United States |\\n| Object Storage namespace | abcdefgh |\\n| Bucket name | cloud-migration |\\n| Workspace ID | 1234567890 |\\n| Workspace name | CONTOSO_WEB |\\n| Workspace administrator username | admin |\\n| Workspace schema name | contoso_apps |\\n| Auxiliary schema names | logger |\\n| APEX applications | 1000 - Contoso Web; 1400 - Contoso Content Management System |\\n| Working directory on workstation | `/home/jdoe/cloud-migration` |\\n\\n> **Note**\\n>\\n> The source server does not expose the database port 1521. Connection to the database from my workspace requires using a SSH tunnel.\\n\\n### Tools Required\\n\\nInstall the following software in the environment that \\n\\n* [SQLcl](https://www.oracle.com/database/technologies/appdev/sqlcl.html)\\n* [Oracle Instant Client](https://www.oracle.com/database/technologies/instant-client.html)\\n    * Basic\\n    * Tools (includes the Oracle Data Pump utilities expdp and impdp)\\n\\n## Export Components from Old Database\\n\\n### Export APEX Workspace and Applications\\n\\n1. Start SQLcl:\\n    ```bash\\n    sql /nolog\\n    ```\\n1. Create SSH tunnel and local port forwarding:\\n    ```sql\\n    sshtunnel oracle@dev.contoso.com -L 1523:localhost:1521\\n    ```\\n1. Connect to the dataabse as *contoso_apps*:\\n    ```sql\\n    connect contoso_apps/secret@//localhost:1523/XE\\n    ```\\n1. Obtained the workspace ID:\\n    ```sql\\n    select to_char(workspace_id) from apex_workspaces where workspace = \'CONTOSO_WEB\';\\n    ```\\n1. Export the workspace:\\n    ```sql\\n    apex export -workspaceid 1234567890 -expWorkspace\\n    ```\\n\\n    > This will produce a file named after the workspace ID, e.g. `w1234567890.sql`\\n1. Export **all** APEX applications in the workspace:\\n    ```sql\\n    apex export -workspaceid 1234567890\\n    ```\\n\\n    > Optionally, specify only the applications by id, using the `-applicationid` parameter. The export process would produce files named after the application ID, e.g. `f1000.sql` and `f1400.sql`.\\n1. If there are any REST services to be migrated, then execute the following:\\n    ```sql\\n    spool rest_services.sql\\n    rest export\\n    spool off\\n    ```\\n1. Logout.\\n1. If REST services were exported, then execute the following:\\n    ```bash\\n    echo \\"/\\" >> rest_services.sql\\n    ```\\n\\n### Export Schemas\\n\\nLogin to the source database using SSH as user *oracle*. The subsequent commands in this section will be executed on the source server.\\n\\n1. Login to the database as *SYSDBA*:\\n    ```sql\\n    sqlplus / as sysdba\\n    ```\\n1. Create the target data dump directory:\\n    ```sql\\n    host mkdir /tmp/cloud-migration\\n    ```\\n1. Create the directory object:\\n    ```sql\\n    create directory CLOUD_MIGRATION_DIR as \'/tmp/cloud-migration\';\\n    ```\\n1. Grant the *system* user `READ` and `WRITE` privileges:\\n    ```sql\\n    grant read, write on directory CLOUD_MIGRATION_DIR to system;\\n    ```\\n1. Exit SQL*Plus.    \\n1. Run the export data pump utility (expdp) to export the schema objects:\\n    ```bash\\n    expdp system schemas=logger,contoso_apps  \\\\\\n      directory=CLOUD_MIGRATION_DIR dumpfile=cloud-migration.dmp \\\\\\n      logfile=exp_cloud-migration.log\\n    ```\\n1. Logout from the source server.\\n\\n### Preparing for Import\\n\\n1. Grab the dumpfile (and log file if necessary):\\n    ```bash\\n    scp -r oracle@dev.contoso.com:/tmp/cloud-migration/* .\\n    ```\\n1. The local working directory `/home/jdoe/cloud-migration` should now contain the following files:\\n    * `cloud-migration.dmp`\\n    * `exp_cloud-migration.log`\\n    * `f1000.sql`\\n    * `f1400.sql`\\n    * `rest_services.sql`\\n    * `w1234567890.sql`\\n\\n## Import Components into ADB\\n\\n\\n### Import the Workspace\\n\\n1. Login to the ADB\'s APEX *Administration Services* as the *admin* user:\\n![Login screen](./assets/apex_instance_admin_login.png)\\n1. Use the *Manage Workspaces* utilities to import the workspace from the file `w1234567890.sql`.\\n1. During the import process, create the workspace schema. Set the password and space quota as required.\\n![Create workspace schema](./assets/create_schema.png)\\n\\n\\n### Prerequisites for Using IMPDP with ADB\\n\\nData can be imported into an ADB using Oracle Data Pump. However, since we do not access to the database server\'s file system, [Object Storage](https://docs.cloud.oracle.com/iaas/Content/Object/Concepts/objectstorageoverview.htm) must be used to deliver the dumpfile. \\n\\nThe official instructions to prepare for data pump can be found [here](https://docs.oracle.com/en/cloud/paas/atp-cloud/atpug/load-data-data-pump.html#GUID-30DB1EEA-DB45-49EA-9E97-DF49A9968E24). The following is a summary account of what I did.\\n\\n#### Object Storage\\n\\n1. Access the Oracle Cloud Object Storage.\\n![Access Object Storage](./assets/goto-object-storage.png)\\n1. Create a bucket and name it `cloud-migration`.\\n![Create bucket](./assets/create-bucket.png)\\n1. Take note of the Object Storage namespace (1) that is required for determining the URI for the dumpfile.\\n![Bucket details](./assets/bucket-details.png)\\n1. Upload the dumpfile `cloud-migration.dmp` to the bucket. \\n\\n#### Set Cloud Credentials for ADB\\n\\n1. [Generate an Auth Token](https://docs.cloud.oracle.com/iaas/Content/Identity/Tasks/managingcredentials.htm#two) for a user in the tenant with the necessary access to the bucket and its contents.\\n![Generate an Auth Token](./assets/generate-auth-token.png)\\n1. Set the Cloud Object Storage credentials:\\n    ```sql\\n    begin\\n      dbms_cloud.create_credential(\\n        credential_name => \'DEF_CRED_NAME\'\\n        , username => \'jdoe\'\\n        , password => \'the_generated_auth_token\'\\n      );\\n    end;\\n    /\\n    ```\\n\\n#### Instant Client Setup for ADB\\n\\nOfficial instructions for setting up your system to connect to the database using the tools provided by the Oracle Instant Client, including the Oracle Data Pump utilies needed for importing the data, can be found [here](https://docs.oracle.com/en/cloud/paas/atp-cloud/atpug/connect-preparing.html#GUID-EFAFA00E-54CC-47C7-8C71-E7868279EF3B).\\n\\nHere\'s a quick overview of what I did:\\n\\n1. Download the Cient Credentials (Wallet) from the ADB\'s Service Console.\\n![Download wallet](./assets/download-wallet.png)\\n1. Set the enviroment variable `TNS_ADMIN`:\\n    ```bash\\n    export TNS_ADMIN=/home/jdoe/.wallets/atp01\\n    ```\\n\\n    > The Oracle Wallet, `sqlnet.ora` and `tnsnames.ora` files will be stored in this path. The directory contents are required by the Oracle Instant Client tools to resolve the network service names for connecting to the ADB. The environment variable **must** be set prior to running any of these applications. \\n1. Create the wallet directory:\\n    ```bash\\n    mkdir -p $TNS_ADMIN\\n    ```\\n1. Extract the contents of the ZIP file into the wallet directory:\\n    ```bash\\n    unzip wallet_ATP01.zip -d $TNS_ADMIN\\n    ```\\n1. The wallet contains a `sqlnet.ora` file. Replace the path `?/network/admin`, specified for the parameter `DIRECTORY`, with the wallet\'s path (`TNS_ADMIN`).\\n    ```bash\\n    sed -i .bak \\"s|?/network/admin|$TNS_ADMIN|g\\" $TNS_ADMIN/sqlnet.ora\\n    ```\\n\\n### Create Auxiliary Schemas\\n\\n1. Launch SQLcl:\\n    ```bash\\n    sql /nolog\\n    ```\\n1. Login to the ADB using SQLcl as *admin*, the administrator account created when the database was provisioned.\\n    ```sql\\n    connect admin/supersecret@atp01_high\\n    ```\\n1. Create the auxiliary user and grant needed privileges and quota. For example:\\n    ```sql\\n    -- Create user and set quota on DATA, the tablespace for users in ADB.\\n    create user logger identified by \\"secret\\" quota 50M on data;\\n\\n    -- Grant privileges required by Logger.\\n    grant connect, create view, create job, create table, create sequence\\n      , create trigger, create procedure, create any context to logger;\\n    ```\\n1. Logout from the database.\\n\\n### Import Schema Objects\\n\\n1. Run the import data pump utility (impdp), avoiding the recreation of certain objects as specified by the `exclude` option:\\n    ```bash\\n    impdp admin@atp01_high directory=data_pump_dir credential=def_cred_name \\\\\\n      dumpfile=https://objectstorage.us-ashburn-1.oraclecloud.com/n/abcdefgh/b/cloud-migration/o/cloud-migration.dmp \\\\\\n      schemas=logger,contoso_apps parallel=16 transform=segment_attributes:n \\\\\\n      transform=dwcs_cvt_iots:y transform=constraint_use_default_index:y \\\\\\n      exclude=cluster,db_link,user,tablespace_quota,default_role\\n    ```\\n\\n    > * Details about the DBMS_CLOUD Package File URI format can be found [here](https://docs.oracle.com/en/cloud/paas/atp-cloud/atpug/file-uri-formats.html#GUID-5D3E1614-ADF2-4DB5-B2B2-D5613F10E4FA). The namespace was noted in an earlier section of this post.\\n    > * The exclusions include the creation of a user and modifications of certain properties about the user. The schemas were created in earlier sections of this article. They are redundant and will likely result in errors.\\n    > * Take note of the data pump logs. If necessary, [download](https://docs.oracle.com/en/cloud/paas/atp-cloud/atpug/load-data-data-pump.html#GUID-CC7F258C-5D58-475B-8361-467C35FB0745) a copy for further analysis and future reference.\\n\\n### Import Applications\\n\\n1. Launch SQLcl:\\n    ```bash\\n    sql /nolog\\n    ```\\n1. Login to the ADB as the workspace schema user:\\n    ```sql\\n    connect contoso_apps/secret@atp01_high\\n    ```\\n1. Set the workspace:\\n    ```sql\\n    apex_util.set_workspace(\'CONTOSO_WEB\');\\n    ```\\n1. Load and execute each application export file to import the application to the target workspace:\\n    ```\\n    @f1000.sql\\n    @f1400.sql\\n    ```\\n1. Import the REST services if any:\\n    ```\\n    @rest_services.sql\\n    ```\\n1. Exit the application.\\n\\n## Post-Migration Tasks\\n\\nAfter migrating the applications and data, here are some recommended tasks:\\n\\n* {% fa check-square %} Check for missing grants and assign them as necessary.\\n* {% fa check-square %} Re-create and database links required by the application.\\n* {% fa check-square %} Search for objects like packages and triggers that did not compile successfully during the import process. Fix any issues and recompile the objects.\\n* {% fa check-square %} Note the issues highlighted in the <a href=\\"/content/id/1919\\">previous post</a> concerning workspace developer/administrator access.\\n* {% fa check-square %} Test all applications.\\n\\nHope I didn\'t miss any steps. If you do notice any or mistakes, please let me know in the comments below.\\n\\nThanks for reading!"},{"id":"id/1919","metadata":{"permalink":"/content/id/1919","source":"@site/posts/autonomous-db-the-developers-playground/index.md","title":"Autonomous DB - The Developers\' Playground","description":"Kids&apos; Playground","date":"2019-09-22T15:43:59.000Z","formattedDate":"September 22, 2019","tags":[{"label":"oracle","permalink":"/content/tags/oracle"},{"label":"cloud","permalink":"/content/tags/cloud"},{"label":"oci","permalink":"/content/tags/oci"},{"label":"orclapex","permalink":"/content/tags/orclapex"},{"label":"autonomous","permalink":"/content/tags/autonomous"},{"label":"alwaysfree","permalink":"/content/tags/alwaysfree"}],"readingTime":5.825,"hasTruncateMarker":true,"authors":[{"name":"Adrian Png","title":"Senior Cloud Solutions Architect @ Insum","url":"https://github.com/fuzziebrain","imageURL":"https://github.com/fuzziebrain.png","key":"fuzziebrain"}],"frontMatter":{"title":"Autonomous DB - The Developers\' Playground","tags":["oracle","cloud","oci","orclapex","autonomous","alwaysfree"],"slug":"id/1919","authors":["fuzziebrain"],"categories":["Technology","Infrastructure","Oracle Cloud"],"date":"2019-09-22T15:43:59.000Z"},"prevItem":{"title":"Making the Move to the Autonomous Database","permalink":"/content/id/1920"},"nextItem":{"title":"That One Thing","permalink":"/content/id/1918"}},"content":"![Kids&apos; Playground](./assets/playground-470402_1280x720.jpg)\\n\\nAs mentioned in a previous [post](/content/id/1919/), the [Free Tier](https://oracle.com/cloud/free/) for [Oracle Cloud](https://cloud.oracle.com) (OCI) is every Oracle Developer\'s dream. For a while now, I have hosted a personal [Oracle Application Express](https://apex.oracle.com) (APEX) instance on Linode. It currently runs an older Oracle Database 11gR2 [Express Edition](https://oracle.com/xe/) (XE), and I would like to upgrade it to version 18c. The key issue I had though, was that the existing virtual machine hosted on [Linode](https://www.linode.com/?r=41672b20d515344de465e9ed44c1a75356445597), only has one gigabyte of RAM and that does not meet the 18c XE minimum requirement of two. That left me procrastinating for a long time, but perhaps a blessing in disguise. With this new offering, the next steps are pretty obvious.\\n\\n\x3c!-- truncate --\x3e\\n\\nAfter [setting up](/content/id/1918/) my new OCI development environment, the next task I looked into was exporting an existing workspace, and then importing it into the new APEX instance, hosted on an [Autonomous Database](https://www.oracle.com/database/what-is-autonomous-database.html) (ADB). During the import process, the you will be asked to either assign an existing schema or create a new one. If you are using an exsiting schema, be sure to include the necessary database privileges for an APEX parsing schema. This all went pretty much as expected, very smoothly!\\n\\nHowever, my workspace did not have an Application Express user, commonly called \\"admin\\" (ADMIN), that had workspace access and administrative rights. Hence, I was unable to login to the migrated workspace.\\n\\nAPEX in ADB is configured to use database user accounts for workspace access. Similar to configuring the workspace access with LDAP, HTTP Header Variable or Social Sign In authentication schemes, a matching Application Express user is required. If your workspace user matches up with a database user, then that user should have no troubles logging in. This user assumes whatever rights the equivalent Application Express user has been granted.\\n\\nSuppose I have the following:\\n\\n* **Workspace**: myworkspace\\n* **Schema**: myschema\\n* **Administrator\'s username**: jdoe\\n\\nTo allow JDOE access to the workspace, create a database user with the same name. Do this using either SQL Developer, SQL*PLus or SQLcl. Instructions to connect to the ADB can be found [here](https://docs.oracle.com/en/cloud/paas/autonomous-data-warehouse-cloud/user/connect-tools.html). Login as ADMIN, the administrator account created when the ADB was provisioned.\\n\\nOnce connected, execute the command:\\n\\n```sql\\ncreate user jdoe identified by \\"mySuper!P@55\\";\\n```\\n\\nFor the purpose of authenticating to APEX, no specific privileges or quota\'s are necessary. Login to the workspace using the workspace and database user\'s credentials.\\n\\n![Set APEX account password](./assets/set-apex-account-password.png)\\n\\nThe Application Express account with the same username will need to have its password reset. If this is not done, you will continually be prompted to do so. Strangely, clicking on that big blue button and then using the password change dialog does not seem to perform this successfully for me. What worked was using the user and group management utilities and then forced setting the user\'s password.\\n\\nAlternatively, before exporting the workspace, create an APEX workspace administrator with the same username as its schema, e.g. MYSCHEMA. Note that the ADB\'s APEX Instance Administration functionality is severely limited and does not appear to allow management of workspace users and groups, so the user must be added from the old instance.\\n\\n## Workspace Authentication with Database Accounts\\n\\nHow Oracle has laid out the approach for managing the APEX environment is limiting, but for good reason. It also does present a different perspective on how an administrator manages developer access to the resource.\\n\\nWe know that with ADB, we have access to a variety of tools for using the dataabase:\\n\\n* APEX\\n* SQL Developer Web (SQLDEVWEB)\\n* Oracle ML SQL Notebooks (OML)\\n* SQL Developer (SQLDEV)\\n* SQLcl\\n* Oracle REST Data Services (ORDS); and\\n* Traditional connectors like OCI, JDBC and ODBC.\\n\\nAn indispensible item in an APEX developer toolchain is access to the database schema. The ability to browse, modify and compile schema objects are an essential part of what we do. Depending on company policies and best practices, there are varying access levels and approaches. For example, workspaces available on apex.oracle.com, only allows developers to access the database schema through SQL Workshop. I\'m sure you\'d agree, this impacts developer productivity. \\n\\nFor corporate APEX instances however, it\'s not uncommon for database administrators to dish out the schema credentials to developers, either in the form of usernames and passwords, or an encrypted SQLDEV connnection export file. But what happens when the password expires, or one of the developers decide to change the it?\\n\\nOne approach I\'m inclined to use these days is to create for developers, a personal account, and then grant them ability to proxy as the APEX schema user.\\n\\n```sql\\n-- As a privileged user, execute:\\nalter user myschema grant connect through jdoe;\\n```\\n\\nIn the context of APEX+ADB, here are some examples of how we could benefit from this approach:\\n\\n* JDOE uses **one** account to access APEX App Builder and workspace schema user using the tool of choice. For example, JDOE could connect to the database as MYSCHEMA by simply authenticating using the username `jdoe[myschema]` and logon with his/her password.\\n* DBAs can, through profiles, apply standards like password expiration and complexity. When a developer leaves the organisation, there is no need to reset the schema password and communicate that to everyone else involved.\\n\\n### SQL Developer Web\\n\\nTo enable access to a schema, it must first be REST-enabled. For schemas linked to APEX workspaces, one option is to use the RESTful Services utility. For other schemas though, the [official documentation](https://docs.oracle.com/en/database/oracle/sql-developer-web/19.1/sdweb/about-sdw.html#GUID-A79032C3-86DC-4547-8D39-85674334B4FE) dictates that we execute the procedure `ords_admin.enable_schema`, for example:\\n\\n```sql\\nbegin\\n  ords_admin.enable_schema(\\n    p_enabled => true\\n    , p_schema => \'JDOE\'\\n    , p_url_mapping_type => \'BASE_PATH\'\\n    , p_url_mapping_pattern => \'jdoe\'\\n    , p_auto_rest_auth => null\\n  );\\n  \\n  commit;\\nend;\\n```\\n\\nTo access SQLDEVWEB, JDOE would have to use the URL in the following pattern:\\n\\n```\\nhttps://<TENANT_CODE>-<ADB_NAME>.adb.<REGION_ID>.oraclecloudapps.com/ords/<SCHEMA_ALIAS>/_sdw/?nav=worksheet\\n```\\n\\n> *where*\\n>\\n> * `<TENANT_CODE>` - a random string assigned to all ADB resources in the tenant. This can be easily obtained using the links in the Service Console.\\n> * `<ADB_NAME>` - the ADB\'s database name.\\n> * `<REGION_ID>` - the [region](https://docs.cloud.oracle.com/iaas/Content/General/Concepts/regions.htm) identifier where the ADB instance was created in.\\n> * `<SCHEMA>` - the value specified in `p_url_mapping_pattern` above.\\n\\nUnfortunately, it appears that SQLDEVWEB currently does not support proxy user access. Hopefully this would be possible in the future.\\n\\n### Oracle ML SQL Notebooks\\n\\nI haven\'t done a lot with this tool, but if you are familiar with [Jupyter](https://jupyter.org/), it sounds very similar, just seemingly SQL-centric, designed for data analytics and a means to keep good records of data processing workflows.\\n\\nEnabling database accounts access to OML is pretty simple and straightforward. Either enable users through OML as ADMIN, or grant users the role `OML_DEVELOPER`. OML users are provisioned personal workspaces that they could then grant other developers access to. One scenario in how this might be useful, is for MYSCHEMA to grant JDOE access to its workspace for running analytics and visualization code that had been collected through an APEX application.\\n\\nAll these is cool isn\'t? And it\'s just the beginning!"},{"id":"id/1918","metadata":{"permalink":"/content/id/1918","source":"@site/posts/that-one-thing/index.md","title":"That One Thing","description":"A helping hand","date":"2019-09-21T02:21:54.000Z","formattedDate":"September 21, 2019","tags":[{"label":"oracle","permalink":"/content/tags/oracle"},{"label":"cloud","permalink":"/content/tags/cloud"},{"label":"oci","permalink":"/content/tags/oci"},{"label":"IaaS","permalink":"/content/tags/iaa-s"},{"label":"infrastructure","permalink":"/content/tags/infrastructure"},{"label":"orclapex","permalink":"/content/tags/orclapex"},{"label":"autonomous","permalink":"/content/tags/autonomous"}],"readingTime":2.315,"hasTruncateMarker":true,"authors":[{"name":"Adrian Png","title":"Senior Cloud Solutions Architect @ Insum","url":"https://github.com/fuzziebrain","imageURL":"https://github.com/fuzziebrain.png","key":"fuzziebrain"}],"frontMatter":{"title":"That One Thing","tags":["oracle","cloud","oci","IaaS","infrastructure","orclapex","autonomous"],"slug":"id/1918","authors":["fuzziebrain"],"categories":["Technology","Infrastructure","Oracle Cloud"],"date":"2019-09-21T02:21:54.000Z"},"prevItem":{"title":"Autonomous DB - The Developers\' Playground","permalink":"/content/id/1919"},"nextItem":{"title":"Guiding Search","permalink":"/content/id/1917"}},"content":"![A helping hand](./assets/baby-560891_1280x720.jpg)\\n\\nToo good to be true? No, it\'s true!!!\\n\\nAfter hearing the rumours for a few years now, Mr Larry Ellison finally [announced](https://www.youtube.com/watch?v=3pkV26sI7D0) the availablility of a Free Tier for the Oracle Cloud. The gift bundle includes **two** small compute instances and **two** [Autonomous Databases](https://www.oracle.com/database/what-is-autonomous-database.html). These offerings are a tiny fraction of what the Oracle Cloud provides, but are sufficient for Oracle Developers to maintain their own 24x7 development environments and possibly even production applications for small businesses and non-profit organisations.\\n\\nLike everyone else in the [Oracle Application Express (APEX)](https://apex.oracle.com) community, I wasted no time in getting my account provisioned and am currently working towards moving workspaces and applications from my old personal APEX instance, hosted on [Linode](https://www.linode.com/?r=41672b20d515344de465e9ed44c1a75356445597).\\n\\n\x3c!-- truncate --\x3e\\n\\nTo get started, simply click on this [link](https://www.oracle.com/cloud/free/) and sign up. The process is similar to what I had earlier described in this [post](/content/id/1903/). You will need to decide on a tenant name and home region. The home region is where all *Always Free* resources will be created. Afterwhich, you will be asked to provide your cellphone number and credit card information for verification. \\n\\n![Layout of resources](./assets/oci-architecture-diagram.png)\\n\\nI am an APEX developer by trade, so setting up a working APEX instance was top on the list. The Autonomous Database comes with APEX included, so setting that up was an easy sequence of mouse clicks and keystrokes. For my main database, I went with an Automous Transaction Processing (ATP) database. \\n\\nUnfortunately, on the backburner, is a convenient method for enabling \\"vanity URLs\\" that we are still waiting for Oracle to provide. Meanwhile, there are workarounds using a web server to proxy HTTP traffic, and so, to facilitate that, I also provisioned a compute instance and installed [Docker](https://www.docker.com). I intend to develop using other programming languages and so, running an Apache web server in a container made sense. Besides, running [Certbot](https://certbot.eff.org/) to manage [Let\'s Encrypt](https://letsencrypt.org/) certificates was much easier.\\n\\nIdeally, I would situate the ATP in a private subnet, away from the prying eyes of the Internet, but it is a shared infrastructure and there are limitations. There are ways to restrict database connections through the use of access control lists, but it doesn\'t allow, for example, the database to access a REST API running on a second compute instance over a private network.\\n\\nFor the second set of *Always Free* resources, I intend to setup an Autonomous Datawarehouse (ADW) and a [Jupyter](https://jupyter.org/) Notebook instance for refreshing my knowledge on Machine Learning, stay tuned to hear more about that.\\n\\nIf you needed more guidance on creating these resources, please check out some of my previous posts on creating and accessing these resources:\\n\\n* [Compute instance](/content/id/1904/)\\n* [Autonomous Database](/content/id/1911/)\\n\\nNote that since publishing these posts, the Oracle Cloud Infrastructure console has changed somewhat."},{"id":"id/1917","metadata":{"permalink":"/content/id/1917","source":"@site/posts/guiding-search/index.md","title":"Guiding Search","description":"The holy grail of search","date":"2019-09-18T01:15:44.000Z","formattedDate":"September 18, 2019","tags":[{"label":"orclapex","permalink":"/content/tags/orclapex"},{"label":"features","permalink":"/content/tags/features"},{"label":"faceted search","permalink":"/content/tags/faceted-search"},{"label":"facet","permalink":"/content/tags/facet"},{"label":"browsing","permalink":"/content/tags/browsing"},{"label":"search","permalink":"/content/tags/search"}],"readingTime":3.395,"hasTruncateMarker":true,"authors":[{"name":"Adrian Png","title":"Senior Cloud Solutions Architect @ Insum","url":"https://github.com/fuzziebrain","imageURL":"https://github.com/fuzziebrain.png","key":"fuzziebrain"}],"frontMatter":{"title":"Guiding Search","tags":["orclapex","features","faceted search","facet","browsing","search"],"slug":"id/1917","authors":["fuzziebrain"],"categories":["Technology","Programming","Oracle Application Express"],"date":"2019-09-18T01:15:44.000Z"},"prevItem":{"title":"That One Thing","permalink":"/content/id/1918"},"nextItem":{"title":"A Case for Markdown","permalink":"/content/id/1916"}},"content":"![The holy grail of search](./assets/ipad-632394_1280x720.jpg)\\n\\nIf you asked me, I think [Mike Hichwa\'s](https://twitter.com/mikehichwa1) favourite 19.2 [new feature](https://apex.oracle.com/pls/apex/f?p=192192:FEATURES) is *Faceted Search*. Since the [early adopter](https://tryapexnow.com/) programme was launched on September 13, 2019, Mike has tweeted seven times about the EA (up till Sep 15).\\n\\n{% twitter https://twitter.com/mikehichwa1/status/1172569124528017410 %}\\n\\n\x3c!-- truncate --\x3e\\n\\nYou can find his other tweets here, describing how fast and easy it is to build these advanced search pages:\\n* https://twitter.com/mikehichwa1/status/1172573900871389184\\n* https://twitter.com/mikehichwa1/status/1173017312238030848\\n* https://twitter.com/mikehichwa1/status/1173019129504100352\\n* https://twitter.com/mikehichwa1/status/1173359613573353473\\n\\nI share the same excitement and here\'s why.\\n\\nFor a little over two years, I was involved in a multi-centre research project that sought to collect and store donor biospecimen from across the province of British Columnbia. There were very strict ethics and privacy protocols on not only specimen collection, but also the process of obtaining and storing patient and specimen data. Deidentified data would be aggregated in a web portal for biomedical scientists to search and request suitable biomaterial for their studies. You can read more about the project [here](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2785772/).\\n\\nMy then supervisor, Mark Wilkinson, was a huge proponent of the [Semantic Web](https://www.w3.org/standards/semanticweb/) and was meticulous about storing accurate metadata and making data easily searchable by both human and machines. Each row of biospecimn data must be carefully characterised using ontological concepts, and then exposed on the web as structured data.\\n\\nThe semantically rich data would then either be:\\n\\n* Consumed by machines (applications)\\n* Searched using a semantic query language like [SPARQL](https://www.w3.org/TR/rdf-sparql-query/); or\\n* Visually using a tool like [SIMILE Exhibit](http://simile.mit.edu/exhibit/)\\n\\nLet\'s build a Movie Database Application to illustrate how *Faceted Search* helps guide a user to search the data. Rather than list the steps, the video below demonstrates how to import the movie dataset and then create an application that includes both an Interactive Report (IR) and a Faceted Search interface.\\n\\n<youtube youTubeId=\\"Gt6EyCEhK5w\\" />\\n\\nSuppose we wanted to look for the following:\\n\\n> A list of dramas from Asia that were released before the year 2000. \\n\\nLet\'s first begin by searching for this data using the IR page:\\n\\n<youtube youTubeId=\\"PW5nd2YfnQ8\\" />\\n\\nNow do the same using the Faceted Search user interface (UI):\\n\\n<youtube youTubeId=\\"lNsZCH30QzQ\\" />\\n\\nIt typically takes about twice as long to get the desired result set using the IR. More importantly though, the facets provide a visual summary of the entire dataset, with a list of distinct groups of values and a row count of each group that help guide how a user might drilldown the data.\\n\\nBefore 19.2, such a sophisticated UI would require quite a bit more time to implement. In a previous project for a leading university library consortium, our team at [Insum](https://insum.ca) helped create a flexible search UI that incorporated Faceted Search using [Oracle Text](https://www.oracle.com/technetwork/database/features/index-098492.html) technology. The latter has other strengths and I would be surprised if there was a way to use it with this new APEX functionality.\\n\\nHere\'s a quick list of tasks to introduce Faceted Search to an existing application.\\n\\n1. After creating a page, add a Classic Report region using the table or query of your choice for data source. It doesn\'t appear to support both Interactive Reports and Interactive Grids.\\n1. Add a new region and select the new *Faceted Search* type. Change the page template to *Left Side Column* and place the new reigon on the *Left Column*.\\n1. Next, under *Source*, select the report for the *Filtered Region*\\n![](./assets/select-filtered-region.png)\\n1. Choose meaningful facets of the data and then decide on the following:\\n    * An appropriate facet type.\\n![Facet type options](./assets/choose-facet-type.png)\\n    * Source for the list of values from a choice of:\\n![List of values source types](./assets/choose-list-of-values-source.png)\\n    * Database column and data type that the facet represents:\\n![Specify the database column and data type](./assets/specify-database-column.png)\\n    * Optionally, customise how the facet is displayed:\\n![Customisation options](./assets/facet-customisations.png)\\n\\nThere\'s so much more to explore. Show this off to your fellow colleagues and be the office super hero!\\n\\nPlease bear in mind though, the information contained in this blog post is based on the early adopters\' version and we can safely assume that the functionality will evolve based on participants\' feedback."},{"id":"id/1916","metadata":{"permalink":"/content/id/1916","source":"@site/posts/a-case-for-markdown/index.md","title":"A Case for Markdown","description":"I\'m embarrassed. If you googled this title, you\'d find many other previous blog posts discussing the utility of Markdown. And in my humble opinion, it can be fully justified depending on the purpose.","date":"2019-09-15T11:45:06.000Z","formattedDate":"September 15, 2019","tags":[{"label":"orclapex","permalink":"/content/tags/orclapex"},{"label":"markdown","permalink":"/content/tags/markdown"},{"label":"features","permalink":"/content/tags/features"},{"label":"search","permalink":"/content/tags/search"},{"label":"oracle text","permalink":"/content/tags/oracle-text"}],"readingTime":3.445,"hasTruncateMarker":true,"authors":[{"name":"Adrian Png","title":"Senior Cloud Solutions Architect @ Insum","url":"https://github.com/fuzziebrain","imageURL":"https://github.com/fuzziebrain.png","key":"fuzziebrain"}],"frontMatter":{"title":"A Case for Markdown","tags":["orclapex","markdown","features","search","oracle text"],"slug":"id/1916","authors":["fuzziebrain"],"categories":["Technology","Programming","Oracle Application Express"],"date":"2019-09-15T11:45:06.000Z"},"prevItem":{"title":"Guiding Search","permalink":"/content/id/1917"},"nextItem":{"title":"Twin Meetup Groups","permalink":"/content/id/1915"}},"content":"<youtube youTubeId=\\"m6Ni3BEE1ww\\" />\\n\\nI\'m embarrassed. If you [googled](https://www.google.com/search?q=a+case+for+markdown) this title, you\'d find many other previous blog posts discussing the utility of [Markdown](https://wikipedia.org/wiki/Markdown). And in my humble opinion, it can be fully justified depending on the purpose.\\n\\n\x3c!-- truncate --\x3e\\n\\nWith the launch of the [early adopter programme](https://tryapexnow.com) for [Oracle Application Express](https://apex.oracle.com/) (APEX), developers now access to what the APEX Development Team has been working on these past few months. What appears not to be mentioned in the mew [features page](https://apex.oracle.com/pls/apex/f?p=192192:FEATURES) is a list of new page items.\\n\\n![A list of available page items in versions 19.1 and 19.2 EA](./assets/page-items--then-and-now.jpg)\\n\\nI think it\'s pretty obvious which of the two new page items stand out. Just check out these tweets that came out soon after the EA programme was announced.\\n\\n> @martindsouza https://twitter.com/martindsouza/status/1172589077947830272\\nAnd thank you @OracleAPEX for adding native markdown support. Been waiting for this for a long time!\\n\\n> @commi235 https://twitter.com/commi235/status/1172599174161981440\\nThanks #orclapex for finally giving us a modern, quick and easy way to style comments. \ud83d\udc4d\\n\\nAs a developer, I use many platforms like [GitHub](https://github.com/), [GitLab](https://gitlab.com/), [Jira](https://www.atlassian.com/software/jira), [StackOverflow](https://stackoverflow.com/) etc. daily. I usually write code, notes and other text using a simple editor like [Microsoft Visual Code](https://code.visualstudio.com/) (VS Code), focusing on content and structure than styling, as it should be! And because these platforms all support Markdown, I can easily transfer the text between applications.\\n\\nI am pretty sure most developers have felt the pain of formatting code when writing technical documentation using Microsoft Word.\\n\\nConsider this simple block of code:\\n\\n```sql\\ndeclare\\n  l_name varchar2(8) := \'John Doe\';\\nbegin\\n  dbms_output.put_line(\'Hello \' || l_name);\\nend;\\n```\\n\\nTo write this in MS Word, here\'s what I might typically do:\\n\\n1. Copy the code to the clipboard.\\n1. Paste the text in Word in **plaintext**.\\n1. Select the code and change the Font to something that\'s fixed-width, e.g. Courier.\\n1. Depending on the language, select each keyword and format to stand out.\\n\\nHow do we do the same with Markdown?\\n\\n1. Copy the code to the clipboard.\\n1. Paste the text in GitHub issue description for example.\\n1. Enclose the code with <code>&#96;&#96;&#96;sql</code> and <code>&#96;&#96;&#96;</code>.\\n\\nDone! The result:\\n\\n```sql\\ndeclare\\n  l_name varchar2(8) := \'John Doe\';\\nbegin\\n  dbms_output.put_line(\'Hello \' || l_name);\\nend;\\n```\\n\\nThat said, please consider who your application\'s end users are. Markdown today, is still very much an acquired taste. A typical end user would probably be more comfortable editing formatted text usng a WYSIWYG editor and that option is still there in APEX.\\n\\nThe other consideration is the support for Markdown content in the database. Consider the text:\\n\\n> Lorem Ipsum is simply dummy text of the printing and typesetting industry. Lorem Ipsum has been the industry\'s standard dummy text ever since the 1500s, when an unknown printer took a galley of type and scrambled it to make a type specimen book. It has survived not only five centuries, but also the leap into electronic typesetting, remaining essentially unchanged. It was popularised in the 1960s with the release of Letraset sheets containing Lorem Ipsum passages, and more recently with desktop publishing software like Aldus PageMaker including versions of Lorem Ipsum.\\n\\nSuppose the user wanted to bold the word \\"Aldus\\" but not \\"PageMaker\\". Using the Rich Text Editor page item, the string would be stored as `<strong>Aldus</strong> PageMaker`, while the Markdown Editor, would store that as `**Aldus** PageMaker`.\\n\\nTo query the data for the phrase \\"Aldus PageMaker\\", you can quite easily find the latter using regular expression (`regexp_like`). To correctly search the HTML-formatted string, either strip the HTML elements or use [Oracle Text](https://www.oracle.com/technetwork/database/enterprise-edition/index-098492.html). Using the appropriate `CONTAINS` operator, e.g. `ABOUT`, it is possible to ignore HTML markup. It might take a few more steps to also ignore Markdown syntax, but not impossible.\\n\\nThe other drawback with Markdown would be functionality like \\"Remove HTML\\" when choosing how an Interactive Report is displayed. There\'s yet to be a \\"Remove Markdown\\" option.\\n\\n![Remove HTML](./assets/remove-html.png)\\n\\nThere are often many solutions to a puzzle. Having options is great, and using Markdown in APEX out-of-the-box is certainly welcomed! One thing\'s for sure, moving towards simplicity doesn\'t mean walking backwards.\\n\\n*This blog post was proudly written in Markdown.*"},{"id":"id/1915","metadata":{"permalink":"/content/id/1915","source":"@site/posts/twin-meetup-groups/index.md","title":"Twin Meetup Groups","description":"Happy siblings","date":"2019-09-14T22:17:43.000Z","formattedDate":"September 14, 2019","tags":[{"label":"meetup","permalink":"/content/tags/meetup"},{"label":"user groups","permalink":"/content/tags/user-groups"},{"label":"Vancouver","permalink":"/content/tags/vancouver"},{"label":"orclapex","permalink":"/content/tags/orclapex"},{"label":"community","permalink":"/content/tags/community"},{"label":"getinvolved","permalink":"/content/tags/getinvolved"}],"readingTime":1.085,"hasTruncateMarker":true,"authors":[{"name":"Adrian Png","title":"Senior Cloud Solutions Architect @ Insum","url":"https://github.com/fuzziebrain","imageURL":"https://github.com/fuzziebrain.png","key":"fuzziebrain"}],"frontMatter":{"title":"Twin Meetup Groups","tags":["meetup","user groups","Vancouver","orclapex","community","getinvolved"],"slug":"id/1915","authors":["fuzziebrain"],"categories":["Technology","Programming","Oracle Application Express"],"date":"2019-09-14T22:17:43.000Z"},"prevItem":{"title":"A Case for Markdown","permalink":"/content/id/1916"},"nextItem":{"title":"Remote Coding with Visual Studio Code","permalink":"/content/id/1914"}},"content":"![Happy siblings](./assets/children-1720484_1280x720.png)\\n\\nIn a blink of an eye, it has been more than four years since I [set up](/content/id/426/) the [ORCLAPEX-YVR](https://meetup.com/orclapex-yvr) Meetup Group. This is a [Meetup](https://meetup.com) group in the beautiful city of Vancouver, British Columbia, that serves as a platform for promoting interest and supporting local [Oracle Application Express (APEX)](https://apex.oracle.com) professionals and enthusiasts. \\n\\nFor those unaware, as organisers, we pay a biannual organiser\'s fee. Thanks to [ODTUG\'s](https://www.odtug.com), these fees are kindly sponsored by [#GetInolved initiative](https://www.odtug.com/page/meetup). \\n\\n**Thank you ODTUG!**\\n\\nRecently, I had an epiphany...\\n\\n\x3c!-- truncate --\x3e\\n\\nThe fee allows organisers to create up to [**THREE**](https://help.meetup.com/hc/en-us/articles/360002865472-Can-I-organize-more-than-one-Meetup-group-) groups! Yeah, not just one. So here\'s what I\'d like to propose.\\n\\nIf you looked at the global [map](https://apex.world/ords/f?p=100:220:::NO:::) of all APEX meetup groups today, you will find \\"coldspots\\" where our community is underrepresented.\\n\\n![Snapshot of the global distribution of APEX Meetup Groups](./assets/orclapex-meetup-map.jpg)\\n\\nStarting with East and Southeast Asia, if you live in a city that does not already have an APEX meetup group, and wish to start one, please reach out to me *via* [Twitter](https://twitter.com/messages/compose?recipient_id=16314839) to discuss. Your responsibility is to lead and grow the group, while I\'ll deal with the paperwork in the backroom.\\n\\nLooking forward to hearing from friends across the Pacific!\\n\\n> <u>Photo Credits</u>\\n>\\n> *Picture of happy siblings*\\n> By [Jess Foami](https://pixabay.com/users/cuncon-3452518/)\\n> License: [Pixabay](https://pixabay.com/service/license/)"},{"id":"id/1914","metadata":{"permalink":"/content/id/1914","source":"@site/posts/remote-coding-with-visual-studio-code/index.md","title":"Remote Coding with Visual Studio Code","description":"Colouring Pens and Remote","date":"2019-09-03T17:50:15.000Z","formattedDate":"September 3, 2019","tags":[{"label":"oracle","permalink":"/content/tags/oracle"},{"label":"tools","permalink":"/content/tags/tools"},{"label":"python","permalink":"/content/tags/python"},{"label":"vscode","permalink":"/content/tags/vscode"},{"label":"ide","permalink":"/content/tags/ide"},{"label":"docker","permalink":"/content/tags/docker"}],"readingTime":1.015,"hasTruncateMarker":true,"authors":[{"name":"Adrian Png","title":"Senior Cloud Solutions Architect @ Insum","url":"https://github.com/fuzziebrain","imageURL":"https://github.com/fuzziebrain.png","key":"fuzziebrain"}],"frontMatter":{"title":"Remote Coding with Visual Studio Code","tags":["oracle","tools","python","vscode","ide","docker"],"slug":"id/1914","authors":["fuzziebrain"],"categories":["Technology","Programming","Tools"],"date":"2019-09-03T17:50:15.000Z"},"prevItem":{"title":"Twin Meetup Groups","permalink":"/content/id/1915"},"nextItem":{"title":"Day 28 - 30 Days on the Oracle Cloud","permalink":"/content/id/1913"}},"content":"![Colouring Pens and Remote](./assets/coloring-pens-3405626_1280x720.jpg)\\n\\nSupport for [remote development](https://code.visualstudio.com/docs/remote/remote-overview) is one of the newer features of Microsoft Visual Studio Code (VS Code) [announced](https://code.visualstudio.com/blogs/2019/05/02/remote-development) in 2019 that I\'m most excited about. With the [Remote Development Extension Pack](https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.vscode-remote-extensionpack), developers are able to work with code on remote servers using SSH, [Docker](https://docker.com) containers and [Windows Subsystem for Linux (WSL)](https://docs.microsoft.com/windows/wsl).\\n\\n\x3c!-- truncate --\x3e\\n\\nBefore, working on Windows almost exclusively, I have had the challenge of running and developing with Docker containers using a virtual machine (VM) running Linux. It gets a little tricky at times trying to synchronize the code between the host system and VM. Hence the [Remote SSH](https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.remote-ssh) extension was a great boost for productivity! I could work on my code on the host system while using VS Code\'s terminal to interact with the VM and Docker service.\\n\\nIt gets even better with Docker running on the host and the [Remote Container](https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.remote-containers) extension. Microsoft provides a few sample projects to get started with various programming languages, but I wanted something that was designed to work for the Oracle Developer and engineered to attach to an Oracle Database. And so, starting with [Python](https://python.org), I created this [starter kit](https://github.com/fuzziebrain/vscode-remote-oradev-python).\\n\\nPlease check out the [README](https://github.com/fuzziebrain/vscode-remote-oradev-python/blob/master/README.md) file for more details."},{"id":"id/1913","metadata":{"permalink":"/content/id/1913","source":"@site/posts/day-28-30-days-on-the-oracle-cloud/index.md","title":"Day 28 - 30 Days on the Oracle Cloud","description":"Reach the Cloud with a Helping Hand","date":"2019-07-29T01:01:37.000Z","formattedDate":"July 29, 2019","tags":[{"label":"oracle","permalink":"/content/tags/oracle"},{"label":"cloud","permalink":"/content/tags/cloud"},{"label":"oci","permalink":"/content/tags/oci"},{"label":"IaaS","permalink":"/content/tags/iaa-s"},{"label":"infrastructure","permalink":"/content/tags/infrastructure"},{"label":"support","permalink":"/content/tags/support"},{"label":"customer service","permalink":"/content/tags/customer-service"}],"readingTime":4.51,"hasTruncateMarker":true,"authors":[{"name":"Adrian Png","title":"Senior Cloud Solutions Architect @ Insum","url":"https://github.com/fuzziebrain","imageURL":"https://github.com/fuzziebrain.png","key":"fuzziebrain"}],"frontMatter":{"title":"Day 28 - 30 Days on the Oracle Cloud","tags":["oracle","cloud","oci","IaaS","infrastructure","support","customer service"],"slug":"id/1913","authors":["fuzziebrain"],"categories":["Technology","Infrastructure","Oracle Cloud"],"date":"2019-07-29T01:01:37.000Z"},"prevItem":{"title":"Remote Coding with Visual Studio Code","permalink":"/content/id/1914"},"nextItem":{"title":"Day 22 - 30 Days on the Oracle Cloud","permalink":"/content/id/1912"}},"content":"![Reach the Cloud with a Helping Hand](./assets/adventure-1807524_1280x720.jpg)\\n\\nWith only three days remaining for my 30-day trial, I will finally close these series by discussing my experiences dealing with various levels of support from Oracle. Having used the [Oracle Cloud (OCI)](https://cloud.oracle.com) both in my day job and as a technology evangelist, I have had the opportunity to engage Oracle support at various levels: sales, customer and technical.\\n\\n\x3c!-- truncate --\x3e\\n\\n## Sales Support\\n\\nWhen I first signed up for a trial, Oracle was quick to assign some to ensure that you were making full use of your trial account and credits. While I found their varying job titles confusing, generally, their goals were simple:\\n\\n1. Understand why you have signed up for the cloud trial.\\n2. Are there any technical challenges and if so, how they could setup some calls with inside technical specialists who could help address those issues.\\n\\nI didn\'t particularly feel a strong pressure to make any purchases, at least not during the initial calls and email correspondences, but perhaps because I was quick to make clear my intentions for participating in the trial.\\n\\n## Customer Support\\n\\n![Live chat options](./assets/chat_options.jpg)\\n\\nLast Friday, I applied for a new cloud trial account on behalf of a non-profit organization. Unfortunately, we had an issue with our DNS MX records and so I was worried that the activation email would be lost. Thankfully, and rather unexpectedly, I was able to locate a button on the to of the Oracle Cloud homepage that gave me several options to start a live chat with a customer support agent. I was close to the start of the chat hours that day and so, was able to connect with a support agent online. He was able to identify my application, but unfortunately, the issue had to be handled by a different department.\\n\\n> **Cloud Support Chat Hours:**\\n> 7.30 pm to 3.30 PM the next day, Pacific Time\\n\\nAlthough he wasn\'t able to help me directly, I definitely appreciated that I could at least \\"talk\\" to someone and have some options for next steps, rather than simply just hitting a brick wall. I did eventually get answers as to what went wrong with my trial application and you can read more about this in the [last section](#Three-Tries-and-I\u2019m-Out) of this article.\\n\\n## Technical Support (MOS)\\n\\nWhile I did not have the opportunity to engage the technical support team, the last serious incident I remember, involved an OCI [DB System](https://docs.cloud.oracle.com/iaas/Content/Database/Concepts/overview.htm) related to a cascade of unexpected events at the data centre. We had contacted [My Oracle Support](https://support.oracle.com) team as soon as the problem was discovered and were able to have an initial call within a short amount of time.\\n\\nAfter providing the necessary details (logs and error messages), the support agent was able to quickly assemble a team made up of specialists supporting the different tiers in the infrastructure. There was never a mention about reproducibility or anything like that, as these systems were running off Oracle\'s hardware and software systems.\\n\\nWe all have our bad days with support teams in general. However, at least for us, this encounter turned out just fine. The MOS team members were fully committed and eventually able to identify the cause and resolved all issues. The DB System was back online and functioning normally in a matter of hours. Phew!\\n\\n## Three Tries and I\'m Out\\n\\nJust last week, I wanted to initiate a new trial with an intention to progress into a paying account. To my dismay, I was told that it was rejected because I had signed up for one too many trials. Here\'s how they added up.\\n\\nI created my first trial to learn the process and also try out many of the different features outside of my regular scope of work. The next one was started to document my 30-day adventures, but then I got busy, had to to take a break and time eventually ran out. Earlier this month, I started my third trial so that I could complete writing up this series.\\n\\nEach customer is limited to only **three** trials, so be warned! While you may use different email addresses to sign up, either the cellphone/mobile number that you provide, or credit card information used for verification, could eventually be used to link all your trial accounts. I wasn\'t aware of this limitation, but while it\'s coming around to bite me, I do not regret attempting to document a complete 30-day Oracle Trial experience.\\n\\nThis post completes my story and I hope it has brought some value to you. If you were sitting on the fence or adverse to the cloud, I strongly recommend taking the 30-day trial and have any doubts answered. Regardless on what you eventually decide on, I am sure you have much to gain by learning and experiencing the platform of the future.\\n\\nAs an aside, I just wanted to mention that for the trials, I had been using an arbitrary string for the tenant name. It was fortunate that I was able to request a name change that suites the organization I was applying on behalf of. I had also used an unaffiliated email address for the Cloud Administrator/Owner account, and was able to quickly remedy this by creating a new account. Then with a single button click, copied over all the same roles that the original sign-up account had. What a relief!"},{"id":"id/1912","metadata":{"permalink":"/content/id/1912","source":"@site/posts/day-22-30-days-on-the-oracle-cloud/index.md","title":"Day 22 - 30 Days on the Oracle Cloud","description":"All sorts of functions","date":"2019-07-23T01:03:15.000Z","formattedDate":"July 23, 2019","tags":[{"label":"oracle","permalink":"/content/tags/oracle"},{"label":"cloud","permalink":"/content/tags/cloud"},{"label":"oci","permalink":"/content/tags/oci"},{"label":"IaaS","permalink":"/content/tags/iaa-s"},{"label":"infrastructure","permalink":"/content/tags/infrastructure"},{"label":"fnproj","permalink":"/content/tags/fnproj"},{"label":"serverless","permalink":"/content/tags/serverless"},{"label":"orclfn","permalink":"/content/tags/orclfn"},{"label":"functions","permalink":"/content/tags/functions"}],"readingTime":5.785,"hasTruncateMarker":true,"authors":[{"name":"Adrian Png","title":"Senior Cloud Solutions Architect @ Insum","url":"https://github.com/fuzziebrain","imageURL":"https://github.com/fuzziebrain.png","key":"fuzziebrain"}],"frontMatter":{"title":"Day 22 - 30 Days on the Oracle Cloud","tags":["oracle","cloud","oci","IaaS","infrastructure","fnproj","serverless","orclfn","functions"],"slug":"id/1912","authors":["fuzziebrain"],"categories":["Technology","Infrastructure","Oracle Cloud"],"date":"2019-07-23T01:03:15.000Z"},"prevItem":{"title":"Day 28 - 30 Days on the Oracle Cloud","permalink":"/content/id/1913"},"nextItem":{"title":"Day 17 - 30 days on the Oracle Cloud","permalink":"/content/id/1911"}},"content":"![All sorts of functions](./assets/mathematics-757566_1280x720.jpg)\\n\\nOracle Function was [announced](https://www.oracle.com/corporate/pressrelease/oracle-cloud-native-framework-121118.html) late last year (2018), with limited availability to select [Oracle Cloud](https://cloud.oracle.com) customers. The platform is built on [Fn](https://fnproject.io) and is Oracle\'s solution for developers wanting to deploy services using a [Serverless Framework](https://wikipedia.org/wiki/Serverless_Framework).\\n\\nAs succinctly explained by [Kaslin Fields](https://twitter.com/kaslinfields), a Fn function is a small embodiment of code that listens and speaks to HTTP streams. Ideally, each Fn function serves one, and only one purpose. It can be implemented in wide variety of programming languages and they are listed [here](https://github.com/fnproject/docs/blob/master/cli/ref/fn-init.md) (see the list of runtime options available).\\n\\n\x3c!-- truncate --\x3e\\n\\n## Development Environment Setup\\n\\nThe Fn project provides a concise [set of instructions](https://fnproject.io/tutorials/install/) on installing the Fn command line interface (CLI) utility. The CLI allows developers to run a Fn server locally, and perform developer and deployment client functions.\\n\\nA rather recent version of [Docker](https://www.docker.com/) is required, and if you are on a Microsoft Windows machine, I highly recommend installing [Virtual Box](https://www.virtualbox.org/) and running a Linux virtual machine (VM) for Fn development. You can create a VM manually, or you can install [Vagrant](https://www.vagrantup.com/) and quickly stand up a suitable using Oracle\'s official [Vagrant build scripts](https://github.com/oracle/vagrant-boxes) (see [Orace Linux 7 with Docker engine](https://github.com/oracle/vagrant-boxes/tree/master/DockerEngine)).\\n\\nIn a nutshell, assuming a Linux Operating System (OS), do the following:\\n\\n1. Install, configure and run Docker.\\n1. Install Fn CLI using the command provided:\\n    ```bash\\n    $ curl -LSs https://raw.githubusercontent.com/fnproject/cli/master/install | sh\\n    ```\\n1. Open a console and start the Fn Server for **local** development:\\n    ```bash\\n    $ fn start\\n    ```\\n\\n    This pulls a Docker image and runs a container. The default server API URL is `http://127.0.0.1:8080`.\\n1. Configure the VM to work with a local registry if you are planning to start out by developing serverless functions locally.\\n1. Follow the [quick tutorial](https://fnproject.io/tutorials/Apps/) on creating a simple \\"Helloworld\\" service, changing the runtime to use the programming language of your choice.\\n\\n## Oracle Cloud Setup\\n\\nOracle Functions is not 100% ready to go, right out of the box when your Oracle Cloud account is provisioned. The setup tasks and deployment instructions are already available in the [official documentation](https://docs.cloud.oracle.com/iaas/Content/Functions/Concepts/functionsoverview.htm). I shall attempt to summarise and highlight the important steps.\\n\\n### OCI Components Required\\n\\n1. A minimally functional Virtual Cloud Network (VCN)\\n1. Policies that allows Oracle Functions (FaaS) to:\\n    a. Access the Oracle Cloud Infrastructure Registry (OCIR); and\\n        ```\\n        Allow service FaaS to read repos in tenancy\\n        ```\\n    b. Use network components in the target compartment.\\n        ```\\n        Allow service FaaS to use virtual-network-family in compartment <COMPARTMENT_NAME>\\n        ```\\n\\n    > One or two policies may be created, however, the first policy statement must be created in a policy within the root compartment.\\n\\n### Configure Environment for Oracle Functions\\n\\nThere are **ten** steps to perform for [configuring](https://docs.cloud.oracle.com/iaas/Content/Functions/Tasks/functionsconfiguringclient.htm) the Fn development environment. The minimum you will need to invoke a simple \\"Helloworld\\" function are:\\n\\n1. [Setting up the OCI API Signing key](https://docs.cloud.oracle.com/iaas/Content/Functions/Tasks/functionssetupapikey.htm) .\\n    1. Run `openssl` command to generate the private and public keys. Note down the **paths** and **passphrase** used.\\n    1. Add the public key to the OCI user\'s API Keys and then noting the **fingerprint**.\\n1. [Create the OCI profile](https://docs.cloud.oracle.com/iaas/Content/Functions/Tasks/functionsconfigureocicli.htm) and configuration file (`~/.oci/config`). Make sure the values required are entered correctly, or API signing will fail and you will not be able to authenticate with the endpoint. Note the **profile** name. The end result is a profile like this:\\n    ```\\n    [my_oci-profile]\\n    user=ocid1.user.oc1..**********\\n    fingerprint=**:**:**:**:**:**:**:**:**:**:**:**:**:**:**:**\\n    key_file=~/.oci/my-private-key.pem\\n    tenancy=ocid1.tenancy.oc1..**********\\n    region=us-ashburn-1\\n    pass_phrase=mysupersecretpassword\\n    ```\\n1. Setup `oci-curl`, which isn\'t really required to do a quick invocation test.\\n1. If you already have a Fn development environment, then chances are, Docker and Fn CLI are already installed, configured and running.\\n1. Setup the [context](https://docs.cloud.oracle.com/iaas/Content/Functions/Tasks/functionscreatefncontext.htm) (and [this](https://docs.cloud.oracle.com/iaas/Content/Functions/Tasks/functionssetoracleprofile.htm)) for Fn CLI to work with Oracle Functions. After performing these steps, a context file will be created in `~/.fn/contexts`, containing contents similar to this:\\n    ```\\n    api-url: https://functions.us-ashburn-1.oraclecloud.com\\n    oracle.compartment-id: ocid1.compartment.oc1..**********\\n    oracle.profile: my-oci-profile\\n    provider: oracle\\n    registry: iad.ocir.io/<TENANT_NAMESPACE>/<REPOSITORY_NAME>\\n    ```\\n1. Generate and note the Auth token, and then login to the OCIR.\\n\\n## Deploying to Oracle Function\\n\\nThe best way to check that the environment is configured (`~/.oci/config`) correctly, is to list the applications available:\\n\\n```bash\\n$ fn list apps\\nNo apps found\\nNAME    ID\\n```\\n\\nTo create an application, either use the OCI console:\\n\\n![Oracle Functions - Create Application](./assets/create-fn-application.png)\\n\\nOr, use the Fn CLI, specifying the name of the application, and the OCID of the subnet to attach to:\\n\\n```bash\\n$ fn create app nodeapp --annotation oracle.com/oci/subnetIds=\'[\\"ocid1.subnet.oc1.iad.**********\\"]\'\\n```\\n\\nList the applications again to ensure it was successfully created:\\n\\n```bash\\n$ fn list apps\\nNAME    ID\\nnodeapp ocid1.fnapp.oc1.iad.**********\\n```\\n\\nFollowing the Fn tutorial, you would have created a directory containing the following artifacts required for a Fn function:\\n\\n```bash\\n[/home/myuser/projects/nodefn]$ ls\\nfunc.js  func.yaml  package.json\\n```\\n\\nSet your working directory here and then attempt to deploy the application:\\n\\n```bash\\n[/home/myuser/projects/nodefn]$ fn -v deploy --app nodeapp\\nDeploying nodefn to app: nodeapp\\nBumped to version 0.0.5\\nBuilding image iad.ocir.io/**********/fndemo/nodefn:0.0.5\\nFN_REGISTRY:  iad.ocir.io/**********/fndemo\\nCurrent Context:  fboci05\\nSending build context to Docker daemon   5.12kB\\nStep 1/9 : FROM fnproject/node:dev as build-stage\\n ---\x3e b557a05fec78\\nStep 2/9 : WORKDIR /function\\n ---\x3e Using cache\\n ---\x3e 80aba4e33a61\\nStep 3/9 : ADD package.json /function/\\n ---\x3e Using cache\\n ---\x3e ce7d7130e497\\nStep 4/9 : RUN npm install\\n ---\x3e Using cache\\n ---\x3e 622fc377f1a9\\nStep 5/9 : FROM fnproject/node\\n ---\x3e c8da69259495\\nStep 6/9 : WORKDIR /function\\n ---\x3e Using cache\\n ---\x3e e4ef6fe5e78e\\nStep 7/9 : ADD . /function/\\n ---\x3e 2d41667e41f6\\nStep 8/9 : COPY --from=build-stage /function/node_modules/ /function/node_modules/\\n ---\x3e 14abb279af71\\nStep 9/9 : ENTRYPOINT [\\"node\\", \\"func.js\\"]\\n ---\x3e Running in 60097a195252\\nRemoving intermediate container 60097a195252\\n ---\x3e 9a54e9d7ef1d\\nSuccessfully built 9a54e9d7ef1d\\nSuccessfully tagged iad.ocir.io/**********/fndemo/nodefn:0.0.5\\n\\nParts:  [iad.ocir.io ********** fndemo nodefn:0.0.5]\\nPushing iad.ocir.io/**********/fndemo/nodefn:0.0.5 to docker registry...The push refers to repository [iad.ocir.io/**********/fndemo/nodefn]\\na71bd36619f2: Pushed\\ne355820d7f01: Pushed\\n9682c276ebcb: Pushed\\na0d7b4199dce: Pushed\\n8aed3db29123: Pushed\\n9c85c117f8f6: Pushed\\na464c54f93a9: Pushed\\n0.0.5: digest: sha256:6dca91254ae9a404236e3ac5f486a9d0fb575b1589f987c146b9489b4900ffdf size: 1780\\nUpdating function nodefn using image iad.ocir.io/**********/fndemo/nodefn:0.0.5...\\nSuccessfully created function: nodefn with iad.ocir.io/**********/fndemo/nodefn:0.0.5\\n\\nFn: [GET /triggers][500] ListTriggers default  &{Fields: Message:Internal server error}\\n\\nSee \'fn <command> --help\' for more information. Client version: 0.5.84\\n```\\n\\nWith increase verbosity, it is clear that Docker is used to containerize the service. I have not delved into the nuts and bolts of the deployment model, but from the little I know, a Docker image is first created and deployed to the OCIR. When a function is invoked, the platform creates a container, executes the function and then terminates, releasing any allocated resources.\\n\\nTo invoke the function, simply execute the command or use the `oci-curl` if available:\\n\\n```bash\\n[/home/myuser/projects/nodefn]]$ fn invoke nodeapp nodefn\\n{\\"message\\":\\"Hello World\\"}\\n```\\n\\n## Summary\\n\\nThe tutorial\'s \\"Helloworld\\" example doesn\'t do much for now. However, as mentioned earlier in this article, Fn functions are wrapped up in a Docker container. By creating a custom `Dockerfile` for your Fn, the possibilities are endless.\\n\\nAlso in the tutorials, the authors provide a roadmap on how to create a custom Docker image that uses [ImageMagick](https://imagemagick.org/index.php) to perform image manipulations. Something that could be useful to us [Oracle Application Express](https://apex.oracle.com) developers when Oracle Multimedia is completely removed in future Oracle Database versions."},{"id":"id/1911","metadata":{"permalink":"/content/id/1911","source":"@site/posts/day-17-30-days-on-the-oracle-cloud/index.md","title":"Day 17 - 30 days on the Oracle Cloud","description":"Autonomous TT","date":"2019-07-18T01:03:47.000Z","formattedDate":"July 18, 2019","tags":[{"label":"oracle","permalink":"/content/tags/oracle"},{"label":"cloud","permalink":"/content/tags/cloud"},{"label":"oci","permalink":"/content/tags/oci"},{"label":"IaaS","permalink":"/content/tags/iaa-s"},{"label":"infrastructure","permalink":"/content/tags/infrastructure"},{"label":"autonomous","permalink":"/content/tags/autonomous"},{"label":"database","permalink":"/content/tags/database"}],"readingTime":3.21,"hasTruncateMarker":true,"authors":[{"name":"Adrian Png","title":"Senior Cloud Solutions Architect @ Insum","url":"https://github.com/fuzziebrain","imageURL":"https://github.com/fuzziebrain.png","key":"fuzziebrain"}],"frontMatter":{"title":"Day 17 - 30 days on the Oracle Cloud","tags":["oracle","cloud","oci","IaaS","infrastructure","autonomous","database"],"slug":"id/1911","authors":["fuzziebrain"],"categories":["Technology","Infrastructure","Oracle Cloud"],"date":"2019-07-18T01:03:47.000Z"},"prevItem":{"title":"Day 22 - 30 Days on the Oracle Cloud","permalink":"/content/id/1912"},"nextItem":{"title":"Relocating Compute Instances","permalink":"/content/id/1910"}},"content":"![Autonomous TT](./assets/autonomous_tt_1280x720.jpg)\\n\\nIn case you missed it (which is almost impossible), the [Oracle Autonomous Database (AutonomousDB)](https://www.oracle.com/database/autonomous-database.html) is Oracle\'s hottest offering on the [Oracle Cloud](https://cloud.oracle.com) and it now [comes fitted](https://blogs.oracle.com/apex/announcing-the-availability-of-oracle-apex-on-autonomous-database-cloud) with what is arguably the best feature of the Oracle Database: [Oracle Application Express (APEX)](https://apex.oracle.com). I have a few more days to go with this [trial](https://cloud.oracle.com/tryit), so why not take one out for a spin?\\n\\n\x3c!-- <div style=\\"height: 1.05em; border-bottom: 1px solid darkgrey;\\"><i id=\\"mini-car\\" class=\\"fas fa-car-side\\" style=\\"position: absolute;\\"></i></div>\\n<script src=\\"{% asset_path animate-mini-car.javascript %}\\" defer><\/script> --\x3e\\n\\n\x3c!-- truncate --\x3e\\n\\n## Create and Autonomous DB\\n\\nCreating an Autonomous DB is easier than saying \\"ABC\\"!\\n\\n1. Login to the Oracle Cloud console.\\n1. Click <i class=\\"fa-solid fa-bars\\"></i> <i class=\\"fa-solid fa-caret-right\\"></i> Autonomous Transaction Processing (or Autonomous Data Warehouse).\\n1. Click \\"Create Autonomous Database\\" and provide the following details:\\n    * Compartment to situate the ATP/ADW in.\\n    * A display name\\n    * A database name (this is important!)\\n    * Choose a workload type, so pick between ADW or ATP depending on how you\'d anticipate the database to be used.\\n    * A deployment type, serverless (shared) or dedicated. Note that at the time of writing, serverless or ATP-S runs the database version 18c and auto indexing is not available. You\'ll need ATP-D (dedicated) to enjoy the benefits of auto indexing.\\n    * Resources needed, CPU core count and storage size (the minimum 1 TB is really daunting).\\n    * Select [auto scaling](https://docs.cloud.oracle.com/iaas/Content/Database/Concepts/adboverview.htm#autoscaling) if required.\\n    * Enter the `ADMIN` user\'s password twice, and then\\n    * Select the license type. Note that for trials, select either doesn\'t really matter. I have addressed this issue [before](/content/id/1906/), don\'t be fooled by the extraordinary low prices to run an autonomous DB. Use the [cost estimator](https://cloud.oracle.com/cost-estimator) for more realistic numbers for budgeting.\\n![Create a serverless Autonomous Transaction Processing database](./assets/01-create_atp.png)\\n\\n## Connecting to the Instance\\n\\nOnce the autonomous DB is alive and breathing, return to the details page. Click <i class=\\"fa-solid fa-bars\\"></i> <i class=\\"fa-solid fa-caret-right\\"></i> Autonomous Transaction Processing (or Autonomous Data Warehouse) and the select the target autonomous DB. On the details page, click \\"DB Connection\\".\\n\\n![Database Connection Dialog Page](./assets/02-db_connection.png)\\n\\nHere, click \\"Download\\" to grab the client credentials and wallet. You will need these for connecting to the database using [SQL Developer](https://www.oracle.com/database/technologies/appdev/sql-developer.html) and [SQLcl](https://www.oracle.com/database/technologies/appdev/sqlcl.html). Also, take note of the connection strings as you will need them to connect to the DB with the required service level.\\n\\nWhen creating a connection profile for the autonomous DB, select the Connection Type \\"Cloud Wallet\\". Then select the downloaded ZIP file for the Configuration File. Select the appropriate Service (TNS name).\\n\\n![SQL Developer Connection Profile](./assets/03-sqldev_connection.jpg)\\n\\nIn SQLcl, select and load the Cloud Wallet using the command:\\n\\n```bash\\nSQL> set cloud config /path/to/my/wallet_file.zip\\n```\\n\\nOnce the cloud wallet is loaded, connect to the DB as you would with any Oracle Database, providing the appropriate TNS Name to connect to.\\n\\n![Connecting to the DB using SQLcl](./assets/04-sqlcl_connection.jpg)\\n\\n## What Makes It Cool?\\n\\nIMHO, here are few good reasons:\\n\\n1. There\'s some cool old school Artificial Intelligence/Machine Learning techniques applied. It ain\'t just a marketing buzzword. Some of the algorithms used to put \\"auto\\" in \\"autonomous\\" are highlighted in [Maria Colgan\'s](https://twitter.com/SQLMaria) Kscope19 [session](https://kscope19.odtug.com/e/in/eid=32&req=info&s=2823&all=1). If you are a full [ODTUG](https://www.odtug.com/) member, you will have access to her presentation slide deck and hopefully a recording as well.\\n1. Oracle Machine Learning Notebooks - if you have used [Jupyter](https://jupyter.org/) before, then you\'ll find the interface and purpose very familiar.\\n![Available Tasks](./assets/05-oml_notebook.png)\\n![An OML Notebook](./assets/06-oml_notebook.png)\\n1. APEX, enough said!\\n1. SQL Developer Web - while we are going to see this available to install on premise in the near future, it doesn\'t hurt to get an early preview of how powerful a development platform we have with SQLDEV Web and APEX combined.\\n![SQL Developer in a browser!](./assets/07-sqldev_web.png)\\n\\nNot convinced? Get a trial account and check it out!\\n\\n> **Photo Credits**\\n>\\n> Photo by [Steve Jurvetson](https://www.flickr.com/photos/jurvetson/)\\n> License: [CC BY 2.0](https://creativecommons.org/licenses/by/2.0/)"},{"id":"id/1910","metadata":{"permalink":"/content/id/1910","source":"@site/posts/relocating-compute-instances/index.md","title":"Relocating Compute Instances","description":"Moving a House, New Zealand Style","date":"2019-07-15T00:02:04.000Z","formattedDate":"July 15, 2019","tags":[{"label":"oracle","permalink":"/content/tags/oracle"},{"label":"cloud","permalink":"/content/tags/cloud"},{"label":"oci","permalink":"/content/tags/oci"},{"label":"IaaS","permalink":"/content/tags/iaa-s"},{"label":"infrastructure","permalink":"/content/tags/infrastructure"},{"label":"compute","permalink":"/content/tags/compute"},{"label":"migration","permalink":"/content/tags/migration"}],"readingTime":3.925,"hasTruncateMarker":true,"authors":[{"name":"Adrian Png","title":"Senior Cloud Solutions Architect @ Insum","url":"https://github.com/fuzziebrain","imageURL":"https://github.com/fuzziebrain.png","key":"fuzziebrain"}],"frontMatter":{"title":"Relocating Compute Instances","tags":["oracle","cloud","oci","IaaS","infrastructure","compute","migration"],"slug":"id/1910","authors":["fuzziebrain"],"categories":["Technology","Infrastructure","Oracle Cloud"],"date":"2019-07-15T00:02:04.000Z"},"prevItem":{"title":"Day 17 - 30 days on the Oracle Cloud","permalink":"/content/id/1911"},"nextItem":{"title":"Seattle Take Two","permalink":"/content/id/1909"}},"content":"![Moving a House, New Zealand Style](./assets/moving_houses_1280x720.jpg)\\n\\nTo everyone following my [30 Days on the Oracle Cloud](/content/id/1903/) experience, I apologise that I was not able to keep up with the series previously, due to work commitments. I have enrolled in yet another [trial](https://cloud.oracle.com/tryit), so I am hoping to continue where I left off and share my thirty day adventure with an [Oracle Cloud](https://cloud.oracle.com) trial. I\'m on Day 14 now, and for this post, I\'ll examine an important issue that many of us face, even in our personal life - moving houses!\\n\\n\x3c!-- truncate --\x3e\\n\\nSuppose I had a compute instance running [Oracle Application Express (APEX)](https://apex.oracle.com/) to perform some proof of concept tasks and now I am ready to move it from my Oracle Cloud tenancy to a client\'s. There are several recipes for achieving this goal. I could for example, use the Oracle Database\'s backup and recovery, or data migration tools, but is there a simpler approach? What if I could clone the server and then ship it over to be deployed on my client\'s tenancy?\\n\\nThis is definitely possible and I will walk you through a simple procedure to perform this migration exercise.\\n\\n## Create and Export a Custom Image\\n\\nFirst, on the source tenant, create a custom image of the compute instance and then prepare the image for export.\\n\\n1. Click <i class=\\"fa-solid fa-bars\\"></i> <i class=\\"fa-solid fa-caret-right\\"></i> Compute <i class=\\"fa-solid fa-caret-right\\"></i> Instances.\\n1. Place the mouse cursor over the <i class=\\"fa-solid fa-ellipsis-h\\"></i> and then click \\"Create Custom Image\\". **WARNING**: this will initiate a system shutdown on the target instance!\\n![Initiate custom image creation. IMPORTANT: Instance will shutdown!](./assets/01-create_custom_image.png)\\n1. Select the target compartment, enter a name and then click \\"Create Custom Image\\".\\n![Specify name and target compartment](./assets/02-create_custom_image.png)\\n1. While waiting for the process to be completed, if a new Object Storage Bucket needs to be created, click <i class=\\"fa-solid fa-bars\\"></i> <i class=\\"fa-solid fa-caret-right\\"></i> Object Storage <i class=\\"fa-solid fa-caret-right\\"></i> Object Storage.\\n![Create an Object Storage Bucket if necessary](./assets/03-create_object_storage_bucket.png)\\n1. Enter a Bucket Name, select Standard Storage Tier, use Oracle-managed keys and then click \\"Create Bucket\\".\\n1. Click <i class=\\"fa-solid fa-bars\\"></i> <i class=\\"fa-solid fa-caret-right\\"></i> Compute <i class=\\"fa-solid fa-caret-right\\"></i> Custom Images. The status should show green and \\"Available\\" when the custom image is ready. At this point, the compute instance would have powered back on automatically.\\n![Custom Image ready](./assets/04-create_custom_image.png)\\n1. Place the mouse cursor over the <i class=\\"fa-solid fa-ellipsis-h\\"></i>  next to the newly created custom image and then click \\"Export Custom Image\\".\\n1. Select Object Storage Bucket, the Bucket compartment and name, and then enter a new Object Name. Click \\"Export Image\\" when you are ready to proceed.\\n![Export Image to Object Storage Bucket](./assets/05-export_custom_image.png)\\n1. Exporting the image takes a while to complete. click <i class=\\"fa-solid fa-bars\\"></i> <i class=\\"fa-solid fa-caret-right\\"></i> Object Storage <i class=\\"fa-solid fa-caret-right\\"></i> Object Storage and then select the bucket that the image was written to. When it is completed, the object will appear in the Objects table.\\n1. Place the mouse cursor over the <i class=\\"fa-solid fa-ellipsis-h\\"></i>  next to the newly created object and click \\"Create Pre-Authenticated Request\\".\\n![Generate a Pre-Authenticated Request](./assets/06-generate_preauthenticated_request.png)\\n1. Enter the name of the request, select Object for the request target, Permit Read on the Object for Access Type and then specify an expiration date. Click \\"Create Pre-Authenticated Request\\" when done.\\n![Enter Pre-Authenticated Request details](./assets/07-generate_preauthenticated_request.png)\\n1. The next dialog presented contains the Pre-Authenticated Request URL. Copy the link in full. It will be required for the import process.\\n![Copy the Pre-Authenticated Request URL](./assets/08-generate_preauthenticated_request.png)\\n\\n## Import the Image and Create Compute Instance\\n\\nNext, on the target tenant, create a new compute instance using the imported custom image.\\n\\n1. Click <i class=\\"fa-solid fa-bars\\"></i> <i class=\\"fa-solid fa-caret-right\\"></i> Compute <i class=\\"fa-solid fa-caret-right\\"></i> Custom Images.\\n1. Click \\"Import Image\\" and enter the following information and then click \\"Import Image\\".\\n![Import the custom image](./assets/09-import_custom_image.png)\\n    * Compartment to create a new custom image in.\\n    * Name of the new custom image.\\n    * Operating System, in this case, Linux.\\n    * The Object Storage Pre-Authenticated Request URL obtained earlier.\\n    * Select OCI for Image Type.\\n1. Importing the image again takes a long time to finish. When it\'s done, the fastest path to create the instance is to click \\"Create Instance\\".\\n![Custom image import details showing progress](./assets/10-import_custom_image.png)\\n1. From here, creating the instance is the same as [previously described](/content/id/1904/).\\n![Create the compute instance from imported custom image](./assets/11-create_compute_from_custom_image.png)\\n\\n## Summary\\n\\nThis looks like a lengthy process, but it\'s really simple to move your compute instances from one tenant to another. This might come in handy, especially when you\'ve put in quite a bit of effort creating your compute instances during a trial. I\'m just saying! <i class=\\"fa-regular fa-smile-wink\\"></i>.\\n\\n> <u>Photo Credits</u>\\n>\\n> Photo by [theyounz](https://www.flickr.com/photos/19953402@N04/)\\n> License: [CC BY-ND 2.0](https://creativecommons.org/licenses/by-nd/2.0/)"},{"id":"id/1909","metadata":{"permalink":"/content/id/1909","source":"@site/posts/seattle-take-two/index.md","title":"Seattle Take Two","description":"The Oracle Downtown Seattle Office Building","date":"2019-07-04T00:35:12.000Z","formattedDate":"July 4, 2019","tags":[{"label":"travel","permalink":"/content/tags/travel"},{"label":"orclapex","permalink":"/content/tags/orclapex"},{"label":"odtug","permalink":"/content/tags/odtug"},{"label":"seattle","permalink":"/content/tags/seattle"},{"label":"learning","permalink":"/content/tags/learning"},{"label":"training","permalink":"/content/tags/training"},{"label":"community","permalink":"/content/tags/community"}],"readingTime":7.67,"hasTruncateMarker":true,"authors":[{"name":"Adrian Png","title":"Senior Cloud Solutions Architect @ Insum","url":"https://github.com/fuzziebrain","imageURL":"https://github.com/fuzziebrain.png","key":"fuzziebrain"}],"frontMatter":{"title":"Seattle Take Two","tags":["travel","orclapex","odtug","seattle","learning","training","community"],"categories":["Technology","Conferences"],"slug":"id/1909","authors":["fuzziebrain"],"date":"2019-07-04T00:35:12.000Z"},"prevItem":{"title":"Relocating Compute Instances","permalink":"/content/id/1910"},"nextItem":{"title":"Social Sign In with Azure","permalink":"/content/id/1908"}},"content":"![The Oracle Downtown Seattle Office Building](./assets/oracle_seattle.jpg)\\n\\nI was glad to be back in Seattle to attend the 2019 edition of ODTUG\'s famed Kscope conference. This is the second time the event is held this close to home (read my [previous recount](/content/id/391/) of the same conference five years ago)! And like every Kscope before, the conference always provides unique experiences, bountiful knowledge and new people to meet! Here\'s a quick summary and highlights of my week.\\n\\n\x3c!-- truncate --\x3e\\n\\n## Sessions\\n\\nI\'d generally group Kscope [Oracle Application Express (APEX)](https://apex.oracle.com/) and Database (DB) sessions into the following categories and that helps me decide what I should attend:\\n\\n#### 1. Insights to what\'s next - attend the Sunday Symposium sessions.\\n\\nMy takeaways? Here are the two new 19.2 features that I am looking forward to:\\n\\n1. Faceted search support for reports\\n1. Enhanced List of Values\\n\\nThere are more cool stuff to come, but these are my top picks as I can see them being useful in Research Data Management (RDM) applications.\\n\\nThen of course there were the big announcements, though it wasn\'t really much of a surprise as rumours about APEX coming to the Autonomous DB have been floating around for quite a while.\\n\\n![Announcement that APEX is now available with Autonomous DB](./assets/autonomous_apex.jpg)\\n\\nI enjoyed attending [Maria Colgan\'s](https://twitter.com/SQLMaria) session where she described the \\"autonomous\\" in Autonomous DB. It was exciting to learn that \\"Machine Learning\\" wasn\'t just a buzzword and it isn\'t driven by \\"Deep Learning\\". Instead, there are very down-to-earth knowledge engineering (KE) techniques that were employed to enhance, an already robust relational database system, with self-healing and auto-indexing.\\n\\n![A slide describing two KE techniques that drive the auto-indexing feature in the Autonomous DB.](./assets/autonomousdb_auto_indexing.jpg)\\n\\n#### 2. Support core expertise - attend APEX and DB sessions that directly impact my day job.\\n\\nThe session in this category that stood out for me was [Jeff Smith\'s](https://twitter.com/thatjeffsmith) on some neat upcoming features of [SQLcl](https://www.oracle.com/technetwork/developer-tools/sqlcl/) and support for [Liquibase](http://www.liquibase.org/). I\'m certainly looking forward to the 19.2 release!\\n\\n![SQLcl help with new commands highlighted.](./assets/sqlcl_new_stuff.jpg)\\n\\n#### 3. Broaden skill set - attend DB sessions on topics that are sometimes out of my current job scope.\\n\\n[Dimitri Gielis\'](https://twitter.com/dgielis) presentations are always revolutionary and his session on [GraphQL](https://graphql.org/) was no exception. I have been hearing a lot about this new approach to building for exposing data as APIs during Node+JS Interactive events in the past, but never delved deeper. Now I\'m inspired!\\n\\n### Emerging Technology\\n\\nThis year, I was honoured to be on the team that helped select a list of eleven sessions that covered topics that were considered \\"emerging\\":\\n\\n* Blockchain\\n* Internet of Things (IoT)\\n* Machine Learning\\n* Robotic Process Automation (RPA); and\\n* Serverless Computing\\n\\nFour of them are topics that I am very passionate about and would have liked to attend all of them, but with so many tracks and concurrent activities going on, it was really hard to sit in on all of them. Thank goodness for the recordings that are to come!\\n\\nMy fingers and toes are crossed. I hope that we will continue to find space to learn about things that are out of our core expertise. To the conference committee, yes, I\'ll volunteer again if called upon to perform this duty.\\n\\n### General Session - The Magician\\n\\nIt\'s been a while since I last attended a magic show and so this year\'s General Session was a treat. [Jason Latimer](https://www.jasonlatimer.com/) is a great showman and I was pretty much floored by a few of his tricks. I tried repeating the half-filled, inverted glass trick at home, but only succeeded in drenching the countertop.\\n\\nThe more important message however, echoes one of the conversations we had during the Unconference session. We have to keep learning, testing boundaries and challenging the status quo. Never settle for enough or risk becoming obsolete.\\n\\n## Achievements\\n\\nIn 2018, I was honoured to join the [Oracle ACE Program](https://www.oracle.com/technetwork/community/oracle-ace/) as an Oracle ACE Associate. This year, I am again privileged to have been promoted to an Oracle ACE. Typically, these awards are announced at a private dinner event, so it was quite a surprise that they were made public during the prelude to General Session.\\n\\n![Oracle ACE Program Announcements](./assets/oracle_ace_program_announcements.jpg)\\n\\nAgain, my heartiest congratulations to everyone whose names were on that slide!\\n\\nNext, certifications. It was not easy feat, but thanks to ODTUG and the conference committee, IMHO, the [Oracle Test Fest](https://kscope19.odtug.com/page/oracle-test-fest) was a success. I knew of quite a few APEX developers who were certified during the three days of testing. For me, thankfully, I achieved my certification goals for 2019 by attaining certifications for both APEX 18 and the [Oracle Cloud](https://cloud.oracle.com). Phew!\\n\\n\x3c!-- <div style=\\"text-align: center\\">\\n  <div data-iframe-width=\\"150\\" data-iframe-height=\\"270\\" data-share-badge-id=\\"567bf42a-9442-4f9e-b17d-cb6033609b4f\\"></div><script type=\\"text/javascript\\" async src=\\"//cdn.youracclaim.com/assets/utilities/embed.js\\"><\/script>\\n  <div data-iframe-width=\\"150\\" data-iframe-height=\\"270\\" data-share-badge-id=\\"39ed6648-9522-4eeb-a1d9-8d18cf77bc16\\"></div><script type=\\"text/javascript\\" async src=\\"//cdn.youracclaim.com/assets/utilities/embed.js\\"><\/script>\\n</div> --\x3e\\n\\nThis is probably my most bountiful Kscope ever!\\n\\n## Challenge Coins for Good\\n\\n![Side-by-side: the APEX and OracleIaaS Challenge Coin](./assets/challenge_coin_buddies.jpg)\\n\\nICYMI, I made a bunch of APEX Challenge Coins [a few months ago](/content/id/1727) and since then, more than half of them have been sponsored and distributed worldwide. To promote the development of the Oracle IaaS (Infrastructure as a Service) community, [Luc Demanche](https://twitter.com/lucdemanche) and I collaborated on an OCI Administrator Challenge Coin and I am happy to share that a few of them have already gone out to deserving people. The rules are pretty much the same:\\n\\n1. Actively manage an Oracle Cloud Infrastructure (OCI) tenant and/or architecture solutions using OCI features.\\n1. Have performed exceedingly well in your professional career and/or contributed significantly to the community or an individual.\\n1. Know someone who thinks you have met both requirements (1) and (2).\\n\\n![Supporting both the Pike Market Food Bank and celebrating outstanding individuals.](./assets/apex_challenge_coin.jpg)\\n\\nKscope events are also great opportunities to give back to the larger community. This is evident by the annual community service events. There are many ways to give back, and this year, I found the opportunity for some us to contribute to the [Pike Market Food Bank](http://www.pmfb.org/) through the sponsorship of coins, awarded to individuals attending the conference. I am thankful for the support received from the following sponsors:\\n\\n* [Joel Kallman](https://twitter.com/joelkallman)\\n* [Stefan Dobre](https://twitter.com/stefan__dobre)\\n* [Niels de Bruijn](https://twitter.com/nielsdb)\\n* [Kai Donato](https://twitter.com/_KaiDonato)\\n* [Alfredo Abate](https://twitter.com/HeyAlfredoDBA)\\n* [Michel St-Amour](https://twitter.com/StAmourMichel); and\\n* [Shakeeb Rahman](https://twitter.com/shakeeb)\\n\\nTogether, we managed to collect USD 334.29 that I\'m sure the food bank would put to good use and feed the hungry.\\n\\nI\'d also like to take the opportunity to thank the many others who have sponsored coins since both programmes started. I believe these coins mean a lot to those who have received one, and knowing that they do, touches my heart too. So thank you!\\n\\n## Special Moments\\n\\nI had quite a bit of fun \\"competing\\" with [Gerald Venzl](https://twitter.com/GeraldVenzl) over who has the coolest conference badge. That lead me to fire up my 3D printer and acquire some really cool electronics and come up with this:\\n\\n\x3c!-- {% twitter https://twitter.com/fuzziebrain/status/1141135537308225537 %} --\x3e\\n\\nIt didn\'t bag me any prizes, but what I got was even more valuable, a new friend! I am very honoured to have met master tinkerer [Todd Sharp](https://twitter.com/recursivecodes). OK, and yes, I received a nice gift too, an Oracle [Code Card](https://developer.oracle.com/codecard/)!\\n\\n![An Oracle Code Card](./assets/oracle_code_card.jpg)\\n\\nYou can expect me to write more about the [Espressif](https://www.espressif.com) chips, [E Ink](https://www.eink.com/) and how they relate to APEX in future posts!\\n\\nThe other special Kscope moment was meeting [Maxime Tremblay](https://twitter.com/mtremblaydev) while waiting for my 24-minute flight from Vancouver to Seattle. ICYMI, Max recently started [askMax](https://askmax.solutions/). If you have a need for low-cost, on-demand APEX support, consider signing up with Max.\\n\\n![askMax on the jet!](./assets/askmax_sticker.jpg)\\n\\nWe chatted quite a bit on the plane, during which I found out (and he showed me) that the APEX Challenge Coin [Juergen Schuster](https://twitter.com/JuergenSchuster) sponsored him earlier this year, was in his wallet! Surprising to me, he wasn\'t the only one. I found out later that [Christoph Ruepprich](https://twitter.com/CRuepprich) travelled with his pair of coins as well! The pride and joy in their faces says a lot to me. Goals met!\\n\\n## Thank you!\\n\\nBefore I end this post, I wanted to thank a few individuals who have been instrumental in all that I have achieved this year:\\n\\n* [Opal Alapat](https://twitter.com/opal_EPM) - it\'s a bittersweet feeling to [know](https://womaninepm.com/2019/07/02/odtug-kscope19-a-fond-farewell/) that this is your final Kscope in a leadership role. I am one of those individuals that have benefited from your strong mentorship and canvassing to enroll me into the ACE program. I don\'t think I could ever repay you for your kindness, so the least I can do is to pay it forward.\\n* [Monty Latiolais](https://twitter.com/monty_latiolais) - we met in Seattle five years ago. Enough said.\\n* [Martin D\'Souza](https://twitter.com/martindsouza) - you got me to Seattle five years ago and then took a flight home the next day. This year, you didn\'t show up at all. Something about city that you don\'t like?\\n* My bosses at [Insum](https://insum.ca/) - thank you for supporting my constant desire to be at Kscope conferences.\\n\\nAnd for everyone who came to my Who\'s Who in APEX session, thank you! I didn\'t get a chance to have one-to-one conversations with everyone, but I would have loved to.\\n\\nThank you, and I hope to see everyone in [Boston 2020](https://kscope20.odtug.com/)!"},{"id":"id/1908","metadata":{"permalink":"/content/id/1908","source":"@site/posts/social-sign-in-with-azure/index.md","title":"Social Sign In with Azure","description":"Photo by Gerd Altmann","date":"2019-04-23T19:01:19.000Z","formattedDate":"April 23, 2019","tags":[{"label":"orclapex","permalink":"/content/tags/orclapex"},{"label":"oauth2","permalink":"/content/tags/oauth-2"},{"label":"authentication","permalink":"/content/tags/authentication"},{"label":"microsoft","permalink":"/content/tags/microsoft"},{"label":"azure","permalink":"/content/tags/azure"},{"label":"active directory","permalink":"/content/tags/active-directory"},{"label":"office 365","permalink":"/content/tags/office-365"},{"label":"sso","permalink":"/content/tags/sso"},{"label":"single sign on","permalink":"/content/tags/single-sign-on"}],"readingTime":5.69,"hasTruncateMarker":true,"authors":[{"name":"Adrian Png","title":"Senior Cloud Solutions Architect @ Insum","url":"https://github.com/fuzziebrain","imageURL":"https://github.com/fuzziebrain.png","key":"fuzziebrain"}],"frontMatter":{"title":"Social Sign In with Azure","tags":["orclapex","oauth2","authentication","microsoft","azure","active directory","office 365","sso","single sign on"],"categories":["Technology","Programming","Oracle Application Express"],"slug":"id/1908","authors":["fuzziebrain"],"date":"2019-04-23T19:01:19.000Z"},"prevItem":{"title":"Seattle Take Two","permalink":"/content/id/1909"},"nextItem":{"title":"Day 15 - 30 Days on the Oracle Cloud","permalink":"/content/id/1907"}},"content":"![Photo by Gerd Altmann](./assets/board-895399_1920x1280.jpg)\\n\\nSince [Oracle Application Express (APEX)](https://apex.oracle.com) version 18.1, application developers are able to easily integrate [OAuth2](https://oauth.net/2/)-based authentication using many popular identity providers including Google, Facebook and Microsoft. [Morten Braten](https://twitter.com/mortenbraten), [Anton Nielsen](https://twitter.com/c2anton) and I have previously written on this subject.\\n\\n* [APEX Authentication with Microsoft account](https://ora-00001.blogspot.com/2018/02/apex-authentication-with-microsoft-account.html)\\n* [Oracle APEX Social Sign-On with Microsoft Azure](http://c2anton.blogspot.com/2019/01/oracle-apex-social-sign-on-with.html)\\n* [Oracle APEX Less Social Sign-On with MS Azure and Office 365](http://c2anton.blogspot.com/2019/02/oracle-apex-less-social-sign-on-with-ms.html)\\n* [APEX Social Sign-In Without a Wallet](/content/id/1711/)\\n\\nStarting May 2019, Microsoft is changing how developers manage their OAuth2 application registrations.\\n\\n\x3c!-- truncate --\x3e\\n\\nCurrently, developers manage these entries using Microsoft\'s Application Registration Portal (https://apps.dev.microsoft.com/). This is being phased out. Going forward, application registrations will be managed via the Azure portal (https://portal.azure.com). While currently in preview, it is functional and remains rather straightforward to manage.\\n\\n![Microsoft Application Registration Portal](./assets/apps_dev_microsoft.png)\\n\\nBefore we begin, make sure that the database is configured with the appropriate network privileges and the APEX instance is configured to use an Oracle Wallet containing all the necessary SSL/TLS certificates. Please read this previous [article](/content/id/1725) if you need some guidance.\\n\\n1. Sign in to [Azure](https://portal.azure.com)\\n1. Click *Azure Active Directory* {% fa chevron-right %} *App registrations (Preview)*\\n1. Click *New registration*\\n1. Enter a name for the app registration\\n1. For supported types, choose between the <u>three</u> options:\\n    * Accounts in this organizational directory only (*<COMPANY_NAME>*)\\\\*\\n    * Accounts in any organizational directory\\n    * Accounts in any organizational directory and personal Microsoft accounts (e.g. Skype, Xbox, Outlook.com)\\n\\n    The list above is ordered in increasing accessibility but decreasing security. Microsoft provides the following descriptions for each option:\\n\\n    > **Accounts in this organizational directory only (*<COMPANY_NAME>*)**\\n    >\\n    > All user and guest accounts in your directory can use your application or API.\\n    >\\n    > Use this option if your target audience is internal to your organization.\\n    >\\n    > **Accounts in any organizational directory**\\n    >\\n    > All users and guests with a work or school account from Microsoft can use your application or API. This includes schools and businesses that use Office 365.\\n    >\\n    > Use this option if your target audience is business or educational customers.\\n    >\\n    > **Accounts in any organizational directory and personal Microsoft accounts (e.g. Skype, Xbox, Outlook.com)**\\n    >\\n    > All users with a work or school, or personal Microsoft account can use your application or API. It includes schools and businesses that use Office 365 as well as personal accounts that are used to sign in to services like Xbox and Skype.\\n    >\\n    > Use this option to target the widest set of Microsoft identities.\\n    >\\n\\n    \\\\* *The first option is only available to Azure AD subscribers. Business accounts should have access to this feature.*\\n1. Enter the APEX callback URL as a *Web* Redirect URI, e.g. https://apex.oracle.com/pls/apex/apex_authentication.callback\\n1. Read the policy and click *Register* to proceed.\\n![New App Registration](./assets/app_registration_register.png)\\n1. Once the app registration is completed, note the *Application (client) ID* on the Overview page. For Azure Active Directory (AD) subscribers (e.g. Azure AD for Office 365), also note the *Directory ID*.\\n![Application Registration Overview Page](./assets/app_registration_overview.png)\\n1. Click *Certificates & secrets* and then the *New client secret* button under the section *Client secrets*.\\n1. Provide a description and expiration period. Click *Add* and copy the secret value as it will be displayed only once.\\n![Adding a client secret](./assets/app_registration_client_secret.png)\\n1. The required API permissions should have been already added, if not, ensure that the app has at least *User.Read* on the *Microsoft Graph* API.\\n1. In APEX App Builder, create a *Web Credential* using the values for the client ID and secret pair generated earlier in the Azure portal.\\n![Azure Web Credential](./assets/app_builder_web_credential.png)\\n1. Create an *Authentication Scheme* with the following values:\\n\\n| Parameter | Value |\\n| - | - |\\n| Credential Store | *Select the credential store containing the client ID and secret from Azure* |\\n| Authentication Provider | Generic OAuth2 Provider |\\n| Authorization Endpoint URL | https://login.microsoftonline.com/common/oauth2/v2.0/authorize |\\n| Token Endpoint URL(Value Required) | https://login.microsoftonline.com/common/oauth2/v2.0/token |\\n| User Info Endpoint URL | https://graph.microsoft.com/v1.0/me |\\n| Scope | User.Read |\\n| Authentication URI Parameters | *Blank* |\\n| Username Attribute | userPrincipalName |\\n| Convert Username To Upper Case | *Set \\"Yes\\" if necessary, otherwise leave it as \\"No\\"* |\\n| Additional User Attributes | *Leave blank unless required* |\\n\\n14. For the *Authorization* and *Token* endpoint URLs, replace `common` with the *Directory ID*.\\n    * *Authorization Endpoint URL* - `https://login.microsoftonline.com/<DIRECTORY_ID>/oauth2/v2.0/authorize`\\n    * *Token Endpoint URL* - `https://login.microsoftonline.com/<DIRECTORY_ID>/oauth2/v2.0/token`\\n\\n![Social Sign In Authentication Scheme for Azure AD subscribers](./assets/app_builder_authentication_scheme.png)\\n1. In the Authentication Scheme\'s *Post-Logout URL*, set the value to https://login.microsoftonline.com/common/oauth2/logout, **if** Single Sign Out is required.\\n\\n### OpenID Connect\\n\\n> Updated on 2019-04-24\\n\\nWith much thanks to Jean-Marc (see comments below), please note that with the Microsoft identity platform, developers can also use the [OpenID Connect](https://openid.net/connect/) to create the Authentication scheme. Unlike the older OpenID v1.0 and v2.0 specifications that are now obsolete, the OpenID Connect specification works on top of the OAuth2 protocol. It provides a JSON-based metadata document that provides the necessary information for the client to execute the OAuth2 authentication workflow.\\n\\nFrom `https://login.microsoftonline.com/common/v2.0/.well-known/openid-configuration`:\\n\\n```json\\n{\\n    \\"authorization_endpoint\\": \\"https://login.microsoftonline.com/common/oauth2/v2.0/authorize\\",\\n    \\"token_endpoint\\": \\"https://login.microsoftonline.com/common/oauth2/v2.0/token\\",\\n    \\"token_endpoint_auth_methods_supported\\": [\\n        \\"client_secret_post\\",\\n        \\"private_key_jwt\\",\\n        \\"client_secret_basic\\"\\n],\\n    \\"jwks_uri\\": \\"https://login.microsoftonline.com/common/discovery/v2.0/keys\\",\\n    \\"response_modes_supported\\": [\\n        \\"query\\",\\n        \\"fragment\\",\\n        \\"form_post\\"\\n],\\n    \\"subject_types_supported\\": [\\n        \\"pairwise\\"\\n],\\n    \\"id_token_signing_alg_values_supported\\": [\\n        \\"RS256\\"\\n],\\n    \\"http_logout_supported\\": true,\\n    \\"frontchannel_logout_supported\\": true,\\n    \\"end_session_endpoint\\": \\"https://login.microsoftonline.com/common/oauth2/v2.0/logout\\",\\n    \\"response_types_supported\\": [\\n        \\"code\\",\\n        \\"id_token\\",\\n        \\"code id_token\\",\\n        \\"id_token token\\"\\n],\\n    \\"scopes_supported\\": [\\n        \\"openid\\",\\n        \\"profile\\",\\n        \\"email\\",\\n        \\"offline_access\\"\\n],\\n    \\"issuer\\": \\"https://login.microsoftonline.com/{tenantid}/v2.0\\",\\n    \\"claims_supported\\": [\\n        \\"sub\\",\\n        \\"iss\\",\\n        \\"cloud_instance_name\\",\\n        \\"cloud_instance_host_name\\",\\n        \\"cloud_graph_host_name\\",\\n        \\"msgraph_host\\",\\n        \\"aud\\",\\n        \\"exp\\",\\n        \\"iat\\",\\n        \\"auth_time\\",\\n        \\"acr\\",\\n        \\"nonce\\",\\n        \\"preferred_username\\",\\n        \\"name\\",\\n        \\"tid\\",\\n        \\"ver\\",\\n        \\"at_hash\\",\\n        \\"c_hash\\",\\n        \\"email\\"\\n],\\n    \\"request_uri_parameter_supported\\": false,\\n    \\"userinfo_endpoint\\": \\"https://graph.microsoft.com/oidc/userinfo\\",\\n    \\"tenant_region_scope\\": null,\\n    \\"cloud_instance_name\\": \\"microsoftonline.com\\",\\n    \\"cloud_graph_host_name\\": \\"graph.windows.net\\",\\n    \\"msgraph_host\\": \\"graph.microsoft.com\\",\\n    \\"rbac_url\\": \\"https://pas.windows.net\\"\\n}\\n```\\n\\nAlong with the necessary endpoint URLs for OAuth2 authentication, the document also indicates the scope and claims supported, which are useful for setting up your APEX Authentication Scheme.\\n\\n#### APEX Authentication Scheme using OpenID Connect\\n\\n![APEX Authentication Scheme using OpenID Connect provider](./assets/app_builder_authentication_scheme_openid.png)\\n\\n| Parameter | Value |\\n| - | - |\\n| Credential Store | *Select the credential store containing the client ID and secret from Azure* |\\n| Authentication Provider | OpenID Connect Provider |\\n| Discovery URL | `https://login.microsoftonline.com/{tenant}/v2.0/.well-known/openid-configuration` \\\\* |\\n| Scope | profile,email |\\n| Authentication URI Parameters | *Blank* |\\n| Username Attribute | email |\\n| Convert Username To Upper Case | *Set \\"Yes\\" if necessary, otherwise leave it as \\"No\\"* |\\n| Additional User Attributes | *Leave blank unless required* |\\n\\n> \\\\* Where `{tenant}` can be either of the following:\\n>\\n> * `common`\\n> * `organizations`\\n> * `consumers`\\n> * Azure AD tenant\'s domain name or GUID identifier (Directory ID)\\n\\nThat\'s it!\\n\\nAs you can see, there is very little work involved to set this up, yet the benefits are far reaching. Once implemented, be sure to get your users\' feedback, and find out how much productivity has increased for them!"},{"id":"id/1907","metadata":{"permalink":"/content/id/1907","source":"@site/posts/day-15-30-days-on-the-oracle-cloud/index.md","title":"Day 15 - 30 Days on the Oracle Cloud","description":"Photo by Genty","date":"2019-03-13T21:00:00.000Z","formattedDate":"March 13, 2019","tags":[{"label":"oracle","permalink":"/content/tags/oracle"},{"label":"cloud","permalink":"/content/tags/cloud"},{"label":"oci","permalink":"/content/tags/oci"},{"label":"IaaS","permalink":"/content/tags/iaa-s"},{"label":"infrastructure","permalink":"/content/tags/infrastructure"},{"label":"devops","permalink":"/content/tags/devops"},{"label":"virtualbox","permalink":"/content/tags/virtualbox"}],"readingTime":4.26,"hasTruncateMarker":true,"authors":[{"name":"Adrian Png","title":"Senior Cloud Solutions Architect @ Insum","url":"https://github.com/fuzziebrain","imageURL":"https://github.com/fuzziebrain.png","key":"fuzziebrain"}],"frontMatter":{"title":"Day 15 - 30 Days on the Oracle Cloud","tags":["oracle","cloud","oci","IaaS","infrastructure","devops","virtualbox"],"slug":"id/1907","authors":["fuzziebrain"],"categories":["Technology","Infrastructure","Oracle Cloud"],"date":"2019-03-13T21:00:00.000Z"},"prevItem":{"title":"Social Sign In with Azure","permalink":"/content/id/1908"},"nextItem":{"title":"Day 11 - 30 Days on the Oracle Cloud","permalink":"/content/id/1906"}},"content":"![Photo by Genty](./assets/wormhole-2514312_1280x720.jpg)\\n\\nOne of my fondest memories of my previous job was working with [VMware](https://www.vmware.com) virtualization products. With VMware Workstation, system administrators had the ability to \\"move\\" a virtual machine (VM) from a local workstation to vSphere. It was no surprise then that I was very exited (and itching to play) when [VirtualBox](https://www.virtualbox.org) 6.0 was released with a new feature: support for exporting a VM to [Oracle Cloud Infrastructure (OCI)](https://cloud.oracle.com/iaas/)!\\n\\n\x3c!-- truncate --\x3e\\n\\n## In a Nutshell\\n\\nHere are major tasks that we need to accomplish:\\n\\n1. [Prepare OCI to receive our VM](#Preparing-OCI-Compartments-and-Networks)\\n1. [Create an OCI user and generate an API key](#Create-OCI-User-and-Generate-API-Key)\\n1. [Create a Bucket in the Object Store](#Create-a-Bucket-on-the-Object-Store)\\n1. [Configure VirtualBox 6, add a Cloud Profile](#Add-a-Cloud-Profile)\\n1. [Deploy a VM to the OCI](#Deploy-a-VM-to-the-Cloud)\\n\\n> All tasks were performed in a Windows environment.\\n\\n## Preparing OCI - Compartments and Networks\\n\\nOn [Day 2 of our journey](/content/id/1904), we went through some of the steps needed to successfully deploy a Compute Instance (VM). Follow the instructions to create a compartment and the required network components, but __do not__ create the Compute Instance. One cool feature I failed to mention in that article though, was the \\"easy option\\". I had gone through step-by-step, how to create the Virtual Cloud Network (VCN), subnets, an Internet Gateway and a Route Table, which is important should you need to customise the network layout. However, the OCI does offer us a simple alternative:\\n\\n![Quick VCN Setup](./assets/oci-vcn-simple.png)\\n\\nSelecting the option *CREATE VIRTUAL CLOUD NETWORK PLUS RELATED RESOURCES* will automatically create the VCN, an Internet Gateway, a Route Table and three public subnets, with one in every Availability Domain.\\n\\n## Create OCI User and Generate API Key\\n\\n> An [Oracle Identity Cloud Service (IDCS)](https://www.oracle.com/cloud/paas/identity-cloud-service.html) account is provisioned when a cloud account is created. Adding API keys to IDCS users follows a different path and will not be discussed here. We will instead, create an OCI user for deploying VMs to the cloud.\\n\\n1. Click {% fa bars %} {% fa caret-right %} Identity {% fa caret-right %} Users\\n1. Click *Create User* and enter:\\n    * **NAME**: *Desired username*\\n    * **DESCRIPTION**: *User\'s full name*\\n    * **Email**: *Email address for recovery*\\n![&nbsp;](./assets/oci-create-user.png)\\n1. Click on the username of the new user created.\\n1. Optionally, click on *Create/Reset Password* to generate a single-use password.\\n\\nNext, setup your workstation to work with OCI APIs.\\n\\nPlease follow the instructions [here](https://docs.cloud.oracle.com/iaas/Content/API/Concepts/apisigningkey.htm) to obtain the necessary keys and OCIDs. The artifact and values needed are documented on this [page](https://docs.cloud.oracle.com/iaas/Content/API/Concepts/sdkconfig.htm). To summarise, here\'s what\'s needed:\\n\\n1. On Windows, please install [Git Bash](https://git-scm.com/download/win). On Linux, most of the tools and commands are already available in the operating system.\\n1. Create the directory `~/.oci` that stores the private key, and for the various SDKs available, a configuration file.\\n1. Generate a private key and its corresponding public key in PEM format.\\n1. Grab the contents of the public key and add this to the OCI user account\'s collection of API Keys.\\n![&nbsp;](./assets/oci-add-pub-key.png)\\n1. Copy the fingerprint from OCI console.\\n![&nbsp;](./assets/oci-api-key-fingerprint.png)\\n1. Obtain the OCID values from the OCI console.\\n1. Decide which region to deploy the server.\\n\\n## Create a Bucket on the Object Store\\n\\n1. Click {% fa bars %} {% fa caret-right %} Object Storage {% fa caret-right %} Object Storage\\n1. Click the *Create Bucket* button.\\n1. Enter a *Bucket Name* and choose *Standard* for the *Storage Tier*. Click the *Create Bucket* button when done.\\n![](./assets/oci-create-bucket.png)\\n\\n## Add a Cloud Profile\\n\\n1. Launch VirtualBox 6.\\n1. Click File {% fa caret-right %} Cloud Profile Manager\\n1. Click the *Add* button and enter a profile name.\\n1. Enter the required OCIDs and values relating to the API key. Be sure to hit *Apply* when done!\\n![](./assets/vbox-cloud-profile.jpg)\\n\\n## Deploy a VM to the Cloud\\n\\n1. Click File {% fa caret-right %} Export Appliance\\n1. Select the VM to export and then click *Next*.\\n1. For *Format*, select *Oracle Cloud Infrastructure* and then select the *Account* that you wish to deploy the VM to.\\n![&nbsp;](./assets/vbox-export-appliance-oci.jpg)\\n1. Click *Next*. Using the private key and other information provided in the selected *Cloud Profile*, VirtualBox will call the OCI APIs to retrieve information and available resource selections for the target Compute Instance. You may change the name of the new Compute Instance, select a different instance Shape or specify a disk size etc.\\n![&nbsp;](./assets/vbox-compute-instance-configuration.jpg)\\n\\nWhen ready, click the *Export* button and VirtualBox will first export the VM as an image and then upload it to the target Object Store Bucket. Once the upload process is completed - and it can take a very long time - the OCI will create the instance from the image and voila! Like magic!\\n\\n![In the Final Stages!](./assets/oci-compute-instance-provisioning.png)\\n\\n## What Next?\\n\\nFor this simple proof-of-concept, I had created a base Oracle Enterprise Linux 7 using [Vagrant](https://www.vagrantup.com/) without putting anything much on it except the necessary user accounts so that I can access the Compute Instance later. This approach can be used, for example, to install and configure a full [Oracle Application Express (APEX)](https://apex.oracle.com) stack in VirtualBox, using either [OXAR](https://github.com/OraOpenSource/OXAR) or more recently, [helper scripts](/content/id/1902) for [Docker](https://www.docker.com)."},{"id":"id/1906","metadata":{"permalink":"/content/id/1906","source":"@site/posts/day-11-30-days-on-the-oracle-cloud/index.md","title":"Day 11 - 30 Days on the Oracle Cloud","description":"Photo by Steve Buissinne","date":"2019-03-09T12:14:38.000Z","formattedDate":"March 9, 2019","tags":[{"label":"oracle","permalink":"/content/tags/oracle"},{"label":"cloud","permalink":"/content/tags/cloud"},{"label":"oci","permalink":"/content/tags/oci"},{"label":"IaaS","permalink":"/content/tags/iaa-s"},{"label":"infrastructure","permalink":"/content/tags/infrastructure"},{"label":"devops","permalink":"/content/tags/devops"}],"readingTime":3.235,"hasTruncateMarker":true,"authors":[{"name":"Adrian Png","title":"Senior Cloud Solutions Architect @ Insum","url":"https://github.com/fuzziebrain","imageURL":"https://github.com/fuzziebrain.png","key":"fuzziebrain"}],"frontMatter":{"title":"Day 11 - 30 Days on the Oracle Cloud","tags":["oracle","cloud","oci","IaaS","infrastructure","devops"],"slug":"id/1906","authors":["fuzziebrain"],"categories":["Technology","Infrastructure","Oracle Cloud"],"date":"2019-03-09T12:14:38.000Z"},"prevItem":{"title":"Day 15 - 30 Days on the Oracle Cloud","permalink":"/content/id/1907"},"nextItem":{"title":"Day 4 - 30 Days on the Oracle Cloud","permalink":"/content/id/1905"}},"content":"![Photo by Steve Buissinne](./assets/insurance-539659_1280x720.jpg)\\n\\nIt took a little longer than I had hoped to publish the next article in this series about my experience with a 30-day Oracle Cloud trial. I had a created compute instance in the same [compartment](https://docs.cloud.oracle.com/iaas/Content/Identity/Tasks/managingcompartments.htm) and that messed up my estimates. That\'s just one piece of the puzzle.\\n\\n\x3c!-- truncate --\x3e\\n\\nThe other shocking observation was that hosting the database was terribly cheap and we\'re talking about Oracle pricing. <i class=\\"fa-regular fa-smile-wink\\"></i>\\n\\nThe [Database Cloud Service](https://cloud.oracle.com/database) is available on the Oracle Cloud Infrastructure (OCI) and featured as an easy to deploy and configure database system. They are available as a Virtual Machine (VM), Bare Metal servers or on state-of-the-art Exadata hardware. Customers can either provision them with the database license included or BYOL (Bring Your Own License). You can get the latest pricing for this service [here](https://cloud.oracle.com/database/pricing).\\n\\n![Cost estimates for an OCI DB System](./assets/oci-dbcs-costs.png)\\n\\nIn the case of the trial account, it appears that I am only being \\"billed\\" for the cost of resources needed to run a DB System:\\n\\n* CA$0.086 per OCPU hour - this is the price for a [VM.Standard2.1](https://cloud.oracle.com/iaas/pricing) VM that comes with 1 OCPU and 15 GB memory.\\n* CA$0.0573 for 1 GB per month of Block Storage - usage for the day was 22.97 GB that totals to approximately 712 GB per month. This is the smallest capacity that can be provisioned for a DB system, of which, 256 GB is for user data.\\n* CA$0.0343842 for 1 GB per month of Object Storage - I had performed only one full backup. For the trial it seems impossible to enable automatic daily backups that would been useful to have a better sense of true costs. The usage for a backup without creating any data was 0.02 GB. That amounts to approximately 0.62 GB for the month. I did not do any recovery or anything that might have been considered a \\"request\\", so no charges were incurred there.\\n\\nIn the absence of an actual charge on using a fully-licensed Standard Edition database, I am forced to rely on how much it would cost to host the DB System through their web pricing, which is CA$0.5437 per OCPU hour. Using the [Cost Estimator](https://cloud.oracle.com/cost-estimator), that amounts to about CA$404.51 per month. Unless someone tells me otherwise, I will also assume that one would need to budget for the minimum 712 GB Block Storage and any Object Storage usage. They aren\'t terribly expensive, but it\'s good to have that buffer and not be surprised at the end of the pay period.\\n\\nIn the case of using Object Storage for RMAN backups, it would be worth noting that in the Cost Estimator, the [Oracle Database Backup Cloud Service](https://cloud.oracle.com/database_backup) is available and can be added to the configuration. It costs more than the \\"regular\\" Object Storage charges but according to the pricing calculator, has unlimited **inbound** data transfer and DELETE requests. There will likely be other charges (**outbound** data transfers and requests) should recovery be needed or creating a new DB System from the backups.\\n\\nWhile pay-per-use models might seem to be a fairer way of charging customers, it does add a huge layer of complexity when estimating costs. For that reason, it\'s no wonder why developers find the OCI (or even Amazon Web Services) \\"hard\\" and prefer more budgeting-friendly cloud providers like [DigitalOcean](https://m.do.co/c/6f9b549ca569) and [Linode](https://www.linode.com/?r=41672b20d515344de465e9ed44c1a75356445597).\\n\\nThat being said, none of these developer-friendly platforms provides Oracle Database Standard Edition and up. Spending about CA$4,848 annually would almost seem reasonable compared to hosting on-premise considering the cost of:\\n\\n1. Hardware - servers, network, power and redundancy equipment\\n1. Software licenses\\n1. Physical space to house the equipment\\n1. Other systems and equipment needed to ensure uptime and security\\n1. Specialists for maintaining equipment, network and systems\\n\\nProbably the only non-subscription cost that is needed is for hiring a qualified expert, the **OCI Administrator**!"},{"id":"id/1905","metadata":{"permalink":"/content/id/1905","source":"@site/posts/day-4-30-days-on-the-oracle-cloud/index.md","title":"Day 4 - 30 Days on the Oracle Cloud","description":"Photo by Oliver Menyhart","date":"2019-03-02T16:32:42.000Z","formattedDate":"March 2, 2019","tags":[{"label":"oracle","permalink":"/content/tags/oracle"},{"label":"cloud","permalink":"/content/tags/cloud"},{"label":"oci","permalink":"/content/tags/oci"},{"label":"IaaS","permalink":"/content/tags/iaa-s"},{"label":"infrastructure","permalink":"/content/tags/infrastructure"},{"label":"devops","permalink":"/content/tags/devops"},{"label":"orclapex","permalink":"/content/tags/orclapex"}],"readingTime":5.15,"hasTruncateMarker":true,"authors":[{"name":"Adrian Png","title":"Senior Cloud Solutions Architect @ Insum","url":"https://github.com/fuzziebrain","imageURL":"https://github.com/fuzziebrain.png","key":"fuzziebrain"}],"frontMatter":{"title":"Day 4 - 30 Days on the Oracle Cloud","tags":["oracle","cloud","oci","IaaS","infrastructure","devops","orclapex"],"slug":"id/1905","authors":["fuzziebrain"],"categories":["Technology","Infrastructure","Oracle Cloud"],"date":"2019-03-02T16:32:42.000Z"},"prevItem":{"title":"Day 11 - 30 Days on the Oracle Cloud","permalink":"/content/id/1906"},"nextItem":{"title":"Day 2 - 30 Days on the Oracle Cloud","permalink":"/content/id/1904"}},"content":"![Photo by Oliver Menyhart](./assets/bookkeeper-1016299_1280x720.jpg)\\n\\nAfter running my [Oracle Application Express (APEX)](https://apex.oracle.com) instance (see the [previous article](/content/id/1904) on what it takes) for more than a day, I think I have enough to take a first measure of costs and fact check the [prices](https://cloud.oracle.com/iaas/pricing) listed by Oracle!\\n\\n\x3c!-- truncate --\x3e\\n\\n## Current Configuration (Recap)\\n\\nAt the moment, I have created the following resources in my [Oracle Cloud Infrastructure (OCI)](https://cloud.oracle.com/iaas/) trial account:\\n\\n* A Virtual Cloud Network (VCN) containing one of each:\\n    * Public Subnet\\n    * Route Table\\n    * Internet Gateway\\n    * Security List\\n    * DHCP configuration\\n* A Compute Instance with shape *VM.Standard.E2.1*, with the following resources:\\n    * 1 OCPU\\n    * 8 GB Memory\\n    * 46.6 GB Block Storage for Boot Volume\\n    * And of course we have an APEX stack housed comfortably on this server!\\n\\n> NOTE: OCI\'s OCPU is equivalent to 2 vCPUs, a unit often used by other cloud hosting providers.\\n\\n## Billing and Cost Analysis\\n\\nTo view the current billing information through the OCI console, click <i class=\\"fa-solid fa-bars\\"></i> <i class=\\"fa-solid fa-caret-right\\"></i> Billing <i class=\\"fa-solid fa-caret-right\\"></i> Cost Analysis.\\n\\n![Cost Analysis for March 01, 2019](./assets/oci-billing-20190301.png)\\n\\nBased on these estimates, I can expect to pay CA$1.06 x 365 days / 12 = **CA$32.24/mth**. Using the [pricing calculator](https://cloud.oracle.com/cost-estimator), the estimate is pretty accurate at CA$33/mth or US$24/mth. Let\'s compare this with similar offerings by other hosting companies.\\n\\n### DigitalOcean\\n\\nDigitalOcean\'s pricing information can be found [here](https://www.digitalocean.com/pricing/?refcode=6f9b549ca569). The product that has specifications closest to OCI\'s *VM.Standard.E2.1* is their new *General Purpose Droplets*. The lowest priced system comes with the following specifications:\\n\\n* 2 vCPUs\\n* 8 GB memory\\n* 25 GB storage <i class=\\"fa-solid fa-arrow-circle-down rate-down\\"></i>\\n* 4 TB network transfer\\n\\nMonthly Fee: **US$60**\\n\\n### Linode\\n\\nLinode\'s pricing information can be found [here](https://www.linode.com/pricing/?r=41672b20d515344de465e9ed44c1a75356445597). Their most similar offering has the following specifications:\\n\\n* 2 vCPUs\\n* 4 GB memory <i class=\\"fa-solid fa-arrow-circle-down rate-down\\"></i>\\n* 80 GB storage <i class=\\"fa-solid fa-arrow-circle-up rate-up\\"></i>\\n* 4 TB network transfer\\n\\nMonthly Fee: **US$20** (DigitalOcean offers the same package at the same price)\\n\\n### Conclusion\\n\\nTo run an APEX stack using [Oracle Database 18c Express Edition (18cXE)](https://oracle.com/xe), the $20/mth package from either DigitalOcean or Linode is more than sufficient. 18cXE limits us to use only 2 GB of memory, 2 CPU threads and store up to 12 GB of user data. Setting up a VM in either of the two hosts are also considerably simpler and more straightforward.\\n\\nFor an enterprise though, consider also the other services that are made available by each provider listed in this article.\\n\\n:::note\\nProviders update the service offerings all the time. Always check with them for the latest available features.\\n:::\\n\\n| Service/Feature                   | DigitalOcean                                       | Linode                                             | OCI                                              |\\n| --------------------------------- | -------------------------------------------------- | -------------------------------------------------- | ------------------------------------------------ |\\n| Advanced Identity Managerment     | <i class=\\"fa-solid fa-times-circle rate-down\\"></i> | <i class=\\"fa-solid fa-times-circle rate-down\\"></i> | <i class=\\"fa-solid fa-check-circle rate-up\\"></i> |\\n| Advanced Network Management       | <i class=\\"fa-solid fa-times-circle rate-down\\"></i> | <i class=\\"fa-solid fa-times-circle rate-down\\"></i> | <i class=\\"fa-solid fa-check-circle rate-up\\"></i> |\\n| Advanced Storage Encryption       | <i class=\\"fa-solid fa-times-circle rate-down\\"></i> | <i class=\\"fa-solid fa-times-circle rate-down\\"></i> | <i class=\\"fa-solid fa-check-circle rate-up\\"></i> |\\n| Auto Backup                       | <i class=\\"fa-solid fa-check-circle rate-up\\"></i>   | <i class=\\"fa-solid fa-check-circle rate-up\\"></i>   | <i class=\\"fa-solid fa-check-circle rate-up\\"></i> |\\n| Auto Scaling                      | <i class=\\"fa-solid fa-times-circle rate-down\\"></i> | <i class=\\"fa-solid fa-times-circle rate-down\\"></i> | <i class=\\"fa-solid fa-check-circle rate-up\\"></i> |\\n| Block Storage                     | <i class=\\"fa-solid fa-check-circle rate-up\\"></i>   | <i class=\\"fa-solid fa-check-circle rate-up\\"></i>   | <i class=\\"fa-solid fa-check-circle rate-up\\"></i> |\\n| DNS Zone Management               | <i class=\\"fa-solid fa-check-circle rate-up\\"></i>   | <i class=\\"fa-solid fa-check-circle rate-up\\"></i>   | <i class=\\"fa-solid fa-check-circle rate-up\\"></i> |\\n| Email Delivery                    | <i class=\\"fa-solid fa-times-circle rate-down\\"></i> | <i class=\\"fa-solid fa-times-circle rate-down\\"></i> | <i class=\\"fa-solid fa-check-circle rate-up\\"></i> |\\n| Monitoring & Alerts               | <i class=\\"fa-solid fa-check-circle rate-up\\"></i>   | <i class=\\"fa-solid fa-check-circle rate-up\\"></i>   | <i class=\\"fa-solid fa-check-circle rate-up\\"></i> |\\n| Object Storage                    | <i class=\\"fa-solid fa-check-circle rate-up\\"></i>   | <i class=\\"fa-solid fa-times-circle rate-down\\"></i> | <i class=\\"fa-solid fa-check-circle rate-up\\"></i> |\\n| VPN to Customer-Premise Equipment | <i class=\\"fa-solid fa-times-circle rate-down\\"></i> | <i class=\\"fa-solid fa-times-circle rate-down\\"></i> | <i class=\\"fa-solid fa-check-circle rate-up\\"></i> |\\n\\n:::note\\nWhile available, some of these listed features are for-fee services and prices vary amongst the different providers.\\n:::\\n\\nI will delve deeper into some of these features in a future post, but suffice to say, OCI customers get access to a whole lot more for just $4 more per month! Not to forget having double the amount of RAM and the ability to customise the boot volume size, upwards of 46.6 GB.\\n\\n## Path to Database Cloud Service\\n\\nPerhaps the larger consideration is how a business would progress to the \\"next level\\". At some point, 12 GB of user data isn\'t going to be enough and we need to progress to the next edition of the database.\\n\\nBefore ending this article, I would like to walk through the steps for creating a lowest-cost DB System in a different compartment. For completeness, I will also add a Compute Instance that would host an Apache Web Server and Tomcat Server to complete the APEX stack. The billing outcome will be reviewed in a future article.\\n\\n1. After creating a new compartment (*PROD*) and VCN, click <i class=\\"fa-solid fa-bars\\"></i> <i class=\\"fa-solid fa-caret-right\\"></i> Bare Metal, VM, and Exadata <i class=\\"fa-solid fa-caret-right\\"></i>\\n1. On the DB Systems page, make sure that the new compartment is created, and then click the *Launch DB System* button.\\n1. Enter the required DB System parameters and then launch the server. If the required fields are empty, the launch button will **not** be enabled.\\n    ![Create DB System dialog](./assets/oci-create-dbsystem.png)\\n    * **DISPLAY NAME**: *Give it a unique name*\\n    * **AVAILABILITY DOMAIN**: * Select a suitable AD*\\n    * **SHAPE TYPE**: VIRTUAL MACHINE\\n    * **ORACLE DATABASE SOFTWARE EDITION**: Standard Edition\\n    * **AVAILABLE STORAGE SIZE (GB)**: 256\\n    * **LICENSE TYPE**: LICENSE INCLUDED\\n    * **SSH PUBLIC KEY**: *Add a SSH public key*\\n    * **VIRTUAL CLOUD NETWORK**: *Select the target VCN*\\n    * **CLIENT SUBNET**: *A private subnet is highly recommended*\\n    * **HOSTNAME PREFIX**: *Give it a unique name*\\n    * **DATABASE NAME**: *Provide the desired CDB name*\\n    * **DATABASE VERSION**: 18.0.0.0\\n    * **PDB NAME**: *Provide the desired PDB name*\\n    * **DATABASE ADMIN PASSWORD**: *Enter a password that complies with the requirements*\\n\\nTill next time..."},{"id":"id/1904","metadata":{"permalink":"/content/id/1904","source":"@site/posts/day-2-30-days-on-the-oracle-cloud/index.md","title":"Day 2 - 30 Days on the Oracle Cloud","description":"Remember this ad?","date":"2019-02-28T00:22:50.000Z","formattedDate":"February 28, 2019","tags":[{"label":"oracle","permalink":"/content/tags/oracle"},{"label":"cloud","permalink":"/content/tags/cloud"},{"label":"oci","permalink":"/content/tags/oci"},{"label":"IaaS","permalink":"/content/tags/iaa-s"},{"label":"infrastructure","permalink":"/content/tags/infrastructure"},{"label":"devops","permalink":"/content/tags/devops"},{"label":"orclapex","permalink":"/content/tags/orclapex"}],"readingTime":5.26,"hasTruncateMarker":true,"authors":[{"name":"Adrian Png","title":"Senior Cloud Solutions Architect @ Insum","url":"https://github.com/fuzziebrain","imageURL":"https://github.com/fuzziebrain.png","key":"fuzziebrain"}],"frontMatter":{"title":"Day 2 - 30 Days on the Oracle Cloud","tags":["oracle","cloud","oci","IaaS","infrastructure","devops","orclapex"],"slug":"id/1904","authors":["fuzziebrain"],"categories":["Technology","Infrastructure","Oracle Cloud"],"date":"2019-02-28T00:22:50.000Z"},"prevItem":{"title":"Day 4 - 30 Days on the Oracle Cloud","permalink":"/content/id/1905"},"nextItem":{"title":"Day 0 - 30 Days on the Oracle Cloud","permalink":"/content/id/1903"}},"content":"<youtube youTubeId=\\"yNVE59GJ9xM\\" />\\n\\nRemember this ad?\\n\\nIBM BladeCenters were revolutionary at its time. System Administrators would know that one of the largest expense when building out a server infrastructure is real estate costs. Shrinking racks of servers and consolidating into blade servers meant you could put more hardware in less space. However, they were expensive and had their fair share of skeptics. Adopting a proprietary backbone simply meant you were investing into a brand and hence subject to vendor lock-in.\\n\\nThen came virtualization. With more powerful CPUs, cheaper RAMs and more cost-effective storage solutions, we are now able to cramp more servers into a single server. Other pieces of traditional server infrastructure, e.g. network and storage, could be \\"software-defined\\". The server footprint very much \\"vanished into thin air\\"!\\n\\n\x3c!-- truncate --\x3e\\n\\nThink of the [Oracle Cloud Infrastructure (OCI)](https://cloud.oracle.com/iaas) as your server room in the cloud. With OCI, customers have access to the basic components needed for every server infrastructure and more! Besides virtual networks, firewalls, machines and storage, customers also have access to various specialty appliances and services. That includes autonomous database systems and container services. You can access the OCI console using a URL like `https://console.us-ashburn-1.oraclecloud.com/?tenant=<MY_TENANT>` where `<MY_TENANT>` is the name of your tenancy. Login with the Oracle Cloud credentials.\\n\\n![OCI Console](./assets/oci-console-landing-page.png)\\n\\nAs an avid [Oracle Application Express (APEX)](https://apex.oracle.com) fan, my main interest is always about setting up the components necessary to build my APEX Stack. On Day 2 of my trial, let\'s examine the shortest path needed to get the environment up and running.\\n\\n## Required Components\\n\\n![A Simple Topology](./assets/oci-topology.png)\\n\\n1. [Virtual Cloud Network (VCN)](https://docs.cloud.oracle.com/iaas/Content/Network/Tasks/managingVCNs.htm)\\n2. Subnet\\n3. Internet Gateway\\n4. Security Rules\\n5. Virtual Machine\\n\\nThere are **no** usage costs for components 1 to 4, so I will create them in that order. All resources will be created physically in the Ashburn data centre (region).\\n\\n### Compartments\\n\\nCompartments are a great way for partitioning your resources for various intents, e.g. billing, access control. There are again, no costs to create one, and for my environment, I have decided to create one per environment: *DEV*, *TEST*, *PROD*.\\n\\n1. Click {% fa bars %} {% fa caret-right %} Identity {% fa caret-right %} Compartments\\n2. On the *Compartments* page, I have two compartments already created. I will create a *DEV* compartment under the tenancy\'s root compartment. Click *Create Compartment* and then enter a name and description for the compartment. Choose *<MY_TENANT> (root)* for the parent compartment.\\n3. Click *Create Compartment*\\n\\n![Create a compartment for DEV resources](./assets/oci-create-dev-compartment.png)\\n\\n### Network\\n\\nAll resources will need to live within a VCN. It encapsulates the subnets, gateways\\n\\n1. Click {% fa bars %} {% fa caret-right %} Networking {% fa caret-right %} Virtual Cloud Networks\\n2. On the *Virtual Cloud Networks* page, click *Create Virtual Cloud Network*. Select the target compartment and enter a name if necessary. Provide the network range (*CIDR BLOCK*) for this subnet, e.g. `10.1.0.0/16`. There online tools that can help determine what the IPv4 range for a [CIDR](https://wikipedia.org/wiki/Classless_Inter-Domain_Routing) block.\\n3. Click *Create Virtual Cloud Network*\\n\\n![Create a VCN](./assets/oci-create-dev-vcn.png)\\n\\n4. Next, create a public subnet. On the VCN Details page, check that the correct compartment is selected (on the left side of the page), then click *Create Subnet*. Enter a name if required, and set the following values:\\n    * **SUBNET TYPE**: REGIONAL\\n    * **CIDR BLOCK**: 10.1.1.0/24 (or whatever CIDR block you choose)\\n    * **ROUTE TABLE**: Default Route Table for vcn-dev\\n    * **SUBNET ACCESS**: PUBLIC SUBNET\\n    * **Security Lists**: Default Security List for vcn-dev\\n\\n![Create a public subnet](./assets/oci-create-dev-subnet.png)\\n\\n5. Back on the VCN Details page, click *Internet Gateways* on the left.\\n6. On the *Internet Gateways* page, click *Create Internet Gateway*.\\n7. Enter a name if required and then click *Create Internet Gateway*.\\n\\n![Create an Internet Gateway](./assets/oci-create-dev-ig.png)\\n\\n8. On the left again, click *Route Tables* and then on the *Route Tables* page, click on the default route table.\\n9. Click *Edit Route Rules* and then enter the following in the dialog:\\n    * **TARGET TYPE**: Internet Gateway\\n    * **DESTINATION CIDR BLOCK**: 0.0.0.0/0\\n    * **COMPARTMENT**: *Select the desired compartment*\\n    * **TARGET INTERNET GATEWAY**: *Select the Internet Gateway created earlier*\\n\\n### Compute Instance - Virtual Machine\\n\\n1. Click {% fa bars %} {% fa caret-right %} Compute {% fa caret-right %} Instances\\n2. On the *Instances* page, click *Create Instance*.\\n3. Provide the following information:\\n    * **Name**: *e.g. fbapex01*\\n    * **Availability Domain**: AD 1\\n    * **Operating System**: Oracle Linux 7.6 (other options include Ubuntu, CentOS and Windows Server)\\n    * **Instance Type**: Virtual Machine\\n    * **Shape**: VM.Standard.E2.1\\n    * **Boot Volume**: *leave the defaults*\\n    * **Add SSH Key**: *Either submit the public key file or paste the SSH key*\\n    * **VCN**: *Select the desired compartment and VCN*\\n    * **Subnet**: *Select the desired compartment and subnet*\\n\\n![Create a Virtual Machine](./assets/oci-create-dev-compute.png)\\n\\nCompute Instances are sold in \\"Shapes\\". Resources provided by the various shapes are documented [here](https://docs.cloud.oracle.com/iaas/Content/Compute/References/computeshapes.htm). Availability is dependent on region. In Ashburn, the new (cheaper) AMD-based shapes are available. I had selected *VM.Standard.E2.1* that provides 1 OCPU (2 vCPUs), a generous 8 GB of memory and ~50 GB storage!\\n\\n## Accessing the Server\\n\\n![The new VM](./assets/oci-compute-created.png)\\n\\nThe server can be accessed via the public IP address. All instances are created with the user `opc`. Use the private key that generated the SSH public key provided when the instance was created. The `opc` user is on the *sudoers* list and can be used to perform system administration tasks on the server.\\n\\nI struggled a little bit getting my APEX stack up and running but was eventually able to get it running with the helper scripts I had shared in an earlier [post](/content/id/1902).\\n\\n![APEX Up and Running](./assets/oci-apex.png)\\n\\n## Summary\\n\\n> ~~3. What do I need to do to create an Oracle Application Express (APEX) stack?~~\\n\\n![Oracle 18c XE - EM Express](./assets/oci-emexpress.png)\\n\\nThis article provides a list of tasks needed to spin up a VM that could then be used to create an APEX stack powered by the free [Oracle Database 18c Express Edition](https://oracle.com/xe). Compared to other hosting platforms like [DigitalOcean](https://m.do.co/c/6f9b549ca569) and [Linode](https://www.linode.com/?r=41672b20d515344de465e9ed44c1a75356445597), the setup process feels like an overkill. However, the OCI was designed for larger goals and this is merely a starting point for me. In the coming days, I will explore other services that OCI provides for creating \\"enterprise-grade\\" IT environments including the much talk about Database Systems.\\n\\nStay tuned!"},{"id":"id/1903","metadata":{"permalink":"/content/id/1903","source":"@site/posts/day-0-30-days-on-the-oracle-cloud/index.md","title":"Day 0 - 30 Days on the Oracle Cloud","description":"Photo by JuergenPM","date":"2019-02-25T23:37:21.000Z","formattedDate":"February 25, 2019","tags":[{"label":"oracle","permalink":"/content/tags/oracle"},{"label":"cloud","permalink":"/content/tags/cloud"},{"label":"oci","permalink":"/content/tags/oci"},{"label":"IaaS","permalink":"/content/tags/iaa-s"},{"label":"infrastructure","permalink":"/content/tags/infrastructure"},{"label":"devops","permalink":"/content/tags/devops"}],"readingTime":4.095,"hasTruncateMarker":true,"authors":[{"name":"Adrian Png","title":"Senior Cloud Solutions Architect @ Insum","url":"https://github.com/fuzziebrain","imageURL":"https://github.com/fuzziebrain.png","key":"fuzziebrain"}],"frontMatter":{"title":"Day 0 - 30 Days on the Oracle Cloud","tags":["oracle","cloud","oci","IaaS","infrastructure","devops"],"slug":"id/1903","authors":["fuzziebrain"],"categories":["Technology","Infrastructure","Oracle Cloud"],"date":"2019-02-25T23:37:21.000Z"},"prevItem":{"title":"Day 2 - 30 Days on the Oracle Cloud","permalink":"/content/id/1904"},"nextItem":{"title":"Helper Scripts - Build an APEX Stack","permalink":"/content/id/1902"}},"content":"![Photo by JuergenPM](./assets/tree-247122_1280x720.jpg)\\n\\nI have read many comments on social media and blogs about various experiences using the [Oracle Cloud](https://cloud.oracle.com) for hosting web applications and databases. There are probably more bad than good accounts, and so I wanted to experience the process myself using a trial subscription.\\n\\nTypically, a trial account only gets you a US$300 credit to use on the Oracle Cloud. The credits are good 30 days, after that, either your upgrade to a paid plan, or you lose everything. You are also required to provide a valid credit card for verification purposes only. According to the sign up page, users are only billed after choosing to \\"upgrade\\" the account.\\n\\n\x3c!-- truncate --\x3e\\n\\n~~Incidentally, and very timely, Oracle had a cloud workshop planned in Vancouver, and I was enrolled in a custom trial that will allow me to participate fully during the workshop. **Thank you Oracle!**~~\\n\\nCouldn\'t get their approval to blog about the trial. Started another trial that I can use for this project, but had to follow through on the verification project, so see below with the additional steps needed to get your trial started.\\n\\n## Objectives\\n\\nTo get started, here are a few questions I\'d like answered through the trial:\\n\\n1. How difficult is it to sign up for the service?\\n2. What services do I have access to?\\n3. What do I need to do to create an [Oracle Application Express (APEX)](https://apex.oracle.com) stack?\\n4. What developer tools that can improve my productivity as an APEX developer and Oracle Cloud Infrastructure (OCI) Administrator?\\n5. How much does it really cost?\\n\\nAs the trial was a welcomed surprise, I haven\'t really had the chance to plan the series, so the list of questions here are organic. I am of course, open to suggestions, so if there are any questions that I can help answer, please leave me a comment.\\n\\n## Disclosure Statement\\n\\n1. I do have some experience working with Oracle Cloud products, but this was my first attempt at setting up an account.\\n2. I work for a company that provides services to manage customers\' Oracle Cloud services. However, any opinions expressed in this blog series and on my personal blog site are my own and in no way represents the views of my employer.\\n3. The custom trial account I have used to write this blog series was intended to be used for a workshop. Using it to write these articles are solely out of convenience of not setting up a separate trial subscription.\\n\\n## Sign Up\\n\\nAs part of the pre-workshop administration process, the folks at Oracle were quick to provide me an email containing a sign up link for the custom trial account. That brought me to a page very similar to the regular sign up page, minus the requirements for submitting a credit card for verification.\\n\\n![Oracle Cloud Trial Sign Up Page](./assets/oci-trial-signup.png)\\n\\n~~There were no hassles involved here, but I wonder what the credit card verification process entails. Also, without the need to share payment information, I felt less threatened by unexpected charges.~~\\n\\n![Verification and Validation](./assets/oci-signup-verification.jpg)\\n\\nI had to start a new trial, and went through the process of verifying my contact number and provided a credit card for validation. It wasn\'t hard to do with the code received in a few seconds and adding a payment method was instantaneous of course.\\n\\n![The Wait Begins](./assets/oci-creation-pause.png)\\n\\nI did receive the welcome email within the estimated time and was pleasantly surprised as I have heard stories about how long it takes just to get the cloud account credentials and begin burning credits.\\n\\n![The Credentials](./assets/oci-signup-complete-email.jpg)\\n\\nI opted to use the account URL provided to me on the sign up page to begin. This URL is based on the Cloud Account Name (or tenancy as I know it) and is in the format: `https://myservices-<CLOUD ACCOUNT_NAME>.console.oraclecloud.com`.\\n\\nSince I was signing in for the first time, I was expected to set a new password that complied with the set of rules, conveniently printed on the side.\\n\\n![Set a Secure Password](./assets/oci-one-time-password.png)\\n\\nOnce that was done, I was redirected to the \\"My Services Dashboard\\" that I could customize for a quick overview of services that I was consuming. Nothing yet!\\n\\n![Oracle Cloud - My Services Dashboard](./assets/oci-landing-page.png)\\n\\n## Summary\\n\\n> ~~1. How difficult is it to sign up for the service?~~\\n\\nOverall, the experience was good, bearing in mind that I skipped the credit card verification step(s). Though I had occasional error messages flash on my browser window, they went away after refreshing the page as instructed.\\n\\nWhile I am tempted to go on and create the compute and database resources through the OCI Console, I am compelled to hit a pause and wait till I am done with the workshop later this week, as I have no clue as to how much credits I\'ll need to complete the exercises and tasks.\\n\\nBack in a few days. Stay tuned!"},{"id":"id/1902","metadata":{"permalink":"/content/id/1902","source":"@site/posts/helper-scripts-build-an-apex-stack/index.md","title":"Helper Scripts - Build an APEX Stack","description":"Photo by Markus Distelrath","date":"2019-02-17T11:03:47.000Z","formattedDate":"February 17, 2019","tags":[{"label":"orclapex","permalink":"/content/tags/orclapex"},{"label":"docker","permalink":"/content/tags/docker"},{"label":"container","permalink":"/content/tags/container"},{"label":"scripting","permalink":"/content/tags/scripting"},{"label":"bash","permalink":"/content/tags/bash"},{"label":"full stack","permalink":"/content/tags/full-stack"}],"readingTime":1.52,"hasTruncateMarker":true,"authors":[{"name":"Adrian Png","title":"Senior Cloud Solutions Architect @ Insum","url":"https://github.com/fuzziebrain","imageURL":"https://github.com/fuzziebrain.png","key":"fuzziebrain"}],"frontMatter":{"title":"Helper Scripts - Build an APEX Stack","tags":["orclapex","docker","container","scripting","bash","full stack"],"slug":"id/1902","authors":["fuzziebrain"],"categories":["Technology","Programming","Oracle Application Express"],"date":"2019-02-17T11:03:47.000Z"},"prevItem":{"title":"Day 0 - 30 Days on the Oracle Cloud","permalink":"/content/id/1903"},"nextItem":{"title":"Announcing the APEX Challenge Coin","permalink":"/content/id/1727"}},"content":"![Photo by Markus Distelrath](./assets/container-2687008_1280x720.jpg)\\n\\nWhen [Oracle Database 18c Express Edition (18cXE)](https://oracle.com/xe) was released late last year, I was motivated to work on an [image](https://github.com/fuzziebrain/docker-oracle-xe) for running the newly released, **FREE** database using [Docker](https://www.docker.com). The scripts were based on the official Docker images [published by Oracle](https://github.com/oracle/docker-images). With much help from [Martin D\'Souza](https://twitter.com/martindsouza), we released the scripts to the community with pointers on how to install and run an [Oracle Application Express (APEX)](https://apex.oracle.com) stack.\\n\\n\x3c!-- truncate --\x3e\\n\\nWhile I am proud of the work Martin and I did, it has always been my intention to mothball the project as soon as the team from Oracle released support for 18cXE, which they did about two months ago.\\n\\nOne of the features of Oracle\'s images that I discussed with [Gerald Venzl](https://twitter.com/GeraldVenzl), who happened to be in Vancouver for the [BCOUG Tech Day](https://bcoug.org/techday) last year, was the `setup` and `startup` scripts folder. Any shell or SQL scripts placed in these folders would be automatically executed when the database is created (running the container the first time with no preexisting `oradata` mounted) and started respectively. The puzzle that needed to be solved was how best to inject the necessary files and code for installing and running APEX and [Oracle REST Data Services (ORDS)](https://www.oracle.com/ca-en/database/technologies/appdev/rest.html).\\n\\nI think I have something figured out and have placed the scripts I used in this [Github](https://github.com) [repository](https://github.com/fuzziebrain/docker-apex-stack). It includes a three-step formula that will:\\n\\n1. Grab the latest code from Oracle\'s repository, but only keep the important pieces for this to work.\\n2. Build the image; and then\\n3. Run the container.\\n\\nDetailed instructions are found in the repository. Please post questions under \\"Issues\\" if you need help and I\'ll do my best to respond as soon as I can.\\n\\nHope you have as much fun as I did trying to get all this to work!"},{"id":"id/1727","metadata":{"permalink":"/content/id/1727","source":"@site/posts/announcing-the-apex-challenge-coin/index.md","title":"Announcing the APEX Challenge Coin","description":"USS Chafee (DDG-90)","date":"2019-02-07T16:17:30.000Z","formattedDate":"February 7, 2019","tags":[{"label":"orclapex","permalink":"/content/tags/orclapex"},{"label":"challenge coin","permalink":"/content/tags/challenge-coin"},{"label":"gratitude","permalink":"/content/tags/gratitude"},{"label":"memorabilia","permalink":"/content/tags/memorabilia"}],"readingTime":2.1,"hasTruncateMarker":true,"authors":[{"name":"Adrian Png","title":"Senior Cloud Solutions Architect @ Insum","url":"https://github.com/fuzziebrain","imageURL":"https://github.com/fuzziebrain.png","key":"fuzziebrain"}],"frontMatter":{"title":"Announcing the APEX Challenge Coin","tags":["orclapex","challenge coin","gratitude","memorabilia"],"slug":"id/1727","authors":["fuzziebrain"],"categories":["Technology","Community","Oracle Application Express"],"date":"2019-02-07T16:17:30.000Z"},"prevItem":{"title":"Helper Scripts - Build an APEX Stack","permalink":"/content/id/1902"},"nextItem":{"title":"Tell Lora I Need Her","permalink":"/content/id/1726"}},"content":"![USS Chafee (DDG-90)](./assets/uss-chafee-ddg-90_1280x720.jpg)\\n\\nLate last year, I started binge watching the entire four seasons of [The Last Ship](https://www.tntdrama.com/shows/the-last-ship). It was one of the rare TV serials that my wife and I shared a common interest in. I think this was mostly because the first season was centered around the ship and its crew\'s fight against total human annihilation by an infectious disease. For me though, it was also about the military hardware and camaraderie amongst the soldiers.\\n\\n\x3c!-- truncate --\x3e\\n\\nIn the second season, Tom Chandler, the hero captain of the USS Nathan James, gave a teenager, Ray Diaz, a [challenge coin](https://wikipedia.org/wiki/Challenge_coin) when they were first introduced. It was a goodwill gesture between \\"leaders\\" and demonstrated his desire for unity between the navy officers and a band of teenage bounty hunters.\\n\\nFrom what I have read, challenge coins are usually given out by military unit commanders, sometimes in recognition of a member\'s contribution to the unit. Bearers carry them with a sense of pride and belonging, and can be used in \\"challenges\\" where the goal was free beer! There are variations to how this might be played, but that\'s not why I decided to create an **APEX Challenge Coin**.\\n\\n![APEX Challenge Coins](./assets/apex-challenge-coin_1280x720.jpg)\\n\\nThat\'s right! I designed and minted coins for exemplary [Oracle Application Express](https://apex.oracle.com) (APEX) Developers like YOU!\\n\\nA few years ago, I had made something very similar and gave them out at various events and presentations I did at [Kscope](https://kscope19.odtug.com). They were available in four colours, printed with 3D printers, and worked great as key chains or luggage tags.\\n\\n![3D-printed #orclapex tags](./assets/3d-printed-tags.jpg)\\n\\nTo preserve the spirit and significance of a challenge coin, I have decided that these will have to be earned. To receive one, you must:\\n\\n1. Be an APEX Developer (duh!).\\n2. Have performed exceedingly well in your professional career and/or contributed significantly to the community or an individual.\\n3. Know someone who thinks you have met both requirements (1) and (2).\\n\\nLike every APEX developer, each coin is unique and comes with a serial number. It sits comfortably in a tough PVC bag to protect the surface and paint from damage. If you know someone who is deserving and would like to give this person one to say a special thank you, then please reach out to me via [Twitter](https://twitter.com/fuzziebrain). There are costs, and as supplies are very limited, each person may only request a maximum of five coins.\\n\\nFor those who will earn this coin, my advanced congratulations! You make us, the community, very proud!"},{"id":"id/1726","metadata":{"permalink":"/content/id/1726","source":"@site/posts/tell-lora-i-need-her/index.md","title":"Tell Lora I Need Her","description":"I have written on two occasions, \\"Just a Drop of Oracle APEX\\" and \\"APEX and the Affordable Cloud!\\", about how easy it is to get up and running on hosting providers like Linode and DigitalOcean. The approach largely involves Vagrant, and though it is an easy to use tool, it still needed a little bit of DevOps heroism, and sometimes, dealing with API keys may not necessarily be a priority for someone looking for a quick way to setup a consistent and robust Oracle Application Express environment.","date":"2018-11-29T00:51:07.000Z","formattedDate":"November 29, 2018","tags":[{"label":"orclapex","permalink":"/content/tags/orclapex"},{"label":"18cXE","permalink":"/content/tags/18-c-xe"},{"label":"database","permalink":"/content/tags/database"},{"label":"cloud","permalink":"/content/tags/cloud"},{"label":"hosting","permalink":"/content/tags/hosting"},{"label":"stackscripts","permalink":"/content/tags/stackscripts"},{"label":"devops","permalink":"/content/tags/devops"},{"label":"linode","permalink":"/content/tags/linode"}],"readingTime":1.365,"hasTruncateMarker":true,"authors":[{"name":"Adrian Png","title":"Senior Cloud Solutions Architect @ Insum","url":"https://github.com/fuzziebrain","imageURL":"https://github.com/fuzziebrain.png","key":"fuzziebrain"}],"frontMatter":{"title":"Tell Lora I Need Her","tags":["orclapex","18cXE","database","cloud","hosting","stackscripts","devops","linode"],"slug":"id/1726","authors":["fuzziebrain"],"categories":["Technology","Programming","Oracle Application Express"],"date":"2018-11-29T00:51:07.000Z"},"prevItem":{"title":"Announcing the APEX Challenge Coin","permalink":"/content/id/1727"},"nextItem":{"title":"Certs for APEX Social Sign-In","permalink":"/content/id/1725"}},"content":"<youtube youTubeId=\\"pTjQgkHzbTk\\" />\\n\\nI have written on two occasions, \\"[Just a Drop of Oracle APEX](/content/id/1710/)\\" and \\"[APEX and the Affordable Cloud!](/content/id/cizx69jya000ra0pvv26u3cez/)\\", about how easy it is to get up and running on hosting providers like [Linode](https://www.linode.com/?r=41672b20d515344de465e9ed44c1a75356445597) and [DigitalOcean](https://m.do.co/c/6f9b549ca569). The approach largely involves [Vagrant](https://vagrantup.com), and though it is an easy to use tool, it still needed a little bit of DevOps heroism, and sometimes, dealing with API keys may not necessarily be a priority for someone looking for a quick way to setup a consistent and robust [Oracle Application Express](https://apex.oracle.com) environment.\\n\\n\x3c!-- truncate --\x3e\\n\\nLinode has a cool feature called [StackScripts<sup>TM</sup>](https://www.linode.com/stackscripts). It allows their customers to quickly choose and deploy a sever based on templates that auto-magically installs and configures the application stacks that required. DigitalOcean calls these \\"One-click apps\\", but unfortunately, I have yet to find one that installs the platform that I love, and neither is there a way for me to create a custom one-click app to do so. While Linode provides official templates, customers are free to create and share StackScripts<sup>TM</sup>, and that\'s what I did!\\n\\nLora means **L**inode **O**racle Database, **R**EST and **A**PEX stack. It was created so that anyone, beginners and experts alike, can quickly stand up (in approximately 30-60 minutes), a consistent APEX environment. [Sign up](https://manager.linode.com/session/signup?r=41672b20d515344de465e9ed44c1a75356445597) for an account, read the [guide](https://github.com/fuzziebrain/stackscripts/blob/master/lora/guide.md) and then deploy Lora today! With a few more tweaks/scripts that I hope to publish in the near future, you can also snap on a valid SSL/TLS certificate from [Let\'s Encrypt](https://letsencrypt.org/), harden SSH access and even [add support for R](/content/id/1724/)!\\n\\nMeanwhile, tell Lora that you love her. And tell Lora how much you need her!"},{"id":"id/1725","metadata":{"permalink":"/content/id/1725","source":"@site/posts/certs-for-apex-social-sign-in/index.md","title":"Certs for APEX Social Sign-In","description":"Verified!","date":"2018-11-17T14:25:02.000Z","formattedDate":"November 17, 2018","tags":[{"label":"orclapex","permalink":"/content/tags/orclapex"},{"label":"oauth2","permalink":"/content/tags/oauth-2"},{"label":"authentication","permalink":"/content/tags/authentication"},{"label":"google","permalink":"/content/tags/google"},{"label":"microsoft","permalink":"/content/tags/microsoft"},{"label":"facebook","permalink":"/content/tags/facebook"},{"label":"sso","permalink":"/content/tags/sso"},{"label":"single sign-on","permalink":"/content/tags/single-sign-on"}],"readingTime":3.505,"hasTruncateMarker":true,"authors":[{"name":"Adrian Png","title":"Senior Cloud Solutions Architect @ Insum","url":"https://github.com/fuzziebrain","imageURL":"https://github.com/fuzziebrain.png","key":"fuzziebrain"}],"frontMatter":{"title":"Certs for APEX Social Sign-In","tags":["orclapex","oauth2","authentication","google","microsoft","facebook","sso","single sign-on"],"slug":"id/1725","authors":["fuzziebrain"],"categories":["Technology","Programming","Oracle Application Express"],"date":"2018-11-17T14:25:02.000Z"},"prevItem":{"title":"Tell Lora I Need Her","permalink":"/content/id/1726"},"nextItem":{"title":"APEX and XE - A Match for Data Science","permalink":"/content/id/1724"}},"content":"![Verified!](./assets/seal-1280x720.jpg)\\n\\n> **Updated November 18, 2018**\\n> Added a help utility to populate an Oracle Wallet with root certificates\\n\\nOracle Database 18c Express Edition comes with a usable Oracle Wallet. In a previous [article](/content/id/1720/), I had described how to:\\n\\n1. Obtain the certificates necessary to make a successful SSL/TLS connection.\\n1. Create and add these certificates to the Oracle Wallet.\\n1. Configure the APEX instance to use the wallet.\\n\\nSomeone had asked in the comments, what certificates were necessary for Social Sign-in in [Oracle Application Express](https://apex.oracle.com) (APEX) to work. It\'s something that I think most of us would like to know too, so I did a little more digging and here\'s what I found for three of the more popular social platforms: Google, Facebook and Microsoft.\\n\\n\x3c!-- truncate --\x3e\\n\\nFortunately, the discovery I did for another previous [work](/content/id/1711/) on circumventing wallet issues helped in determining which URLs were called by the authentication workflow. To add a Generic OAuth2 Provider, three of these are required: Authorization, Token and User Info endpoints. \\n\\nWhen the authentication process begins, the user is first redirected to the Authorization endpoint and that interaction is performed through the browser. Upon successful authentication, the Identity Provider (IdP) then calls the APEX authentication callback URL with the necessary parameters. APEX calls the Token and User Info to obtain an access token and user information respectively. These are made through the APEX_WEB_SERVICE APIs and hence, the certificates for these two endpoints will be required. Here they are:\\n\\n| IdP | Endpoint | URL |\\n|-|-|-|\\n| Google | Token | https://www.googleapis.com/oauth2/v4/token |\\n|| User Info | https://www.googleapis.com/oauth2/v2/userinfo |\\n| Facebook<sup>*</sup> | Token | https://graph.facebook.com/v2.10/oauth/access_token |\\n|| User Info | https://graph.facebook.com/v2.10/me |\\n| Microsoft | Token | https://login.microsoftonline.com/common/oauth2/v2.0/token |\\n|| User Info | https://graph.microsoft.com/v1.0/me |\\n\\n> <sup>*</sup> Based on the findings of [Dimitri Gielis](https://twitter.com/dgielis) in this comprehensive [blog post](http://dgielis.blogspot.com/2018/06/facebook-google-and-custom.html)\\n\\nUsing the OpenSSL client, we can obtain the necessary certificates like we did [before](/content/id/1720/). However, I\'d like to highlight an important issue that I had missed in the previous post, and that is how to obtain the **root certificates**.\\n\\nDuring the [certificate validation process](https://knowledge.digicert.com/solution/SO16297.html), the full chain is needed to verify and trust the server certificate presented by the web service. Servers however, are not required to include the root certificate. Clients are expected to have those in the trust store and in fact, should ignore any root certificates that a server might send over the channel. Hence, when we attempted to retrieve the certificates for import into the Oracle Wallet, only the server and intermediate certificates were returned.\\n\\nBy importing only the intermediate certificates though, it looks like there\'s an implicit trust that the certificates were correctly signed and will validate even if you do not have the root certificates in the wallet. Always download these intermediate certificates with eyes wide open. The OpenSSL client does attempt to verify the certificates, so keep a look out for a message like this:\\n\\n```bash\\n$ openssl s_client -connect sha1-intermediate.badssl.com:443 -showcerts\\n...\\n---\\nSSL handshake has read 1938 bytes and written 302 bytes\\nVerification error: unable to verify the first certificate\\n---\\n...\\n```\\n\\nOr towards the end:\\n\\n```plaintext\\nVerify return code: 21 (unable to verify the first certificate)\\n```\\n\\nAssuming that these web services always transmit all the intermediate certificates, then we could reduce the amount of work by adding the root certificates to complete the trust chain. Based on the OpenSSL outputs, today, these are:\\n\\n| IdP | Root Certificate | Source |\\n|-|-|-|\\n| Google | GlobalSign (expected to change, see [announcement](https://security.googleblog.com/2017/01/the-foundation-of-more-secure-web.html))| [Google](https://pki.goog) |\\n| Facebook | DigiCert High Assurance EV Root CA | [Digicert](https://www.digicert.com/digicert-root-certificates.htm) |\\n| Microsoft | Baltimore CyberTrust Root | [Digicert](https://www.digicert.com/digicert-root-certificates.htm) |\\n\\nIf you trusted Google enough, they maintain a [collection](https://pki.goog/roots.pem) of root certificates that contains all the certificates above. Adding all the root certificates in this file will probably allow access to nearly every legitimate and secure web service on the planet. Alternatively, if OpenSSL is installed, and it probably is, then these certificates can also be found in `/etc/ssl/certs` or `/etc/pki/tls`.\\n\\n## Helper Script\\n\\nA simple Bash [script](https://gist.github.com/fuzziebrain/202f902d8fc6d8de586da5097a501047) to help populate an Oracle Wallet with bundled root certificates obtained from a reliable source.\\n\\n{% gist 202f902d8fc6d8de586da5097a501047 %}"},{"id":"id/1724","metadata":{"permalink":"/content/id/1724","source":"@site/posts/apex-and-xe-a-match-for-data-science/index.md","title":"APEX and XE - A Match for Data Science","description":"1280 720 APEX and ORE in Action","date":"2018-11-10T14:01:16.000Z","formattedDate":"November 10, 2018","tags":[{"label":"orclapex","permalink":"/content/tags/orclapex"},{"label":"ore","permalink":"/content/tags/ore"},{"label":"data","permalink":"/content/tags/data"},{"label":"science","permalink":"/content/tags/science"},{"label":"analytics","permalink":"/content/tags/analytics"},{"label":"rstats","permalink":"/content/tags/rstats"},{"label":"research","permalink":"/content/tags/research"},{"label":"rdm","permalink":"/content/tags/rdm"}],"readingTime":5.71,"hasTruncateMarker":true,"authors":[{"name":"Adrian Png","title":"Senior Cloud Solutions Architect @ Insum","url":"https://github.com/fuzziebrain","imageURL":"https://github.com/fuzziebrain.png","key":"fuzziebrain"}],"frontMatter":{"title":"APEX and XE - A Match for Data Science","tags":["orclapex","ore","data","science","analytics","rstats","research","rdm"],"categories":["Technology","Programming","Oracle Application Express"],"slug":"id/1724","authors":["fuzziebrain"],"date":"2018-11-10T14:01:16.000Z"},"prevItem":{"title":"Certs for APEX Social Sign-In","permalink":"/content/id/1725"},"nextItem":{"title":"For Your Eyes Only - Redact to Protect","permalink":"/content/id/1723"}},"content":"![1280 720 APEX and ORE in Action](./assets/using-r-with-orclapex.png)\\n\\nI [wrote](https://insum.ca/oracle-apex-statistics-r-u-serious/), quite a while ago, on how [Oracle Application Express](https://apex.oracle.com) (APEX) developers could integrate [R](https://www.r-project.org) functionality/code in their applications. This was done primarily through [Node.js](https://nodejs.org) packages and exposed as RESTful Web Services. Not the ideal solution, but useful for people wanting to integrate R statistical calculations and complex charting, but could not afford the hefty price tag for an Oracle Database Enterprice Edition license, plus the [Oracle Advanced Analytics](https://www.oracle.com/technetwork/database/options/advanced-analytics/overview/index.html) database option.\\n\\nThe revolutionary 18c release of the [Oracle Database Express Edition](https://oracle.com/xe) (XE) significantly changed the Data Analytics playing field. The no-cost platform has resource caps (2 CPU, 2 GB RAM and 12 GB data storage). It\'s not going to help perform complex [GWAS](https://wikipedia.org/wiki/Genome-wide_association_study) (Genome-wide Association Study), but should suffice for many smaller research projects, statistical reporting and creating useful prediction models.\\n\\nUnfortunately, the XE website has only a single line that says anything about this cool feature of the database:\\n\\n> If you prefer R programming, Oracle Database supports that too.\\n\\nHow do we get started?\\n\\n\x3c!-- truncate --\x3e\\n\\n## A Docker Project for ORE\\n\\nThe [Oracle R Enterprise (ORE) Documentation](https://docs.oracle.com/cd/E83411_01/) provides quite a lot of information on what\'s involved and how to setup a ORE environment. However, looking at what\'s installed in XE, it looks like the server components have already been installed (see `$(ORACLE_HOME/R)`). And if you attempted a silent ORE server install, the script complains about how a later version has already been installed and would not proceed.\\n\\nI took it upon myself to waddle through the mud and figure out what\'s involved to get ORE going on my workstation. The end result was a suite of scripts and [Dockerfile](https://docs.docker.com/engine/reference/builder/) for creating an ORE server and client for running R. These have been published in this public [Github repository](https://github.com/fuzziebrain/docker-oracle-ore). \\n\\n**NOTE** that this environment is meant for an explorer to bash quickly through the undergrowth, and head right into some interesting R and APEX coding. The [instructions](https://github.com/fuzziebrain/docker-oracle-ore/blob/master/README.md) and simplicity of running [Docker](https://www.docker.com) containers should provide you a clear path forward.\\n\\n## APEX and ORE\\n\\nI\'m an APEX developer, so including the environment to create great apps is a must! In this Docker environment, you will find APEX 18.2 and ORDS 18.3 pre-installed, configured and running. The banner image in this post is an actual screenshot of a running APEX app with ORE integration. Both charts are produced by R functions [plot](https://www.rdocumentation.org/packages/graphics/versions/3.3.0/topics/plot) and [ggplot2](https://cran.r-project.org/web/packages/ggplot2/) using [Gapminder](https://www.gapminder.org) [data](https://github.com/swcarpentry/r-novice-gapminder/blob/gh-pages/data/gapminder-FiveYearData.csv) imported into the database.\\n\\n### 1. Create the Table `GAPMINDER`\\n\\n```sql\\ncreate table gapminder(\\n  row_id number generated by default as identity\\n  , country varchar2(100)\\n  , year number\\n  , pop number \\n  , continent varchar2(100)\\n  , lifeExp number\\n  , gdpPercap number\\n)\\n/\\n\\nalter table gapminder\\nadd constraint gapminder_pk\\nprimary key (row_id)\\n;\\n```\\n\\n### 2. Create the scripts `PLOTDEMO` and `GGPLOTDEMO`\\n\\nWe have an option of using R or SQL APIs to create scripts for embedded execution. The following uses the SQL API `sys.rqScriptCreate`. In order to create scripts, the database user must have the `RQADMIN` role. Be wary of who you assigned this to, as R scripts allow access to the database server and there are inherent security risks involved.\\n\\n```plpgsql\\nbegin\\n  -- Demo using core R function plot\\n  sys.rqScriptCreate(\\n    v_name => \'PLOTDEMO\'\\n    , v_script => q\'[\\nfunction(data, title) {\\n  plot(data, main=title, type=\\"o\\", col=\\"blue\\")\\n}\\n]\'\\n    , v_global => false\\n    , v_overwrite => true\\n  );\\n\\n  -- Demo using a third-party library ggplot2\\n  sys.rqScriptCreate(\\n    v_name => \'GGPLOTDEMO\'\\n    , v_script => q\'[\\nfunction(mdata) {\\n  library(ggplot2)\\n\\n  gplot <- ggplot(\\n    mdata\\n    , aes(\\n        x = GDPPERCAP\\n        , y = LIFEEXP\\n        , color = CONTINENT\\n    )\\n  ) + geom_point()\\n  plot(gplot)\\n}\\n]\'\\n    , v_global => false\\n    , v_overwrite => true\\n  );\\nend;\\n/\\n```\\n\\n### 3. Grant Rights\\n\\n[BrendanTierney](https://twitter.com/brendantierney) wrote an excellent [blog post](https://www.oralytics.com/2016/08/how-to-get-ore-to-work-with-apex.html) on the new security model that ORE uses and what is needed to grant the appropriate privileges to allow embedded R scripts to run in APEX. The parsing schema does not require the rights to run these scripts, and in fact, the script will not run in APEX, even if the parsing schema created the script. The grants should be made to the user `APEX_PUBLIC_USER` instead.\\n\\n```plpgsql\\nbegin \\n  rqGrant(\'PLOTDEMO\', \'rqscript\', \'APEX_PUBLIC_USER\');\\n  rqGrant(\'GGPLOTDEMO\', \'rqscript\', \'APEX_PUBLIC_USER\');\\nend;\\n/\\n```\\n\\nAlternatively, with security in mind, you may also compile the scripts as \\"public\\". Do this by setting the `sys.rqScriptCreate` argument `v_global` to `true`.\\n\\n### 4. Applying the Scripts in APEX\\n\\nFor displaying both charts, a Display Image Page Item was used. With some [guidance](https://www.oralytics.com/2016/07/creating-ggplot2-graphics-using-sql.html), here\'s how I have them set up.\\n\\n1. Set the \\"Based On\\" setting to `BLOB Column returned by SQL statement`.\\n1. For the \\"SQL Statement\\", something like these should work.\\n\\n#### Using R `plot`\\n\\n```sql\\nwith data as (\\n  select year as \\"Year\\", lifeexp as \\"Life Expectancy\\"\\n  from gapminder\\n  where 1 = 1\\n    and lower(country) = \'singapore\'\\n  order by year\\n)\\nselect image\\nfrom table(\\n  rqTableEval(\\n    cursor(select * from data)\\n    , cursor(\\n        select \'Life Expectancy in Singapore\' as \\"title\\" \\n        from dual\\n    )\\n    , \'PNG\'\\n    , \'DEMO.PLOTDEMO\'\\n  )\\n)\\nwhere id = 1\\n```\\n\\n#### Using `ggplot`\\n\\n```sql\\nwith data as (\\n  select gdppercap, lifeexp, continent\\n  from gapminder\\n  where 1 = 1\\n)\\nselect image\\nfrom table(\\n  rqTableEval(\\n    cursor(select * from data)\\n    , null\\n    , \'PNG\'\\n    , \'DEMO.GGPLOTDEMO\'\\n  )\\n)\\nwhere id = 1\\n```\\n\\nIn both queries, using the `rqTableEval` funcction, data is retrieved from the database and passed into the scripts\' function parameters as a [`data.frame`](https://www.rdocumentation.org/packages/base/versions/3.3.0/topics/data.frame). R developers can then use the data and run various operations, returning the results as a `data.frame`, (PNG) image or XML.\\n\\nAlso, when referencing scripts, include the schema name that created the script. Without it, only references to public scripts are valid.\\n\\n> Brendan is a well-known presenter and writer on all Data Analytics and Machine Learning with Oracle tools. Be sure to [follow him](https://twitter.com/brendantierney), [read his work](https://www.oralytics.com/) and maybe buy his books if this is something you wish to pursue further. He\'s been doing this for quite a while and evidently, I saved a lot of time by looking up his blog in this mini project.\\n\\n## Besides Docker\\n\\nThe Docker environment I have put together isn\'t production ready. To set this up without Docker and have it running in your environment, take a look at the `Dockerfile` and relevant shell scripts to figure out what is involved. The following is a summary of activities that need to happen on the server-side.\\n\\n* Install a compatible R environment including dependencies for building and install R packages (see [`Dockerfile`](https://github.com/fuzziebrain/docker-oracle-ore/blob/master/server/Dockerfile)).\\n* Install R packages listed in [`enableORE.sh`](https://github.com/fuzziebrain/docker-oracle-ore/blob/master/server/scripts/enableORE.sh).\\n* Run the SQL script `$ORACLE_HOME/R/server/rqcfg.sql` against the target PDB (see [`enableORE.sh`](https://github.com/fuzziebrain/docker-oracle-ore/blob/master/server/scripts/enableORE.sh)).\\n\\n## Conclusion\\n\\nAPEX is an outstanding tool for **rapidly** building **modern** day web applications at **minimal costs and resources**. Combine this with the **power of R**, it should be a no-brainer for anyone choosing a development environment for [Research Data Management](https://www2.le.ac.uk/services/research-data/rdm/what-is-rdm)."},{"id":"id/1723","metadata":{"permalink":"/content/id/1723","source":"@site/posts/for-your-eyes-only-redact-to-protect/index.md","title":"For Your Eyes Only - Redact to Protect","description":"1280 720 Photo by Kaleidico","date":"2018-11-03T14:43:57.000Z","formattedDate":"November 3, 2018","tags":[{"label":"orclapex","permalink":"/content/tags/orclapex"},{"label":"data","permalink":"/content/tags/data"},{"label":"redact","permalink":"/content/tags/redact"},{"label":"confidential","permalink":"/content/tags/confidential"},{"label":"security","permalink":"/content/tags/security"},{"label":"protection","permalink":"/content/tags/protection"},{"label":"compliance","permalink":"/content/tags/compliance"}],"readingTime":8.385,"hasTruncateMarker":true,"authors":[{"name":"Adrian Png","title":"Senior Cloud Solutions Architect @ Insum","url":"https://github.com/fuzziebrain","imageURL":"https://github.com/fuzziebrain.png","key":"fuzziebrain"}],"frontMatter":{"title":"For Your Eyes Only - Redact to Protect","tags":["orclapex","data","redact","confidential","security","protection","compliance"],"categories":["Technology","Programming","Oracle Application Express"],"slug":"id/1723","authors":["fuzziebrain"],"date":"2018-11-03T14:43:57.000Z"},"prevItem":{"title":"APEX and XE - A Match for Data Science","permalink":"/content/id/1724"},"nextItem":{"title":"Preview PDF Uploads in an APEX App (Cross-post)","permalink":"/content/id/1722"}},"content":"![1280 720 Photo by Kaleidico](./assets/mistake-968334_1920_1280x720.jpg)\\n\\nIn a previous [blog post](/content/id/1721), I provided a recipe for enforcing encryption for data-at-rest. However, it is important to remember that [Transparent Data Encryption](https://docs.oracle.com/en/database/oracle/oracle-database/18/asoag/introduction-to-transparent-data-encryption.html#GUID-769EC29B-0107-40FE-9A9D-BF81A4BBD0E9) (TDE) protects your data from bad actors, who might have illegally gained access to the data files. Data exists unencrypted when loaded and accessed in the database. Malicious attacks can come from internal sources as well. When storing user confidential information, data stewards need to take greater responsibility in ensuring that data is only seen by staff who are authorized and require access to privileged information. That could mean creating tiered access and other protections like data redaction.\\n\\nFor example, an e-commerce website that handles credit card transactions, may require call centre staff to have access to the last four digits of a credit card number for verifying customers\' identity when support call are received. Or an insurance claims website that requires claimants to submit their health identity number. The application displays the partially or fully-masked ID, enough to suggest that the data has been captured, but insufficient for a passerby to steal a glance.\\n\\nOracle\'s Data Redaction makes this possible, and with the latest release of [Oracle Database 18c Express Edition](https://oracle.com/xe), this technology is now [acessible](https://docs.oracle.com/en/database/oracle/oracle-database/18/xelic/licensing-information.html) to developers for **FREE**! If you are new to Data Redaction, here\'s a simple recipe to get started!\\n\\n\x3c!-- truncate --\x3e\\n\\n## Staging the Example\\n\\n### Create Table `PAYMENTINFO`\\n\\nCreate a simple table called `paymentinfo` to store customers\' names and credit card information.\\n\\n```sql\\ncreate table paymentinfo (\\n  transactionid number generated always as identity\\n  , first_name varchar2(200) not null\\n  , last_name varchar2(200) not null\\n  , ccard_number varchar2(30) not null\\n)\\n/\\n```\\n\\n### Add Mock Data\\n\\nThere are many sites and applications out there that help you generate mock data. Here are two examples:\\n\\n* https://www.generatedata.com/\\n* https://www.mockaroo.com/\\n\\n![Generate Mock Data](./assets/mockaroo.png)\\n\\nBoth sites help generate various types of data. For the purpose of demo, I generated a 100 rows of first names, last names and fictitious credit card numbers (any similarities is pure coincidence). Load the data.\\n\\n```sql\\ninsert into paymentinfo (first_name, last_name, ccard_number) values (\'Brigham\', \'Raftery\', \'3558886053942589\');\\ninsert into paymentinfo (first_name, last_name, ccard_number) values (\'Willi\', \'Milksop\', \'3566444037931078\');\\ninsert into paymentinfo (first_name, last_name, ccard_number) values (\'Spencer\', \'Toy\', \'3578657283591918\');\\ninsert into paymentinfo (first_name, last_name, ccard_number) values (\'Eleanor\', \'Sherlaw\', \'201745575867996\');\\ninsert into paymentinfo (first_name, last_name, ccard_number) values (\'Matelda\', \'Joskowicz\', \'3567589132290124\');\\ninsert into paymentinfo (first_name, last_name, ccard_number) values (\'Hyacinth\', \'Haville\', \'201542493937096\');\\ninsert into paymentinfo (first_name, last_name, ccard_number) values (\'Cristi\', \'Yakubowicz\', \'3535186423765655\');\\ninsert into paymentinfo (first_name, last_name, ccard_number) values (\'Bess\', \'Atte-Stone\', \'501891982886122702\');\\ninsert into paymentinfo (first_name, last_name, ccard_number) values (\'Madeleine\', \'Spacy\', \'5299626156704075\');\\ninsert into paymentinfo (first_name, last_name, ccard_number) values (\'Alick\', \'Lilly\', \'4041597161949423\');\\n```\\n\\nNow query the data.\\n\\n```sql\\nSQL> column first_name format a20\\nSQL> column last_name format a20\\nSQL> column ccard_number format a20\\nSQL> select * from paymentinfo where rownum <= 10;\\n\\nTRANSACTIONID FIRST_NAME           LAST_NAME            CCARD_NUMBER\\n------------- -------------------- -------------------- --------------------\\n            1 Brigham              Raftery              3558886053942589\\n            2 Willi                Milksop              3566444037931078\\n            3 Spencer              Toy                  3578657283591918\\n            4 Eleanor              Sherlaw              201745575867996\\n            5 Matelda              Joskowicz            3567589132290124\\n            6 Hyacinth             Haville              201542493937096\\n            7 Cristi               Yakubowicz           3535186423765655\\n            8 Bess                 Atte-Stone           501891982886122702\\n            9 Madeleine            Spacy                5299626156704075\\n           10 Alick                Lilly                4041597161949423\\n\\n10 rows selected.\\n```\\n\\n## Data Redaction\\n\\nWhen working with [Oracle Application Express](https://apex.oracle.com) (APEX) or any other programming languages, we would typically use database views to query data for display, e.g. Interactive Reports. We could attempt to \\"mask\\" the data with a query something like this:\\n\\n```sql\\ncreate or replace force view vw_paymentinfo\\nas\\n  select\\n    transactionid\\n    , first_name\\n    , last_name\\n    , case\\n        when length(ccard_number) > 4 then\\n          regexp_replace(\\n            substr(ccard_number, 0, length(ccard_number)-4)\\n            , \'\\\\d\'\\n            , \'*\'\\n          )\\n          || substr(ccard_number, -4)\\n        else\\n          regexp_replace(ccard_number,\'\\\\d\',\'*\')\\n      end as ccard_number\\n  from paymentinfo\\n;\\n```\\n\\nThen query the data using the view.\\n\\n```sql\\nSQL> select * from vw_paymentinfo where rownum <= 10;\\n\\nTRANSACTIONID FIRST_NAME           LAST_NAME            CCARD_NUMBER\\n------------- -------------------- -------------------- --------------------\\n            1 Brigham              Raftery              ************2589\\n            2 Willi                Milksop              ************1078\\n            3 Spencer              Toy                  ************1918\\n            4 Eleanor              Sherlaw              ***********7996\\n            5 Matelda              Joskowicz            ************0124\\n            6 Hyacinth             Haville              ***********7096\\n            7 Cristi               Yakubowicz           ************5655\\n            8 Bess                 Atte-Stone           **************2702\\n            9 Madeleine            Spacy                ************4075\\n           10 Alick                Lilly                ************9423\\n\\n10 rows selected.\\n```\\n\\nGreat! The credit card numbers are successfully masked. However, there are some key concerns with this approach. When implementing security for applications and system, I generally subscribe to a few \\"golden rules\\".\\n\\n1. Avoid reinventing the wheel - the same is said with data encrypting methods. It\'s always preferred that we apply commonly used, well-tested and accepted algorithms. There are many credit card options. Designing a comprehensive regular expression or string replacement scheme to cover all the available number formats is not easy. Why not rely on off-the-shelf solutions from companies that have invested significantly and has a lot of experience dealing with these issues, for handling these monumental and critical tasks.\\n1. Separation of duties - if developers have no need to view the data of the applications they build, then perhaps they should not. Creating data security policies and enforcement should preferably be the responsibility of a separate person or team.\\n\\n## Oracle\'s Gift\\n\\nWhen Oracle Database 12c was released, the company restructured its Advanced Security Option offering. Secure database connections became a \\"feature\\" and a new technology, Data Redaction, was introduced and became part of the for-fee product. We, Oracle Database Developers, are very fortunate that this premium functionality is now available for free with the Oracle Database 18c XE.\\n\\nWith Data Redaction, I am relying on Oracle\'s vast experiences implementing security and compliance with well-known, well-defined security requirements. It offers predefined redaction rules that not only mask credit card numbers, but national identification numbers, phone numbers, email addresses and even IP addresses.\\n\\nRedaction policies are created at the \\"database level\\", so that means they apply regardless of how you access the data. Running a query in SQL Developer, SQLcl or through an APEX application yields the exact same results.\\n\\nAs a best practice (separation of duties), Oracle encourages customers to use a different user/role for managing redaction policies. Let\'s begin by creating a user `sec_admin` for this task. We note the following requirements for creating such a user (from the docs):\\n\\n> * To create redaction policies, you must have the EXECUTE privilege on the DBMS_REDACT PL/SQL package.\\n> * You do not need any privileges to access the underlying tables or views that will be protected by the policy.\\n\\nLet\'s do that. As `SYS` user, execute the following commands:\\n\\n```sql\\ncreate user sec_admin identified by \\"sec_admin\\";\\ngrant create session to sec_admin;\\ngrant execute on dbms_redact to sec_admin;\\n```\\n\\nNote that the user `sec_admin`, would have rights to create and enforce data redaction policies, but will not have access to the data it is tasked to protect.\\n\\nNext, [create a data redaction policy](https://docs.oracle.com/en/database/oracle/oracle-database/18/asoag/configuring-oracle-data-redaction-policies.html#GUID-D32FC6CA-A3B5-47EE-B288-52F1124F9FBD) for masking the data in column `ccard_number`.\\n\\n```sql\\nbegin\\n  dbms_redact.add_policy(\\n    object_schema => \'secure_user\'\\n    , object_name => \'paymentinfo\'\\n    , policy_name => \'ccardnum_mask\'\\n    , column_name => \'ccard_number\'\\n    , function_type => dbms_redact.regexp\\n    , function_parameters => null\\n    , regexp_pattern => dbms_redact.re_pattern_ccn\\n    , regexp_replace_string => dbms_redact.re_redact_ccn\\n    , regexp_position => dbms_redact.re_beginning\\n    , regexp_occurrence => dbms_redact.re_first\\n    , regexp_match_parameter => dbms_redact.re_case_insensitive\\n    , expression => \'1=1\'\\n  );\\nend;\\n/\\n```\\n\\nRun the query again.\\n\\n```sql\\nSQL> select * from paymentinfo where rownum <= 10;\\n\\nTRANSACTIONID FIRST_NAME           LAST_NAME            CCARD_NUMBER\\n------------- -------------------- -------------------- --------------------\\n            1 Brigham              Raftery              ************2589\\n            2 Willi                Milksop              ************1078\\n            3 Spencer              Toy                  ************1918\\n            4 Eleanor              Sherlaw              ************7996\\n            5 Matelda              Joskowicz            ************0124\\n            6 Hyacinth             Haville              ************7096\\n            7 Cristi               Yakubowicz           ************5655\\n            8 Bess                 Atte-Stone           ************2702\\n            9 Madeleine            Spacy                ************4075\\n           10 Alick                Lilly                ************9423\\n\\n10 rows selected.\\n```\\n\\n## Applying Data Redaction in APEX\\n\\nData Redaction policies are enforced regardless of how the data is accessed. In the previous examples, data was queried using SQLcl as the schema owner. Let\'s try accessing the data using a simple Classic Report that runs the same query against the table `paymentinfo`. Below is a short screen recording of data redaction in action.\\n\\n<youtube youTubeId=\\"2m3aqqhZn8o\\" />\\n\\n<span class=\\"caption\\">APEX Classic Report with Data Redaction Policy Enforced</span>\\n\\nNotice that the data is masked regardless of who accesses the application as the expresion condition is `1=1`.\\n\\n## Selectively Applying Policies\\n\\nIn this next and final section, we will use [Expressions](https://docs.oracle.com/en/database/oracle/oracle-database/18/asoag/configuring-oracle-data-redaction-policies.html#GUID-CF59A02C-1DE8-439B-BC1C-7235915C6D23) to determine if the data should or should not be masked, depending on the user\'s role in the application.\\n\\nWe begin by adding an Application Item the should be restricted and may not be set by the browser. Set the application state using a Post-Authentication procedure in the application\'s current Authentication Scheme.\\n\\nNext, update the `expression` attribute for the `ccardnum_mask` policy to use the Application Item\'s session state.\\n\\n```sql\\nbegin\\n  dbms_redact.alter_policy(\\n    object_schema => \'secure_user\'\\n    , object_name => \'paymentinfo\'\\n    , policy_name => \'ccardnum_mask\'\\n    , column_name => \'ccard_number\'\\n    , action => dbms_redact.modify_expression\\n    , expression => \'v(\'\'APP_ROLE\'\') != \'\'Administrator\'\'\'\\n  );\\nend;\\n/\\n```\\n\\nWatch the screen recording of how the results of the Classic Report now differs between a User and an Administrator.\\n\\n<youtube youTubeId=\\"yeYrZ4092uQ\\" />\\n\\n<span class=\\"caption\\">Using an Expression for Determining When Data Redaction Policy is Applied</span>\\n\\nEven with this approach, a potential loophole remains. A malicious actor (developer) could alter the session state of this item or how it is set, and gain access to the data unprotected. Think about combining data redaction (using `SYS_XS_CONTEXT` in the expression) with Oracle\'s [Real Application Security](https://www.oracle.com/database/technologies/security/virtual-private-db.html) (RAS) and you\'ll have a near air-tight solution. RAS incidentally, is also available to use in XE, and if you\'d like to learn more, check out Dimitri Gielis\' [blog post](http://dgielis.blogspot.com/2015/04/enable-real-application-security-ras-in.html) on enabling RAS in APEX.\\n\\n## Closing Remarks\\n\\nTo remove a policy, run the following procedure and specify the object\'s schema, name and the policy name to drop.\\n\\n```sql\\nbegin\\n  dbms_redact.drop_policy(\\n    object_schema => \'secure_user\'\\n    , object_name => \'paymentinfo\'\\n    , policy_name => \'ccardnum_mask\'\\n  );\\nend;\\n/\\n```\\n\\nFinally, through this article, I hope I have given you a preview of what the technology is capable of, and have sparked some interest There\'s much to know and learn about Oracle Data Redaction, the full spectrum of what it offers and its limitations. You are highly encouraged to read the [official documentation](https://docs.oracle.com/en/database/oracle/oracle-database/18/asoag/asopart2.html), test out various scenarios and consider using this technology, especially if you are a custodian of confidential data."},{"id":"id/1722","metadata":{"permalink":"/content/id/1722","source":"@site/posts/preview-pdf-uploads-in-an-apex-app-cross-post/index.md","title":"Preview PDF Uploads in an APEX App (Cross-post)","description":"1280 720 Photo by Kaleidico","date":"2018-10-27T12:13:40.000Z","formattedDate":"October 27, 2018","tags":[{"label":"orclapex","permalink":"/content/tags/orclapex"},{"label":"pdf","permalink":"/content/tags/pdf"},{"label":"preview","permalink":"/content/tags/preview"},{"label":"viewer","permalink":"/content/tags/viewer"},{"label":"demo","permalink":"/content/tags/demo"}],"readingTime":2.375,"hasTruncateMarker":true,"authors":[{"name":"Adrian Png","title":"Senior Cloud Solutions Architect @ Insum","url":"https://github.com/fuzziebrain","imageURL":"https://github.com/fuzziebrain.png","key":"fuzziebrain"}],"frontMatter":{"title":"Preview PDF Uploads in an APEX App (Cross-post)","tags":["orclapex","pdf","preview","viewer","demo"],"categories":["Technology","Programming","Oracle Application Express"],"slug":"id/1722","authors":["fuzziebrain"],"date":"2018-10-27T12:13:40.000Z"},"prevItem":{"title":"For Your Eyes Only - Redact to Protect","permalink":"/content/id/1723"},"nextItem":{"title":"Protect Your Data with Transparent Data Encryption","permalink":"/content/id/1721"}},"content":"![1280 720 Photo by Kaleidico](./assets/kaleidico-754509-unsplash_1280x720.jpg)\\n\\nCame across an interesting [question](https://stackoverflow.com/questions/53003083/oracle-apex-pdf-viewer) on {% fab stack-overflow %} stack**overflow** this morning, so I thought I\'d give it a shot. Turned out to be an exciting morning of learning and a pretty lengthy response that I thought I\'d cross-post to keep a copy for myself. If you have any suggestions for improvement, please feel free to contribute to the thread. Thanks!\\n\\n\x3c!-- truncate --\x3e\\n\\n## Demo Application\\n\\n**Link:** https://apex.oracle.com/pls/apex/f?p=34781\\n\\n> Username: demo\\n> Password: demo\\n\\n## The Recipe\\n\\nThis uses the [PDF.js](https://mozilla.github.io/pdf.js/) project by Mozilla. Here\'s a quick recipe of what you may need.\\n\\n1. Create a File Browse page item and set the Storage Type to `Table APEX_APPLICATION_TEMP_FILES`.\\n2. Create a page button to submit the page.\\n3. Create a Classic Report region and enter the following query:\\n\\n```sql\\nselect\\n    id\\n    , filename\\nfrom apex_application_temp_files\\nwhere application_id = :APP_ID\\n```\\n\\n4. Add a virtual column and set the HTML Expression:\\n\\n```html\\n        <button type=\\"button\\" class=\\"btn-preview-pdf\\" data-id=\\"#ID#\\">Preview</button>\\n```\\n\\n5. Create a region and enter the following in the Source:\\n\\n```html\\n<canvas id=\\"preview-pane\\"></canvas>\\n```\\n\\n6. Create a Click dynamic action.\\n    a. Set the selection Type to `jQuery Selector`.\\n    b. Enter the jQuery Selector `.btn-preview-pdf`.\\n\\n7. Add a `Execute JavaScript Code` action with the following JS code (check out the [examples](https://mozilla.github.io/pdf.js/examples/) from the PDF.js website for more details on what the code does):\\n\\n```javascript\\nvar fileId = $(this.triggeringElement).data(\'id\');\\nvar docUrl = \'f?p=&APP_ID.:0:&APP_SESSION.:APPLICATION_PROCESS=DOWNLOADPDF:::FILE_ID:\' + fileId;\\nvar previewPane = this.affectedElements[0];\\n\\n// from PDF.js examples\\npdfjsLib.getDocument(docUrl).then(function(pdf) {\\n    var pageNumber = 1;\\n    pdf.getPage(pageNumber).then(function(page) {\\n        console.log(\'Page loaded\');\\n\\n        var scale = 1.5;\\n        var viewport = page.getViewport(scale);\\n\\n        // Prepare canvas using PDF page dimensions\\n        var canvas = previewPane;\\n        var context = canvas.getContext(\'2d\');\\n        canvas.height = viewport.height;\\n        canvas.width = viewport.width;\\n\\n        // Render PDF page into canvas context\\n        var renderContext = {\\n            canvasContext: context,\\n            viewport: viewport\\n        };\\n        var renderTask = page.render(renderContext);\\n        renderTask.then(function () {\\n            console.log(\'Page rendered\');\\n        });\\n    })\\n}, function(reason) {\\n    console.error(reason);\\n});\\n```\\n\\n8. For the action, also set the Affected Elements:\\n  a. Selection Type: `jQuery Selector`\\n  b. jQuery Selector: `#preview-pane`\\n\\n9. Follow Joel Kallman\'s [post](https://joelkallman.blogspot.com/2014/03/yet-another-post-how-to-link-to.html) on creating a link to download a file. You will need an Application Process (`DOWNLOADPDF`) and an Application Item (`FILE_ID`) The modified code for the Application Process `DOWNLOADPDF` looks like this:\\n\\n```\\nbegin\\n    for file in (select *\\n                from apex_application_temp_files\\n                where id = :FILE_ID) loop\\n        --\\n        sys.htp.init;\\n        sys.owa_util.mime_header( file.mime_type, FALSE );\\n        sys.htp.p(\'Content-length: \' || sys.dbms_lob.getlength( file.blob_content));\\n        sys.htp.p(\'Content-Disposition: attachment; filename=\\"\' || file.filename || \'\\"\' );\\n        sys.htp.p(\'Cache-Control: max-age=3600\');\\n        sys.owa_util.http_header_close;\\n        sys.wpg_docload.download_file( file.blob_content );\\n\\n        apex_application.stop_apex_engine;\\n    end loop;\\nend;\\n```\\n\\n10. Almost missed this out. On the Page Attributes, set the JavaScript File URLs to any of the CDNs [listed](https://mozilla.github.io/pdf.js/getting_started/). For example:\\n\\n```html\\n//cdnjs.cloudflare.com/ajax/libs/pdf.js/2.0.550/pdf.min.js\\n```\\n\\n**Note** that this is a very basic prototype. The preview only allows you to view the first page. You will need to figure out the [API](https://mozilla.github.io/pdf.js/api/draft/index.html) and then do the necessary to allow multipage viewing."},{"id":"id/1721","metadata":{"permalink":"/content/id/1721","source":"@site/posts/protect-your-data-with-transparent-data-encryption/index.md","title":"Protect Your Data with Transparent Data Encryption","description":"1280 720 Photo by TheDigitalArtist","date":"2018-10-27T01:31:06.000Z","formattedDate":"October 27, 2018","tags":[{"label":"oracle database","permalink":"/content/tags/oracle-database"},{"label":"18cXE","permalink":"/content/tags/18-c-xe"},{"label":"orclapex","permalink":"/content/tags/orclapex"},{"label":"tde","permalink":"/content/tags/tde"},{"label":"tls","permalink":"/content/tags/tls"},{"label":"security","permalink":"/content/tags/security"},{"label":"pki","permalink":"/content/tags/pki"}],"readingTime":5.115,"hasTruncateMarker":true,"authors":[{"name":"Adrian Png","title":"Senior Cloud Solutions Architect @ Insum","url":"https://github.com/fuzziebrain","imageURL":"https://github.com/fuzziebrain.png","key":"fuzziebrain"}],"frontMatter":{"title":"Protect Your Data with Transparent Data Encryption","tags":["oracle database","18cXE","orclapex","tde","tls","security","pki"],"categories":["Technology","Programming","Oracle Application Express"],"slug":"id/1721","authors":["fuzziebrain"],"date":"2018-10-27T01:31:06.000Z"},"prevItem":{"title":"Preview PDF Uploads in an APEX App (Cross-post)","permalink":"/content/id/1722"},"nextItem":{"title":"We Have a Wallet","permalink":"/content/id/1720"}},"content":"![1280 720 Photo by TheDigitalArtist](./assets/hacker-2002907_1920_1280x720.jpg)\\n\\n[Transparent Data Encryption](https://docs.oracle.com/en/database/oracle/oracle-database/18/asoag/introduction-to-transparent-data-encryption.html#GUID-769EC29B-0107-40FE-9A9D-BF81A4BBD0E9) (TDE) is a powerful database feature that allows developers and administrators to very quickly, persist data encrypted at-rest. Applications do not require explicit coding to encrypt data for storage. TDE manages encryption during storage, and decryption when data is read, transparently and automatically.\\n\\nTDE can also be used during backups. In the typical \\"*tape falls off the back of the truck*\\" scenario, malicious actors would not be able to decipher the data on the lost tapes without the crucial encryption key stored in the Oracle Wallet. Assuming of course, the wallet was not backed up to the same tape.\\n\\nThis enterprise-grade feature is found in the [Oracle Advanced Security](https://www.oracle.com/database/technologies/security/advanced-security.html) Database Option. That requires user to be licensed for database enterprise edition. However, with [Oracle Database 18c Express Edition](https://oracle.com/xe) (18cXE), Oracle has made it free and accessible by everyone!\\n\\nThis post was specially written for [Oracle Application Express](https://apex.oracle.com) (APEX) developers who are new to this technology. It provides a simple workflow for creating an encrypted tablespaces. Since APEX application live on the database, placing the parsing schema on a TDE-enabled tablespace, automatically protects the entire application. Hopefully this will encourage you to use it for protecting [Personally Identifiable Information](https://wikipedia.org/wiki/Personally_identifiable_information) (PII) or any other sensitive and confidential data.\\n\\n\x3c!-- truncate --\x3e\\n\\nIf you are trying out 18cXE using the [Docker image](https://github.com/fuzziebrain/docker-oracle-xe) [Martin D\'Souza](https://twitter.com/martindsouza) and I worked on, access the database container using the following command:\\n\\n```bash\\n$ docker exec -it -u oracle oracledb bash\\n```\\n\\nTypically, the wallets would be located (`WALLET_ROOT`) in `$ORACLE_BASE/admin/$ORACLE_SID/wallet`. However, when running the DB as a Docker container, developers are encouraged to map `/opt/oracle/oradata` to a directory on the host machine. That way, the container can be destroyed without losing your data files, configuration files and now Oracle Wallet(s).\\n\\nAs the `oracle` user, create the directory `/opt/oracle/oradata/wallets`\\n\\n```bash\\nmkdir -p /opt/oracle/oradata/wallets\\n```\\n\\nLogin to the database as `SYSDBA`:\\n\\n```bash\\n$ . oraenv\\nORACLE_SID = [oracle] ? XE\\nThe Oracle base has been set to /opt/oracle\\n$ sqlplus / as sysdba\\n```\\n\\nCheck that `WALLET_ROOT` hasn\'t already been set:\\n\\n```sql\\nSQL> show parameter wallet_root\\n\\nNAME\\t\\t\\t\\t     TYPE\\t VALUE\\n------------------------------------ ----------- ------------------------------\\nwallet_root\\t\\t\\t     string\\n```\\n\\nSet the `wallet_root` location, then restart the database:\\n\\n```sql\\nSQL> alter system set wallet_root=\'/opt/oracle/oradata/wallets\' scope=spfile;\\n\\nSystem altered.\\n\\nSQL> shutdown immediate;\\nDatabase closed.\\nDatabase dismounted.\\nORACLE instance shut down.\\nSQL> startup\\nORACLE instance started.\\n\\nTotal System Global Area 1610609928 bytes\\nFixed Size\\t\\t    8897800 bytes\\nVariable Size\\t\\t  805306368 bytes\\nDatabase Buffers\\t  788529152 bytes\\nRedo Buffers\\t\\t    7876608 bytes\\nDatabase mounted.\\nDatabase opened.\\n```\\n\\nCheck that the `TDE_CONFIGURATION` hasn\'t already been set:\\n\\n```sql\\nSQL> show parameter tde_configuration;\\n\\nNAME\\t\\t\\t\\t     TYPE\\t VALUE\\n------------------------------------ ----------- ------------------------------\\ntde_configuration\\t\\t     string\\n```\\n\\nSet the `TDE_CONFIGURATION`:\\n\\n```sql\\nSQL> alter system set tde_configuration=\\"keystore_configuration=file\\" scope=both;\\n\\nSystem altered.\\n```\\nCreate a wallet in `/opt/oracle/oradata/wallets/tde`:\\n\\n```sql\\nSQL> administer key management create keystore identified by \\"superSecret111\\";\\n\\nkeystore altered.\\n```\\n\\nOpen the keystore for `CDB$ROOT` and `XEPDB1` containers:\\n\\n```sql\\nSQL> administer key management set keystore open identified by \\"superSecret111\\" container = all;\\n\\nkeystore altered.\\n```\\nSet the master key for `CDB$ROOT` and `XEPDB1` containers:\\n\\n```sql\\nSQL> administer key management set key identified by \\"superSecret111\\" with backup container = all;\\n\\nkeystore altered.\\n```\\n\\nCheck that the keystores (except the seed container) are all open and master keys set:\\n\\n```sql\\nSQL> column name format a30\\nSQL> select c.name, w.status\\n  2  from v$encryption_wallet w, v$containers c\\n  3  where c.con_id = w.con_id;\\n\\nNAME\\t\\t\\t       STATUS\\n------------------------------ ------------------------------\\nCDB$ROOT\\t\\t       OPEN\\nPDB$SEED\\t\\t       CLOSED\\nXEPDB1\\t\\t\\t       OPEN\\n```\\n\\nIn the PDB, create a tablespace, a user with the encrypted tablespace as default and grant the necessary roles. Supported encryption algorithms are listed [here](https://docs.oracle.com/en/database/oracle/oracle-database/18/asoag/introduction-to-transparent-data-encryption.html#GUID-AAE3C965-C6A8-45D6-BA5D-F88B300A301A). Below is a sample script for creating an encrypted tablespace:\\n\\n```sql\\ncreate tablespace secure_ts\\n  datafile \'/opt/oracle/oradata/XE/XEPDB1/secure_ts01.dbf\'\\n  size 10M autoextend on next 5M\\n  encryption using \'AES256\' encrypt\\n;\\n\\ncreate user secure_user identified by secure_user\\n  default tablespace secure_ts\\n  quota unlimited on secure_ts\\n;\\n\\ngrant connect to secure_user;\\ngrant resource to secure_user;\\n```\\n\\nLogin to the PDB as the user `secure_user` and create simple database objects and insert some records, for example:\\n\\n```sql\\ncreate table sample(id number);\\n\\ninsert into sample values (1);\\n\\ncommit;\\n\\nselect * from sample;\\n```\\n\\nRestart the database, reconnect as `secure_user` and query the table again.\\n\\n```sql\\nSQL> alter session set container = xepdb1;\\n\\nSession altered.\\n\\nSQL> select * from secure_user.sample;\\nselect * from secure_user.sample\\n                          *\\nERROR at line 1:\\nORA-28365: wallet is not open\\n```\\n\\nA quick check on the wallet status should explain why:\\n\\n```sql\\nSQL> alter session set container = cdb$root;\\n\\nSession altered.\\n\\nSQL> select c.name, w.status\\n  2  from v$encryption_wallet w, v$containers c\\n  3  where c.con_id = w.con_id;\\n\\nNAME\\t\\t\\t       STATUS\\n------------------------------ ------------------------------\\nCDB$ROOT\\t\\t       CLOSED\\nPDB$SEED\\t\\t       CLOSED\\nXEPDB1\\t\\t\\t       CLOSED\\n```\\n\\nThe wallet is no longer open after the database restarted. We will need to make this happen automatically. We do that by converting the keystore into an `auto_login` keystore:\\n\\n```sql\\nSQL> administer key management create auto_login keystore from keystore \'/opt/oracle/oradata/wallets/tde\' identified by \\"superSecret111\\";\\n\\nkeystore altered;\\n```\\n\\n**IMPORTANT NOTES**\\n\\n* There are many [security considerations](https://docs.oracle.com/en/database/oracle/oracle-database/18/asoag/general-considerations-of-using-transparent-data-encryption.html#GUID-ED481093-51B0-42F2-BADB-E5E55889AD47) that require thought before roling out TDE.\\n    * One of them is about keeping the keys safe.In this post, the keystore was made to `auto_login`, which really isn\'t as safe as insisting, with the `local` keyword, that the wallet only opens on one host. I did not use this in the example as it is intended to work with Docker. Making the wallet set to allow automatic wallet opening could cause data loss if the container is dropped then restored.\\n    * Hardware-based key management systems are preferred and safer. They should be considered if budget and resource permits.\\n* The keystore and TDE master encryption keys are managed in United Mode. I had intended to use the Isolated Mode, but apparently, that might not be supported in XE.\\n\\n```sql\\nSQL> alter system set tde_configuration=\\"keystore_configuration=file\\" scope=both;\\nalter system set tde_configuration=\\"keystore_configuration=file\\" scope=both\\n*\\nERROR at line 1:\\nORA-32017: failure in updating SPFILE\\nORA-12754: Feature \'Per-PDB TDE keystore\' is disabled due to missing capability\\n\'Runtime Environment\'.\\n```\\n\\n* Last but not least, keep your wallets saved/safe in a separate secure location. **DO NOT** loose them. Failing to protect them will cause irreversible, catastrophic loss of data.\\n\\nEnjoy Oracle Advanced Security!"},{"id":"id/1720","metadata":{"permalink":"/content/id/1720","source":"@site/posts/we-have-a-wallet/index.md","title":"We Have a Wallet","description":"1280 720 Photo by Pexels","date":"2018-10-23T22:17:22.000Z","formattedDate":"October 23, 2018","tags":[{"label":"oracle database","permalink":"/content/tags/oracle-database"},{"label":"18cXE","permalink":"/content/tags/18-c-xe"},{"label":"orclapex","permalink":"/content/tags/orclapex"},{"label":"ssl","permalink":"/content/tags/ssl"},{"label":"tls","permalink":"/content/tags/tls"},{"label":"security","permalink":"/content/tags/security"},{"label":"https","permalink":"/content/tags/https"},{"label":"pki","permalink":"/content/tags/pki"}],"readingTime":5.185,"hasTruncateMarker":true,"authors":[{"name":"Adrian Png","title":"Senior Cloud Solutions Architect @ Insum","url":"https://github.com/fuzziebrain","imageURL":"https://github.com/fuzziebrain.png","key":"fuzziebrain"}],"frontMatter":{"title":"We Have a Wallet","tags":["oracle database","18cXE","orclapex","ssl","tls","security","https","pki"],"categories":["Technology","Programming","Oracle Application Express"],"slug":"id/1720","authors":["fuzziebrain"],"date":"2018-10-23T22:17:22.000Z"},"prevItem":{"title":"Protect Your Data with Transparent Data Encryption","permalink":"/content/id/1721"},"nextItem":{"title":"Sweet 18 - A New Oracle XE Release","permalink":"/content/id/1719"}},"content":"![1280 720 Photo by Pexels](./assets/wood-1284077_1920_1280x720.jpg)\\n\\n> **Updated November 17, 2018** \\n> Please read this follow-up [post](/content/id/1725/) as well. It contains an important note about intermediate certificates and where to download the root certificates.\\n\\nOne of the difficulties working with web services in the previous version of Oracle Database Express Edition (XE) was the lack of a usable Oracle Wallet. This was fast becoming a huge problems for developers, as many API providers started enforcing requirements to that clients accessed services through secured channels. When working with 11g XE, I often relied on proxies within a sandbox to mask the need for SSL/TLS. I discussed this somewhat at length in a previous [blog post](/content/id/1711/).\\n\\nWith the 18c release, Oracle has opened up and provided us developers the opportunities to make our apps safer. We now have access to the ([`orapki`](https://docs.oracle.com/en/database/oracle/oracle-database/18/dbseg/using-the-orapki-utility-to-manage-pki-elements.html#GUID-83EF7865-373A-4C39-8A64-44A73EB87759)) tools needed to manage an Oracle Wallet. I don\'t do this a lot, so I keep a snippets of what I do to make this easy.\\n\\n\x3c!-- truncate --\x3e\\n\\nI had to revisit this procedure while configuring [Oracle Application Express](https://apex.oracle.com) (APEX) to support email sending using the [`APEX_MAIL`](https://docs.oracle.com/database/apex-18.2/AEAPI/APEX_MAIL.htm#AEAPI341) package. The target SMTP server is Google\'s `smtp.google.com`, and because we want to secure the user credentials during authentication, we have to use TLS. We will use this for context.\\n\\n## Obtain Root and Intermediate Certificates\\n\\nTo obtain the certificates for Google\'s SMTP servers, execute the following command:\\n\\n```txt\\n$ openssl s_client -starttls smtp -connect smtp.gmail.com:587 -showcerts\\nCONNECTED(000001B4)\\n---\\nCertificate chain\\n 0 s:/C=US/ST=California/L=Mountain View/O=Google LLC/CN=smtp.gmail.com\\n   i:/C=US/O=Google Trust Services/CN=Google Internet Authority G3\\n-----BEGIN CERTIFICATE-----\\nMIIEgjCCA2qgAwIBAgIIYYzIf9CCTNQwDQYJKoZIhvcNAQELBQAwVDELMAkGA1UE\\nBhMCVVMxHjAcBgNVBAoTFUdvb2dsZSBUcnVzdCBTZXJ2aWNlczElMCMGA1UEAxMc\\nR29vZ2xlIEludGVybmV0IEF1dGhvcml0eSBHMzAeFw0xODEwMDkxMzA4MDBaFw0x\\nOTAxMDExMzA4MDBaMGgxCzAJBgNVBAYTAlVTMRMwEQYDVQQIDApDYWxpZm9ybmlh\\nMRYwFAYDVQQHDA1Nb3VudGFpbiBWaWV3MRMwEQYDVQQKDApHb29nbGUgTExDMRcw\\nFQYDVQQDDA5zbXRwLmdtYWlsLmNvbTCCASIwDQYJKoZIhvcNAQEBBQADggEPADCC\\nAQoCggEBAObRl1CBmcUdG0DKuFBKZUrtiI9LEjkWdLkCTG5GZN2dmmyoVF2Um+3Q\\n8GkxPSp4cNE/pV/+eKpX0swjsX7OSr+H75tG3SwPAwgsxKK/hciP5+UZn3hNxIGe\\nvkNZUIIiZcLpSmUm2C13AVYO+RrSQoswy72Pqq9L5MOin40RJq2zF1MCqA8AG3S6\\nc5uNevdHhRAfF9PuFc+FS+KRfMIOKZ1mnE6iMR9rBV4n3I5muVb0F24NlwtnQfKq\\n8KvUBIZzgZsoj9ED7WeTunTny1n70ElE9AgrOx7+YOpx8UiBsN1uMhszzb+l+Fck\\nY6sJl4bjb7HfnYBKbihqauFcxGbsocECAwEAAaOCAUIwggE+MBMGA1UdJQQMMAoG\\nCCsGAQUFBwMBMBkGA1UdEQQSMBCCDnNtdHAuZ21haWwuY29tMGgGCCsGAQUFBwEB\\nBFwwWjAtBggrBgEFBQcwAoYhaHR0cDovL3BraS5nb29nL2dzcjIvR1RTR0lBRzMu\\nY3J0MCkGCCsGAQUFBzABhh1odHRwOi8vb2NzcC5wa2kuZ29vZy9HVFNHSUFHMzAd\\nBgNVHQ4EFgQUHPvi7USz6x/8/XVkgFyMk9ndjJAwDAYDVR0TAQH/BAIwADAfBgNV\\nHSMEGDAWgBR3wrhQmmd2drEtwobQg6B+pn66SzAhBgNVHSAEGjAYMAwGCisGAQQB\\n1nkCBQMwCAYGZ4EMAQICMDEGA1UdHwQqMCgwJqAkoCKGIGh0dHA6Ly9jcmwucGtp\\nLmdvb2cvR1RTR0lBRzMuY3JsMA0GCSqGSIb3DQEBCwUAA4IBAQC+CB4l2nvFPQck\\n8aJZRZ8vvT1G3gs5BGIPnpB3wIUEp6Hg4xHuPt3CAGktttMEu/wGJABxzfIr4RFo\\nqgafq3cg7/w5R9+4kaDX0YcNNdgvC6/8UXlHHTjEsNgJiGFDJGRzttHmGGhUvsxg\\nMwCihV1N0E5YZYIMfxVFHmbLTZe0tI4Ma0n+hxxbXSlSCOH4fjbaZ+sKX0RbWTDx\\nwweZxQM9JIn7Lwz4Fe34e+YfpyNqOow6FdvaOf5J3dEmqHoS5rXJ+dp8jTSTld5L\\nwn+ganMirE5mLNJNh9AvfIVtz4sHw1Wg5v+TkvWZ6t5J1oJY3UJPSfJrPQy6fssu\\n9rs+s4BX\\n-----END CERTIFICATE-----\\n 1 s:/C=US/O=Google Trust Services/CN=Google Internet Authority G3\\n   i:/OU=GlobalSign Root CA - R2/O=GlobalSign/CN=GlobalSign\\n-----BEGIN CERTIFICATE-----\\nMIIEXDCCA0SgAwIBAgINAeOpMBz8cgY4P5pTHTANBgkqhkiG9w0BAQsFADBMMSAw\\nHgYDVQQLExdHbG9iYWxTaWduIFJvb3QgQ0EgLSBSMjETMBEGA1UEChMKR2xvYmFs\\nU2lnbjETMBEGA1UEAxMKR2xvYmFsU2lnbjAeFw0xNzA2MTUwMDAwNDJaFw0yMTEy\\nMTUwMDAwNDJaMFQxCzAJBgNVBAYTAlVTMR4wHAYDVQQKExVHb29nbGUgVHJ1c3Qg\\nU2VydmljZXMxJTAjBgNVBAMTHEdvb2dsZSBJbnRlcm5ldCBBdXRob3JpdHkgRzMw\\nggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQDKUkvqHv/OJGuo2nIYaNVW\\nXQ5IWi01CXZaz6TIHLGp/lOJ+600/4hbn7vn6AAB3DVzdQOts7G5pH0rJnnOFUAK\\n71G4nzKMfHCGUksW/mona+Y2emJQ2N+aicwJKetPKRSIgAuPOB6Aahh8Hb2XO3h9\\nRUk2T0HNouB2VzxoMXlkyW7XUR5mw6JkLHnA52XDVoRTWkNty5oCINLvGmnRsJ1z\\nouAqYGVQMc/7sy+/EYhALrVJEA8KbtyX+r8snwU5C1hUrwaW6MWOARa8qBpNQcWT\\nkaIeoYvy/sGIJEmjR0vFEwHdp1cSaWIr6/4g72n7OqXwfinu7ZYW97EfoOSQJeAz\\nAgMBAAGjggEzMIIBLzAOBgNVHQ8BAf8EBAMCAYYwHQYDVR0lBBYwFAYIKwYBBQUH\\nAwEGCCsGAQUFBwMCMBIGA1UdEwEB/wQIMAYBAf8CAQAwHQYDVR0OBBYEFHfCuFCa\\nZ3Z2sS3ChtCDoH6mfrpLMB8GA1UdIwQYMBaAFJviB1dnHB7AagbeWbSaLd/cGYYu\\nMDUGCCsGAQUFBwEBBCkwJzAlBggrBgEFBQcwAYYZaHR0cDovL29jc3AucGtpLmdv\\nb2cvZ3NyMjAyBgNVHR8EKzApMCegJaAjhiFodHRwOi8vY3JsLnBraS5nb29nL2dz\\ncjIvZ3NyMi5jcmwwPwYDVR0gBDgwNjA0BgZngQwBAgIwKjAoBggrBgEFBQcCARYc\\naHR0cHM6Ly9wa2kuZ29vZy9yZXBvc2l0b3J5LzANBgkqhkiG9w0BAQsFAAOCAQEA\\nHLeJluRT7bvs26gyAZ8so81trUISd7O45skDUmAge1cnxhG1P2cNmSxbWsoiCt2e\\nux9LSD+PAj2LIYRFHW31/6xoic1k4tbWXkDCjir37xTTNqRAMPUyFRWSdvt+nlPq\\nwnb8Oa2I/maSJukcxDjNSfpDh/Bd1lZNgdd/8cLdsE3+wypufJ9uXO1iQpnh9zbu\\nFIwsIONGl1p3A8CgxkqI/UAih3JaGOqcpcdaCIzkBaR9uYQ1X4k2Vg5APRLouzVy\\n7a8IVk6wuy6pm+T7HT4LY8ibS5FEZlfAFLSW8NwsVz9SBK2Vqn1N0PIMn5xA6NZV\\nc7o835DLAFshEWfC7TIe3g==\\n-----END CERTIFICATE-----\\n---\\nServer certificate\\nsubject=/C=US/ST=California/L=Mountain View/O=Google LLC/CN=smtp.gmail.com\\nissuer=/C=US/O=Google Trust Services/CN=Google Internet Authority G3\\n---\\nNo client certificate CA names sent\\nPeer signing digest: SHA256\\nServer Temp Key: ECDH, P-256, 256 bits\\n---\\nSSL handshake has read 3240 bytes and written 469 bytes\\n---\\nNew, TLSv1/SSLv3, Cipher is ECDHE-RSA-AES128-GCM-SHA256\\nServer public key is 2048 bit\\nSecure Renegotiation IS supported\\nCompression: NONE\\nExpansion: NONE\\nNo ALPN negotiated\\nSSL-Session:\\n    Protocol  : TLSv1.2\\n    Cipher    : ECDHE-RSA-AES128-GCM-SHA256\\n    Session-ID: 5DF77BA747412BE89EB308BEAFE46CB25A88516376424ED100CA56C2BDD34225\\n    Session-ID-ctx:\\n    Master-Key: 565E04E20E6D8200A4FECD2A5F09ED4549EA2FA982350A598B6A9D4EDA2B9FD26F434F66475CA6524B8EF469A1A0A563\\n    Key-Arg   : None\\n    PSK identity: None\\n    PSK identity hint: None\\n    SRP username: None\\n    TLS session ticket lifetime hint: 100800 (seconds)\\n    TLS session ticket:\\n    0000 - 00 85 11 42 e7 58 60 f5-50 28 ca bc 82 57 ec 48   ...B.X`.P(...W.H\\n    0010 - 68 64 84 5a 2b ca 65 e3-2f 92 2a 46 16 43 f3 7b   hd.Z+.e./.*F.C.{\\n    0020 - 13 e2 99 52 c2 8f bb 9a-de d1 89 3e 22 de 5a 60   ...R.......>\\".Z`\\n    0030 - 90 fc 24 8c d6 4f f6 40-d0 f4 e7 2c bc f9 46 e7   ..$..O.@...,..F.\\n    0040 - 7f 7b a1 96 0f da da 6e-14 cd 32 2a 98 f1 97 e6   .{.....n..2*....\\n    0050 - 09 06 9f 59 d5 ef 93 a3-e9 7f c7 b6 88 64 b0 1d   ...Y.........d..\\n    0060 - 98 93 06 a9 01 57 9b b7-fc 57 14 45 0a cb 3a 6a   .....W...W.E..:j\\n    0070 - 39 4e 80 43 d7 04 e9 16-62 61 d5 ef 36 7b 75 b4   9N.C....ba..6{u.\\n    0080 - 0a db bb 0b 2d 58 bd 32-7e 15 e7 20 b6 61 9d 39   ....-X.2~.. .a.9\\n    0090 - 2d 74 b1 e8 67 da f1 3d-40 2d 62 e3 b2 ab 51 3e   -t..g..=@-b...Q>\\n    00a0 - 56 0b ec 51 90 df d4 90-54 76 a2 5b b0 e3 3c 95   V..Q....Tv.[..<.\\n    00b0 - 1b af 75 d5 70 10 c7 e9-e1 e0 b4 1d 8c ea 7a 85   ..u.p.........z.\\n    00c0 - e5 89 f8 81 81 02 22 99-b7 a9 2f 80 e3 37 d3 c6   ......\\".../..7..\\n    00d0 - 88 be 7d 33 b7                                    ..}3.\\n\\n    Start Time: 1540348953\\n    Timeout   : 300 (sec)\\n    Verify return code: 0 (ok)\\n---\\n```\\n\\nThe first certificate in the chain (lines 7 to 33) belongs to the server you intend to connect to. You do/should **NOT** put this in the wallet. The remaining entries are the intermediate(s) and root certificate. These are needed. In this case, there is only an intermediate certificate. Take the content from lines 36 to 61 and save it in a file named `/home/oracle/certs/Google-Internet-Authority-G3.pem`.\\n\\n> For regular websites over SSL/TLS, you would use this instead:\\n> ```bash\\n$ openssl s_client -connect google.com:443 -showcerts\\n```\\n\\nThe application `openssl` is commonly found in Linux distributions and probably in the MacOS as well. For Windows users, use Git Bash.\\n\\n## Create the Oracle Wallet\\n\\nDo this only if you have not already done so. Create the Oracle Wallet using the following command:\\n\\n```bash\\n$ orapki wallet create -wallet /home/oracle/wallets/mywallet \\\\\\n  -pwd averysecurepassword\\n```\\n\\n## Add Certificates\\n\\nNow add the certificate using the following command against every certificate that was retrieved in the previous step.\\n\\n```bash\\n$ orapki wallet add -wallet /home/oracle/wallets/mywallet \\\\\\n  -trusted_cert -cert /home/oracle/certs/Google-Internet-Authority-G3.pem\\n```\\n\\n## Don\'t Leave Home Without It\\n\\nCheck the wallet to make sure that the certificate has been successfully added:\\n\\n```bash\\n$ orapki wallet display -wallet /home/oracle/wallets/mywallet/\\nOracle PKI Tool Release 18.0.0.0.0 - Production\\nVersion 18.1.0.0.0\\nCopyright (c) 2004, 2017, Oracle and/or its affiliates. All rights reserved.\\n\\nEnter wallet password:\\nRequested Certificates:\\nUser Certificates:\\nTrusted Certificates:\\nSubject:        CN=Google Internet Authority G3,O=Google Trust Services,C=US\\n```\\n\\n## Set Up APEX to Use the Wallet\\n\\nLogin to APEX as the instance administrator and navigate to \\"Manage Instance > Instance Settings\\".\\n\\n![APEX Instance Setting - Wallet](./assets/apex-instance-setting-wallet.jpg)\\n\\nThat\'s it!\\n\\n## One Last Thing\\n\\nI did most of this in a Docker container using this [image build](https://github.com/fuzziebrain/docker-oracle-xe). The image doesn\'t have `openssl` available, so I grabbed the certs from the host operating system environment. For the Oracle Wallet management, you can access the tool by obtaining a Bash session in the database container.\\n\\n```bash\\n[opc@hostserver ~]$ docker exec -it -u oracle oracledb bash\\n[oracle@oracledb /]$ . oraenv\\nORACLE_SID = [XE] ?\\nThe Oracle base remains unchanged with value /opt/oracle\\n[oracle@oracledb /]$\\n```\\n\\nHope this helps. Lock it down!"},{"id":"id/1719","metadata":{"permalink":"/content/id/1719","source":"@site/posts/sweet-18-a-new-oracle-xe-release/index.md","title":"Sweet 18 - A New Oracle XE Release","description":"1280 720 Photo by danny howe","date":"2018-10-23T02:15:44.000Z","formattedDate":"October 23, 2018","tags":[{"label":"oracle database","permalink":"/content/tags/oracle-database"},{"label":"18cXE","permalink":"/content/tags/18-c-xe"},{"label":"orclapex","permalink":"/content/tags/orclapex"},{"label":"launch","permalink":"/content/tags/launch"},{"label":"release","permalink":"/content/tags/release"}],"readingTime":4.39,"hasTruncateMarker":true,"authors":[{"name":"Adrian Png","title":"Senior Cloud Solutions Architect @ Insum","url":"https://github.com/fuzziebrain","imageURL":"https://github.com/fuzziebrain.png","key":"fuzziebrain"}],"frontMatter":{"title":"Sweet 18 - A New Oracle XE Release","tags":["oracle database","18cXE","orclapex","launch","release"],"categories":["Technology","Programming","Oracle Application Express"],"slug":"id/1719","authors":["fuzziebrain"],"date":"2018-10-23T02:15:44.000Z"},"prevItem":{"title":"We Have a Wallet","permalink":"/content/id/1720"},"nextItem":{"title":"Build Two Walls","permalink":"/content/id/1718"}},"content":"![1280 720 Photo by danny howe](./assets/danny-howe-762860-unsplash_1280x720.jpg)\\n\\nWhat a prelude to [Oracle OpenWorld 2018](https://www.oracle.com/openworld/) (OOW18)! ICYMI, the next-generation Oracle Database 18c Express Edition (18cXE) was released late last week. The database software comes with a bunch of free enterprise features and options that usually come with a hefty price tag. Among them, [Oracle Advanced Analytics](https://www.oracle.com/technetwork/database/options/advanced-analytics/), [Oracle Advanced Security](https://www.oracle.com/database/technologies/security/advanced-security.html) and [Oracle RDF Semantic Graph](https://www.oracle.com/technetwork/database/options/spatialandgraph/documentation/documentation-087054.html) are my favourites! See the [Features Availability](https://docs.oracle.com/en/database/oracle/oracle-database/18/xelic/licensing-information.html#GUID-9D776CC6-7154-488B-A2D4-638534A6A66B) for more details on the [Oracle](https://www.oracle.com) great giveaway!\\n\\nSince Saturday, I have been kept busy working on getting the development environment ready for [Fabe](https://forallabeautiful.earth/). It was just weeks ago since we had the environment up and running on 11gXE. The instance is fully driven on the back of [Docker](https://www.docker.com) that made it easier to swap out the database.\\n\\n\x3c!-- truncate --\x3e\\n\\nI couldn\'t wait for [Gerald Venzi](https://twitter.com/GeraldVenzl) and his team to publish the [official Docker images](https://github.com/oracle/docker-images) for 18cXE, so I went ahead to create my own. My working files are [available](https://github.com/fuzziebrain/docker-oracle-xe) on GitHub and is somewhat based on their excellent work. [Martin D\'Souza](https://twitter.com/martindsouza) has since also contributed some few \\"minor\\" changes.\\n\\nDo note however, that I highly recommend using the official images instead, as soon as they are released. Another caution is that data files and database configuration persistance have not been implemented. Dropping your container will likely cause complete data loss. You have been warned.\\n\\nI followed the \\"[Exporting and Importing Data for Oracle Application Express (APEX) Users](https://docs.oracle.com/en/database/oracle/oracle-database/18/xeinl/exporting-and-importing-data-apex-users.html)\\" procedure to move data between the two database versions. The process was simple and straight-forward, but I was nervous nonetheless. The following are some additional notes that I thought might be useful to fellow [Oracle Application Express](https://apex.oracle.com) developers.\\n\\nBefore launching the new 18cXE container, I performed a data dump using Oracle\'s [Data Pump](https://docs.oracle.com/en/database/oracle/oracle-database/18/sutil/oracle-data-pump.html#GUID-501A9908-BCC5-434C-8853-9A6096766B5A) utility. The existing 11gXE container was then dropped. Prudence would have insisted that I left the container running, but I wanted to reuse the container name, so I had it blown away. The original container was created using the official Docker images and a host directory was mounted to the container as `/u01/app/oracle/oradata`, keeping the data and configuration files safe, so I had very little concerns deciding to delete the container once the data dump was done.\\n\\nFor convenience, I had set the `DUMP_DIR` to `/u01/app/oracle/oradata/export`, ensuring access after the 11gXE container had gone away. Then comes the fun part: building and running the brand new 18cXE Docker container. It took a few minutes, but was otherwise a very smooth process.\\n\\nOnce the database was up and running, the next thing to do is to perform the data dump import process. Using the [`docker cp`](https://docs.docker.com/engine/reference/commandline/cp/) (copy) command, I uploaded the dump file to the container\'s `/opt/oracle/oradata/export`. After that, I used the [`docker exec`](https://docs.docker.com/engine/reference/commandline/exec/) to get a Bash session. Once inside, I changed the ownership of the dump file: `chown oracle.oinstall /opt/oracle/oradata/export/db11g.dmp`.\\n\\nWith everything in place, I switched to the oracle user (`su - oracle`), [set the environment variables](https://docs.oracle.com/en/database/oracle/oracle-database/18/xeinl/setting-oracle-database-xe-environment-variables.html) (`. oraenv`) and then proceeded to use `impdp` to import the data. Make sure that the `REMAP_DIRECTORY` parameter was duly added as the default `ORACLE_BASE` path is different between 11g and 18c. It was rather unnerving as I was seeing a lot more errors than what was listed in the documentation that could be safely ignored. Thankfully, the process appeared to have completed successfully.\\n\\nRunning `apxfix.sql` was a first for me. APEX had already been updated to 18.2 in the previous 11gXE container and so I executed the following command in SQL*Plus: `apxfix.sql APEX_180200`. Felt like an APEX patching/upgrade process, but I\'m sure there was a little more to it. It didn\'t take very long to complete and to my relief, it was error free. From here, I skipped over the next two steps in the documentation as I had APEX already configured to use [Oracle REST Data Services](https://www.oracle.com/database/technologies/appdev/rest.html) (ORDS). The last prescribed step was to unlock the `ANONYMOUS` account in the CDB.\\n\\nBeyond what was documented, here are a few other steps needed to get the APEX instance up and running successfully:\\n\\n1. For the ORDS configuration, the `db.sid` parameter in `<CONFIG_DIR>/ords/defaults.xml` was replaced with `db.servicename` and set to `XEPDB1`, the PDB created by default during the database configuration process.\\n1. Update the network access control lists (ACL) using the [instructions](https://docs.oracle.com/database/apex-18.2/HTMIG/enabling-network-services-in-Oracle-db11g-or-later.htm#GUID-8D125731-989A-4A51-AFFE-00181729F87F) for Oracle Database 12c or later:\\n```sql\\nbegin\\n  dbms_network_acl_admin.append_host_ace(\\n    host => \'*\' -- or be specific on the host to allow access to\\n    , ace => xs$ace_type(\\n        privilege_list => xs$name_list(\'connect\')\\n        , principal_name => \'APEX_180200\'\\n        , principal_type => xs_acl.ptype_db\\n      )\\n  );\\nend;\\n/\\n```\\n\\nIf you are transitioning directly from 11g and have little or no experience with [Oracle Multitenant](https://www.oracle.com/database/technologies/multitenant.html) databases, please be especially mindful of the container switching steps (`alter session set container=XEPDB1;`) in the migration documentation. It takes a little effort to learn the new concepts, but the knowledge and experience are essential as non-CDB databases will soon be history. From now on, your work happens in a pluggable database (PDB) and will pretty much leave the container database (CDB) alone. You get three PDBs to use in 18cXE, more than what\'s available in current standard and enterprise editions without purchasing the additional multitenant database option.\\n\\nFeeling privileged? You should! Make full use of these goodies and build even more robust, secure and intelligent APEX apps!"},{"id":"id/1718","metadata":{"permalink":"/content/id/1718","source":"@site/posts/build-two-walls/index.md","title":"Build Two Walls","description":"1280 720 The Great Wall of China","date":"2018-10-18T00:28:32.000Z","formattedDate":"October 18, 2018","tags":[{"label":"orclapex","permalink":"/content/tags/orclapex"},{"label":"security","permalink":"/content/tags/security"},{"label":"authentication","permalink":"/content/tags/authentication"},{"label":"two-factor","permalink":"/content/tags/two-factor"},{"label":"multi-factor","permalink":"/content/tags/multi-factor"},{"label":"2fa","permalink":"/content/tags/2-fa"},{"label":"mfa","permalink":"/content/tags/mfa"}],"readingTime":3.545,"hasTruncateMarker":true,"authors":[{"name":"Adrian Png","title":"Senior Cloud Solutions Architect @ Insum","url":"https://github.com/fuzziebrain","imageURL":"https://github.com/fuzziebrain.png","key":"fuzziebrain"}],"frontMatter":{"title":"Build Two Walls","tags":["orclapex","security","authentication","two-factor","multi-factor","2fa","mfa"],"categories":["Technology","Programming","Oracle Application Express"],"slug":"id/1718","authors":["fuzziebrain"],"date":"2018-10-18T00:28:32.000Z"},"prevItem":{"title":"Sweet 18 - A New Oracle XE Release","permalink":"/content/id/1719"},"nextItem":{"title":"ODC Appreciation Day: Goodbye Oracle Multimedia","permalink":"/content/id/1717"}},"content":"![1280 720 The Great Wall of China](./assets/china-2097075_1280x720.jpg)\\n\\nAuthentication is an essential component of any enterprise application. These days though, it isn\'t enough to protect your applications with only a username and password. These are easily stolen by key loggers, sniffing non-encrypted HTTP traffic, phishing, hacked Internet services and the list goes on. To make matters worse, many users have a poor habit of reusing passwords that thieves then use to penetrate other systems. I have had my fair share of that experience.\\n\\nThe question is, can we implement [two-factor authentication](https://wikipedia.org/wiki/Multi-factor_authentication) (TFA) in [Oracle Application Express](https://apex.oracle.com) (APEX)?\\n\\n\x3c!-- truncate --\x3e\\n\\n**Yes we can!**\\n\\n## The Code\\n\\nRight around the time when I grew interested in implementing the [Time-based One-time Password](https://wikipedia.org/wiki/Time-based_One-time_Password_algorithm) (TOTP) algorithm, I stumbled upon [code](https://community.oracle.com/thread/3905510) written by Rabbit on the Oracle Community Forums, only months earlier. Hurrah! I didn\'t have to deal with the mathematical intricacies of implementing the algorithm in PL/SQL code.\\n\\nAfter discussing with [Martin D\'Souza](https://twitter.com/martindsouza), we decided that the best way to share this useful algorithm was through OraOpenSource [oos-utils](https://github.com/OraOpenSource/oos-utils). All it needed was some additional functionality to make it useful for implementing a custom authentication scheme with TFA support in APEX. We also swapped out the `dbms_crypto` requirement and used [Anton Scheffer\'s](https://twitter.com/AntonScheffer) `oos_util_crypto` library to perform the [HMAC](https://wikipedia.org/wiki/HMAC) hashing instead. This means, the code should be usable in your workspace on [apex.oracle.com](https://apex.oracle.com).\\n\\nWhat was missing in the release though, was a demo application to showcase its use, so two years later, here it is!\\n\\n* {% fa globe %} [Demo application](https://apeks.app/ords/f?p=TFADEMO)\\n* {% fa git %} [Source code](https://github.com/fuzziebrain/orclapex-tfa-demo)\\n\\n## Highlights\\n\\nThe following are a few key points about the implementation that I thought might be of interest.\\n\\n### Registration\\n\\nThe registration does not automatically \\"turn on\\" TFA for new accounts. The new user is expected to login to the system and activate the TFA protection. During registration, a random and unique key was generated for the account. Whenever TFA is activated, a QR code is generated using the function `oos_util_totp.format_key_uri`, user\'s \\"shared secret\\" and a suitable, trusted QR code generation JavaScript library. This allows the user to setup the security application and generate one-time passwords (OTP). The user must validate a OTP before the feature is enabled for the account.\\n\\n### Authentication\\n\\nThe authentication process will allow users to authenticate with or without OTP validation depending on the user\'s preference. The original (default) login page 9999 was modified to support both authentication workflows.\\n\\nThe `Login` process typically calls `apex_authentication.login` that triggers a sequence of events [listed](https://docs.oracle.com/database/apex-18.2/AEAPI/LOGIN-Procedure.htm#AEAPI29157) in the official APEX documentation. The process calls an authentication function that matches usernames and passwords. If the function returns true, then the APEX engine will, in a nutshell, set the username for the session and redirect to the requested resource. This function call was replaced with a procedure, `pkg_tfa_apex.p_authenticate_user`.\\n\\n```plsql\\nif l_password_hash = f_hash_password(p_password => p_password) then\\n  if l_tfa_enabled = 0 then\\n    apex_authentication.post_login(\\n      p_username => p_username\\n      , p_password => null\\n    );\\n    apex_util.clear_page_cache();\\n  end if;\\nelse\\n  raise login_failed;\\nend if;\\n```\\n\\nIf TFA is **not** enabled, then `apex_authentication.post_login` is called. We have already established that the username and password are correct, so this allows us to bypass the first two steps in a typical authentication sequence.\\n\\nUsually, the default login page has a final process that clears the session state for the page, but that was removed. A branch to a second-step login page (10000) was added. It passes the username entered on page 9999 to the the next page for OTP validation. The username is used to retrieve the user\'s shared secret that is is required by `oos_util_totp.validate_otp`. Hence, I opted to call `apex_util.clear_page_cache` only if TFA was turned off.\\n\\n### OTP Validation\\n\\nThe login process redirects to a second login page that requires the user to enter the OTP from a \\"registered\\" security app. This is verified (must match with what the system generates) using a page validation. If that checks out, then we will proceed to call the `apex_authentication.post_login` procedure and complete the login process.\\n\\n## Closing\\n\\nAs always, security is an important issue in any software application we build. I am always learning, so please drop me a note if you spot anything wrong with the approach, or have suggestions for improvement.\\n\\nThanks for reading!"},{"id":"id/1717","metadata":{"permalink":"/content/id/1717","source":"@site/posts/odc-appreciation-day-goodbye-oracle-multimedia/index.md","title":"ODC Appreciation Day: Goodbye Oracle Multimedia","description":"Salmon Run - Honour the Dead","date":"2018-10-11T09:00:00.000Z","formattedDate":"October 11, 2018","tags":[{"label":"orclapex","permalink":"/content/tags/orclapex"},{"label":"oracle","permalink":"/content/tags/oracle"},{"label":"database","permalink":"/content/tags/database"},{"label":"multimedia","permalink":"/content/tags/multimedia"},{"label":"image processing","permalink":"/content/tags/image-processing"},{"label":"ThanksODC","permalink":"/content/tags/thanks-odc"}],"readingTime":1.4,"hasTruncateMarker":true,"authors":[{"name":"Adrian Png","title":"Senior Cloud Solutions Architect @ Insum","url":"https://github.com/fuzziebrain","imageURL":"https://github.com/fuzziebrain.png","key":"fuzziebrain"}],"frontMatter":{"title":"ODC Appreciation Day: Goodbye Oracle Multimedia","tags":["orclapex","oracle","database","multimedia","image processing","ThanksODC"],"categories":["Technology","Programming","Oracle Application Express"],"slug":"id/1717","authors":["fuzziebrain"],"date":"2018-10-11T09:00:00.000Z"},"prevItem":{"title":"Build Two Walls","permalink":"/content/id/1718"},"nextItem":{"title":"Upcoming Chrome APIs of Note","permalink":"/content/id/1716"}},"content":"![Salmon Run - Honour the Dead ](./assets/salmon-run-honour-the-dead.jpg)\\n\\nHere\'s my first contribution to the annual [ODC Appreciation Day](https://oracle-base.com/blog/2018/09/27/oracle-developer-community-odc-appreciation-day-2018-thanksodc/) that was initiated by [Tim Hall](https://twitter.com/oraclebase) three years ago.\\n\\nThe product feature that I\'d like to pay special tribute to this year is **Oracle Multimedia (OMM)**. By now, I believe most would have heard that OMM has been deprecated. [Mike Dietrich](https://twitter.com/MikeDietrichDE) made special mention of it in a [blog post](https://mikedietrichde.com/2018/08/10/oracle-multimedia-is-deprecated-in-oracle-18c/) earlier this year, with some clarity as to exactly when we can expect to see this useful feature go away for good.\\n\\n\x3c!-- truncate --\x3e\\n\\nAs Mike pointed out, we can anticipate that OMM will be completely removed from Oracle Database 19c, and that\'s scheduled to be in January 2019 or perhaps even earlier [says](http://www.dbta.com/Editorial/News-Flashes/Larry-Ellison-Previews-Roadmap-to-Oracle-Database-Version-19c-126774.aspx) the man who knows.\\n\\n> \\"Oracle recommends that you store multimedia content in SecureFiles LOBs, and use open source or third-party products for image processing and conversion.\\"\\n>\\n> <cite>-- <a href=\\"https://docs.oracle.com/en/database/oracle/oracle-database/18/upgrd/terminal-release-oracle-multimedia.html#GUID-DBA90E57-2BD8-4CEE-A74B-BFC889B78057\\" target=\\"_blank\\">Oracle Database Upgrade Guide - Terminal Release of Oracle Multimedia</a></cite>\\n\\nAt Kscope17, I spoke about using OMM and various other techniques to capture and manipulate images with [Oracle Application Express](https://apex.oracle.com/). The demo application I wrote and deployed on [apex.oracle.com](https://apex.oracle.com), ran without issues. It included functionality that used OMM PL/SQL APIs to perform simple image processing tasks like thumbnail generation and applying watermarks to uploaded photos. Months later, the code stopped working (though it does now), and I remembered having conversations about this issue that eventually lead to the understanding that OMM was going away.\\n\\nGoing forward, what are our alternatives? Perhaps a discussion for another day. For now, a **BIG THANK YOU** to the brilliant minds at Oracle who brought us Oracle Mutlimedia a.k.a. Oracle interMedia."},{"id":"id/1716","metadata":{"permalink":"/content/id/1716","source":"@site/posts/upcoming-chrome-apis-of-note/index.md","title":"Upcoming Chrome APIs of Note","description":"Screenshot of application demonstrating Face Detection API","date":"2018-09-26T19:56:41.000Z","formattedDate":"September 26, 2018","tags":[{"label":"orclapex","permalink":"/content/tags/orclapex"},{"label":"javascript","permalink":"/content/tags/javascript"},{"label":"emerging technology","permalink":"/content/tags/emerging-technology"},{"label":"face detection","permalink":"/content/tags/face-detection"},{"label":"barcode","permalink":"/content/tags/barcode"},{"label":"ocr","permalink":"/content/tags/ocr"},{"label":"text recognition","permalink":"/content/tags/text-recognition"}],"readingTime":2.26,"hasTruncateMarker":true,"authors":[{"name":"Adrian Png","title":"Senior Cloud Solutions Architect @ Insum","url":"https://github.com/fuzziebrain","imageURL":"https://github.com/fuzziebrain.png","key":"fuzziebrain"}],"frontMatter":{"title":"Upcoming Chrome APIs of Note","tags":["orclapex","javascript","emerging technology","face detection","barcode","ocr","text recognition"],"categories":["Technology","Programming","Oracle Application Express"],"slug":"id/1716","authors":["fuzziebrain"],"date":"2018-09-26T19:56:41.000Z"},"prevItem":{"title":"ODC Appreciation Day: Goodbye Oracle Multimedia","permalink":"/content/id/1717"},"nextItem":{"title":"Search Better with Oracle Text","permalink":"/content/id/1715"}},"content":"![Screenshot of application demonstrating Face Detection API](./assets/face-detection.jpg)\\n\\nAt Kscope17, I presented a session on various techniques for capturing and manipulating images. For those of who attend either in-person or the follow-up webinar, you might recall I briefly talked about image recognition and how they can be integrated with an [Oracle Application Express](https://apex.oracle.com) solution using third-party JavaScript libraries.\\n\\n\x3c!-- truncate --\x3e\\n\\n## Shape Detection API\\n\\nGoogle has submitted a draft of the [Shape Detection API](https://wicg.github.io/shape-detection-api/) specification for consideration as a standard under the [W3C](https://www.w3.org/). These experimental APIs will be available as an experimental feature from Chrome 70 (see the [announcement](https://blog.chromium.org/2018/09/chrome-70-beta-shape-detection-web.html)).\\n\\nThe following will be available under this specification:\\n\\n* Face Detection\\n* Barcode Detection\\n* Text Detection\\n\\nThese APIs are very promising and easy to use. Both barcode and text detection returns the raw value, which you can then use to store in the database. Imagine the implications.\\n\\nI am very excited, and have created a [demo application](https://apeks.app/ords/f?p=126) to try them out. Remember, Chrome 70 will be out around mid-October. If you can\'t wait, then download [Chrome Canary](https://www.google.com/chrome/canary/). A mobile device will be required for the barcode detection demo to work.\\n\\nHopefully, this specification will mature quickly and be accepted for implementation in other web browsers. While that\'s incubating, let\'s look at another noteworthy API, mentioned in the same blog post.\\n\\n## Web Authentication API\\n\\nThe article mentioned some changes and additions including using Mac TouchID and Android fingerprint sensors for authentication. Unlike the Shape Detection API, the [WebAuthn](https://www.w3.org/TR/webauthn/) specification is already a candidate recommendation and is supported by current versions of Chrome and Firefox. Microsoft Edge support is currently available under the Windows Insider Program, but should be generally available in October (again), when the 2018 Fall update is released.\\n\\nWebAuthn takes away our reliance on vulnerable passwords. Users can use biometrics, a TPM (Trusted Platform Module) or [FIDO U2F](https://www.yubico.com/solutions/fido-u2f/) device to securely generate, exchange and verify keys, to gain access to systems. For Windows 10 users, Edge support will include [Windows Hello](https://support.microsoft.com/en-ca/help/17215/windows-10-what-is-hello). Users will be able to authenticate using face or fingerprint recognition.\\n\\n![Just Arrived - FIDO2 Security Key by Yubico](./assets/fido2-device.jpg)\\n\\nThe specification provides for two workflows: registration and authentication. A device must first be registered with an account before it can be used to perform the authentication workflow. Both workflows are driven by JavaScript on the client-side, but will require server interaction to complete the process. Something that I hope to continue exploring in the coming weeks, if time permits.\\n\\nMeanwhile, I believe both Google and Microsoft are working towards supporting WebAuthn for their accounts. With [APEX 18c support for OAuth2 authentication](/content/id/1709), developers can simply rely upon the identity providers to enforce security/login requirements, including the user of WebAuthn devices."},{"id":"id/1715","metadata":{"permalink":"/content/id/1715","source":"@site/posts/search-better-with-oracle-text/index.md","title":"Search Better with Oracle Text","description":"Photo by John Michael Thompson","date":"2018-09-25T01:00:29.000Z","formattedDate":"September 25, 2018","tags":[{"label":"orclapex","permalink":"/content/tags/orclapex"},{"label":"Oracle Text","permalink":"/content/tags/oracle-text"},{"label":"search","permalink":"/content/tags/search"},{"label":"rdm","permalink":"/content/tags/rdm"}],"readingTime":8.685,"hasTruncateMarker":false,"authors":[{"name":"Adrian Png","title":"Senior Cloud Solutions Architect @ Insum","url":"https://github.com/fuzziebrain","imageURL":"https://github.com/fuzziebrain.png","key":"fuzziebrain"}],"frontMatter":{"title":"Search Better with Oracle Text","tags":["orclapex","Oracle Text","search","rdm"],"categories":["Technology","Programming","Oracle Application Express"],"slug":"id/1715","authors":["fuzziebrain"],"date":"2018-09-25T01:00:29.000Z"},"prevItem":{"title":"Upcoming Chrome APIs of Note","permalink":"/content/id/1716"},"nextItem":{"title":"Research-Ready","permalink":"/content/id/1714"}},"content":"![Photo by John Michael Thompson](./assets/john-michael-thomson-555276-unsplash_1280x720.jpg)\\n\\nIt\'s not unusual for an established [principal investigator](https://en.wikipedia.org/wiki/Principal_investigator) (PI) to have desks and shelves full of books, journal articles, grant applications, graduate student theses etc. Often, they\'d also have a mountain of such material in electronic form and stored on desktop hard drives that sometimes get forgotten. It takes an extremely good memory to remember which articles are relevant to a particular keyword you might be interested in.\\n\\nThankfully, there\'s Oracle Text (and [Oracle Application Express](https://apex.oracle.com/) (APEX)) to keep you sane!\\n\\n\x3c!-- More --\x3e\\n\\n## Setting the Stage\\n\\nI\'m not a researcher myself, so naturally, I don\'t horde a lot of these materials. For the purpose of this blog post, I will use [open data](https://en.wikipedia.org/wiki/Open_data) made available, courtesy of the Canadian Government.\\n\\nThe [National Sciences and Engineering Council of Canada](http://www.nserc-crsng.gc.ca) (NSERC) is one of Canada\'s federal agencies that is responsible for promoting and providing funding for research in the natural sciences and engineering fields. They publish annual lists of grants that were successfully awarded. You can find them [here](https://open.canada.ca/data/en/dataset/c1b0f627-8c29-427c-ab73-33968ad9176e). I used only one year\'s (2017) worth of data. There\'s a lot of research activity going on in the north!\\n\\nStart off by first creating the necessary data structure to hold the data. I used the [Quick SQL](https://apex.oracle.com/quicksql/) tool to quickly generate the necessary [DDL](https://en.wikipedia.org/wiki/Data_definition_language) script and then create the database.\\n\\n#### Quick SQL Shorthand\\n\\n```\\n# settings = { genPK: false, db: 11g }\\naward\\n    id number /pk\\n    name varchar2(200)\\n    department varchar2(200)\\n    organization_id number\\n    institution varchar2(200)\\n    province varchar2(100)\\n    country varchar2(100)\\n    fiscal_year number\\n    competition_year number\\n    award_amount number\\n    program_id varchar2(50)\\n    program_name varchar2(200)\\n    program_group varchar2(100)\\n    committee_code number\\n    committee_name varchar2(200)\\n    area_of_application_code number\\n    area_of_application_group varchar2(200)\\n    area_of_application varchar2(200)\\n    research_subject_code number\\n    research_subject_group varchar2(200)\\n    research_subject varchar2(200)\\n    application_title varchar2(500)\\n    source varchar2(50)\\n    application_summary varchar2(4000)\\n```\\n\\n#### Oracle SQL Output\\n\\n```sql\\ncreate table award (\\n    id                             number not null constraint award_id_pk primary key,\\n    name                           varchar2(200),\\n    department                     varchar2(200),\\n    organization_id                number,\\n    institution                    varchar2(200),\\n    province                       varchar2(100),\\n    country                        varchar2(100),\\n    fiscal_year                    number,\\n    competition_year               number,\\n    award_amount                   number,\\n    program_id                     varchar2(50),\\n    program_name                   varchar2(200),\\n    program_group                  varchar2(100),\\n    committee_code                 number,\\n    committee_name                 varchar2(200),\\n    area_of_application_code       number,\\n    area_of_application_group      varchar2(200),\\n    area_of_application            varchar2(200),\\n    research_subject_code          number,\\n    research_subject_group         varchar2(200),\\n    research_subject               varchar2(200),\\n    application_title              varchar2(500),\\n    source                         varchar2(50),\\n    application_summary            varchar2(4000)\\n)\\n;\\n```\\n\\n> **NOTE**\\n> Change the DB version as required. The default is 12c, but since I am running the database Express Edition (XE), I used 11g.\\n\\n## Filling Up the Repository\\n\\nThe data from NSERC is fairly clean and formatted. The 2017 data that I downloaded however, was a little too large to import using the built-in SQL Workshop tool in APEX. Fortunately, [SQL Developer](https://www.oracle.com/database/technologies/appdev/sql-developer.html) has a rather sophisticated functionality for doing the same, though it wasn\'t without challenges. I\'ll leave data importing methods for another day\'s discussion. For the purpose of this exercise, I did managed to import at least 5,000 records.\\n\\n## Search-Enabling the Table of Grants Awarded\\n\\nOracle Text is a very powerful, no-cost feature of the Oracle Database that doesn\'t get enough attention as it should. There are many interesting concepts and customizations that can be applied and tune the Oracle Text Index. However, creating a basic index is rather straightforward and should not be feared.\\n\\nMy goal:\\n\\n* List grant awards that are about fossil fuels and the effects on the environment.\\n* The corpus contains applications written in both English and French. Users should be able to search with either language.\\n* Users enter search terms using only <u>one</u> text field, but search **must** include the following columns:\\n    * `application_title`\\n    * `application_summary`\\n    * `research_subject`\\n    * `area_of_application`\\n\\n### Required Database Privileges\\n\\nTo work with Oracle Text, the database user requires a few additional privileges that have already been granted to the role `CTXAPP`. Assigning the user to the role should be all that\'s needed:\\n\\n```sql\\ngrant ctxapp to dbuser;\\n```\\n\\n### Create the Oracle Text Index.\\n\\n```sql\\nbegin\\n    -- Remove any previously created DATASTORE preference with the same name.\\n    begin\\n       ctx_ddl.drop_preference(\'SEARCH_DATASTORE\');\\n    exception\\n        when others then null;\\n    end;\\n\\n    -- Create the DATASTORE preference.\\n    ctx_ddl.create_preference(\'SEARCH_DATASTORE\', \'MULTI_COLUMN_DATASTORE\');\\n    ctx_ddl.set_attribute(\'SEARCH_DATASTORE\', \'COLUMNS\', \'application_title, application_summary, research_subject, area_of_application\');\\n\\n    -- Drop any existing index with the same name.\\n    begin\\n        execute immediate \'drop index award_idx\';\\n    exception\\n        when others then null;\\n    end;\\n\\n    -- Create the index.\\n    execute immediate q\'[\\ncreate index award_idx on award(application_title)\\nindextype is ctxsys.context\\nparameters (q\'{\\n    DATASTORE SEARCH_DATASTORE\\n}\')\\n]\';\\nend;\\n/\\n```\\n\\nThe comments in the procedure shown above explains the basic commands needed to create the index. For convenience, they are listed below:\\n\\n1. Remove any previously created preference with the same name.\\n1. Create the [DATASTORE](https://docs.oracle.com/cd/E11882_01/text.112/e24436/cdatadic.htm#CCREF0203) preference.\\n1. Drop any existing index with the same name.\\n1. Create the index.\\n\\nAs mentioned, there are additional indexing elements that could be used to further optimise and customize the search index, but they are out of scope at this time. This simple setup will already allow some powerful search abilities with minimal query design.\\n\\n### Query the Data\\n\\nThere are different operators to use in the `WHERE` clause, depending on the index type created. In the example, an index of type `CONTEXT` was created, thus the query will have to be constructed using the `CONTAINS` operator.\\n\\n```sql\\nselect\\n    id\\n    , application_title\\n    , application_summary\\nfrom award\\nwhere 1 = 1\\n    and contains(application_title, \'\\"fossil fuels\\" and environment\', 1) > 0\\norder by search_score desc;\\n```\\n\\nOracle provides a long list of `CONTAINS` [query operators](https://docs.oracle.com/cd/E11882_01/text.112/e24436/cqoper.htm#CCREF0300) that allows users to formulate powerful queries. The example above is a simple boolean search (`AND` operator) requiring both the phrase \\"fossil fuels\\" and word \\"environment\\" for a match. A simple misspelling of \\"environment\\" would likely yield no results. That\'s where other operators like `fuzzy` provide a margin error, allowing returning results even when there are no exact matches. The same operator might even forgive simple nuances like \\"colour\\" versus \\"color\\". There are also wildcard (`%`) and other operators that are denoted using special characters that could sometimes spell trouble even for a seasoned Oracle Text user.\\n\\nTo protect the untrained user, it is recommended to pre-process the user input and restructure keywords and options into proper query operator constructs. This isn\'t always easy, and so, we are fortunate that [Roger Ford](https://blogs.oracle.com/author/roger-ford), Oracle Text\'s Product Manager, posted a [blog](https://blogs.oracle.com/searchtech/oracle-text-query-parser) and useful utility package for parsing user inputs from a simple text field. The syntax closely resembles Google\'s search operators and should be familiar with most users. The code is available to download at the end of Ford\'s article.\\n\\nAfter compiling the script, functions such as `parser.simpleSearch` and `parser.andSearch` will be available to use. Check the comments in the code for details about each function.\\n\\n```sql\\nSQL> select parser.andsearch(\'\\"fossil fuels\\" environment\') from dual;\\n-- returns ({fossil fuels}&{environment})\\n```\\n\\n## Querying with APEX\\n\\nWith too little screen space on a blog post, it\'s easier to demonstrate the search results using APEX. The accompanying [demo application](https://apeks.app/ords/f?p=awardsearch) showcases Oracle Text integration using two different user interface approaches.\\n\\nThe first uses Interactive Grids (IG) that has built-in support for Oracle Text. After creating the IG, click on `Attributes` and then on the right, scroll down to the `Advanced` section and select the indexed column as the `Oracle Text Index Column` attribute.\\n\\n![Setup Oracle Text for Interactive Grids](./assets/oracle-text-ig.jpg)\\n\\nInteractive Grids allows developers to specify a PL/SQL function for converting simple keyword inputs to well-formed Oracle Text syntax. Go to the application\'s `Shared Components`> `Component Settings` > `Interactive Grids` and then enter the name of the function to use. We can use one of the functions from Ford\'s Parser package as they meet the requirement of accepting a single string argument.\\n\\n![Specify a PL/SQL Function for Preprocessing Input String](./assets/oracle-text-ig-settings.jpg)\\n\\nThe second approach uses a Classic Report (CR) that allows more flexibility on how the results are displayed, for example, sorting the results by score (descending) and application title (ascending). The user enters the search keywords in a single text field page item (`P2_SEARCH_TERMS`) that is then parsed and used with a `CONTAINS` operator to query the table. The SQL query used is as follows:\\n\\n```sql\\nselect\\n    application_title as search_title\\n    , application_summary as search_desc\\n    , round(score(1), 2) as value_01\\n    , \'Score\' as label_01\\n    , q\'[javascript:apex.message.alert(\'Not implemented.\');void(0);]\' as search_link\\nfrom award\\nwhere 1 = 1\\n    and contains(application_title, parser.progrelax(:P2_SEARCH_TERMS), 1) > 0\\n```\\n\\n![Displaying Oracle Text Search Results Using Classic Reports](./assets/oracle-text-cr.jpg)\\n\\n## Multilingual Search\\n\\nBoth search interfaces will accept either English or French keywords. However, search results are language-specific. If an English keyword is entered, then only applications submitted in English will be returned. Likewise with French search terms. For a heterogenous search, a thesaurus will be required. The parsing function will also need to be \\"translation-aware\\", and either accept or transform queries to use operators such as `TR`. These translation operators will expand the search keywords to include the specified language\'s equivalent term.\\n\\nThe `ctxload` utility provides developers the ability to import a thesaurus contained in a plain-text file. Developers can also script the thesaurus:\\n\\n```sql\\ndeclare\\n    l_thes_name varchar2(11) := \'SCIENG_THES\';\\nbegin\\n    ctx_thes.create_thesaurus(name => l_thes_name);\\n    ctx_thes.create_phrase(tname => l_thes_name, phrase => \'water\');\\n    ctx_thes.create_phrase(tname => l_thes_name, phrase => \'waters\');\\n    ctx_thes.create_translation(tname => l_thes_name, phrase => \'water\', language => \'FRENCH\', translation => \'eau\');\\n    ctx_thes.create_translation(tname => l_thes_name, phrase => \'waters\', language => \'FRENCH\', translation => \'eaux\');\\nend;\\n/\\n```\\n\\nUnfortunately, Ford\'s parser library does not support translations, but here\'s an example query that could be used to demonstrate the translation capability. The search term is \\"water\\", and the results will return both English and French applications containing either \\"water\\" or \\"eau\\":\\n\\n```sql\\nselect\\n    id\\n    , score(1) as search_score\\n    , application_title\\n    , application_summary\\nfrom award\\nwhere 1 = 1\\n    and contains(application_title, \'tr(water, french, scieng_thes)\', 1) > 0\\norder by search_score desc;\\n```\\n\\n## Final Notes\\n\\nBesides searching large bodies of text, Oracle Text has the ability to index and search binary files such as PDF, HTML, XML and even Microsoft Word documents. Check out a full list of [supported document formats](https://docs.oracle.com/cd/E11882_01/text.112/e24436/afilsupt.htm#CCREF1300).\\n\\n> **NOTE:**\\n> If the document filtering appears not to be working , i.e. a query does not return any expected results, then check that the needed operating system libraries, e.g. `compat-libstdc++-33`, have been installed. You can test this by running the binary: `$ORACLE_HOME/ctx/bin/ctxhx`.\\n>\\n> Get more background about this [thread](https://community.oracle.com/thread/4028703) on the Oracle community forums.\\n\\nAll the technology described and used in this blog post and demo are available to at no cost. That\'s always a welcomed surprise for any research project.\\n\\n#### Software Used\\n\\n| Software | Cost |\\n|-|-|\\n| Oracle Database 11gR2 XE | FREE |\\n| Oracle APEX 18.1 | FREE |\\n| SQL Developer 18.2 | FREE |"},{"id":"id/1714","metadata":{"permalink":"/content/id/1714","source":"@site/posts/research-ready/index.md","title":"Research-Ready","description":"Photo by Louis Reed](./assets/louis-reed-747388-unsplash-1280x720.jpg) --\x3e","date":"2018-09-06T21:04:35.000Z","formattedDate":"September 6, 2018","tags":[{"label":"orclapex","permalink":"/content/tags/orclapex"},{"label":"research","permalink":"/content/tags/research"},{"label":"data","permalink":"/content/tags/data"},{"label":"management","permalink":"/content/tags/management"},{"label":"rdm","permalink":"/content/tags/rdm"}],"readingTime":13.225,"hasTruncateMarker":true,"authors":[{"name":"Adrian Png","title":"Senior Cloud Solutions Architect @ Insum","url":"https://github.com/fuzziebrain","imageURL":"https://github.com/fuzziebrain.png","key":"fuzziebrain"}],"frontMatter":{"title":"Research-Ready","tags":["orclapex","research","data","management","rdm"],"slug":"id/1714","authors":["fuzziebrain"],"categories":["Technology","Others"],"date":"2018-09-06T21:04:35.000Z"},"prevItem":{"title":"Search Better with Oracle Text","permalink":"/content/id/1715"},"nextItem":{"title":"Snap in Storage For Your Docker Playground","permalink":"/content/id/1713"}},"content":"\x3c!-- ![<a target=\\"_blank\\" href=\\"https://unsplash.com/photos/pwcKF7L4-no?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText\\">Photo by Louis Reed</a>](./assets/louis-reed-747388-unsplash-1280x720.jpg) --\x3e\\n\\nIn a flash, it has been almost three years since I left the [Centre for Heart Lung Innovation](https://www.hli.ubc.ca), so it\'s no surprise that a recent [tweet](https://twitter.com/OracleAPEX/status/1034395229334523905) caught my eye and tugged the memory chord. I was introduced to [Oracle Application Express](https://apex.oracle.com/) (APEX) on August 26, 2010. A year (and a major release) later, I had the opportunity to demonstrate it\'s capabilities with proof that it could help improve productivity in our research centre. As a sidenote, back then, attendees receive DVDs full of developer software goodies. APEX however, didn\'t need much, just check out the hands-on lab requirements below.\\n\\n\x3c!-- truncate --\x3e\\n\\n![Event confirmation email from 2010](./assets/blast-email-from-the-past.jpg)\\n\\nYup, nothing! All I needed was a laptop, Internet connection and a modern day web browser!\\n\\n> **TL;DR** APEX is a Rapid Application Developer (RAD) platform that is **secure**, **robust** and **extremely flexible** for building web-accessible applications to support research, e.g. data collection forms, surveys, freezer/biospecimen inventory management, laboratory inventory management and many more!\\n\\n## Research Data Management\\n\\nNearly every form of research would involve some form of data collection. As responsible stewards, researchers must ensure data is acquired ethically and accurately, used appropriately, and archived for future validation and reuse. For clinical researchers, rigorous, well-defined data collection methods are even more critical due to the nature and type of data that is collected.\\n\\nClinical research studies that involves human subjects are often governed by Institution Review Boards (IRB). It is the duty of such committees to ensure that the study is ethical and complies with regulatory requirements. That sometimes involves thorough examination of the Data Management Plan (DMP), thereby ensuring that patients\'/participants\' interests are safeguarded.\\n\\nA good Research Data Management (RDM) strategy should consider the following:\\n\\n* Collection\\n    * Representation - data structures/schemas\\n    * Storage - #SayNoToSpreadsheets\\n    * Documentation - maintaining an up-to-date data dictionary\\n    * Capture - tools for collecting the data\\n* Maintenance and Compliance\\n    * Security - storage, access and audits\\n    * Backups\\n    * Disaster Recovery\\n    * Monitoring\\n* Use\\n    * Access and sharing\\n    * Quality control\\n    * Processing and transformations\\n* Preservation or Destruction\\n\\n## Why APEX?\\n\\nOne of the more popular software for Electronic Data Capture (EDC) in research is [REDCap](https://projectredcap.org/). The application was first developed and released by [Vanderbilt University](https://www.vanderbilt.edu/) in 2004, to provide clinical researchers with a secure data collection tool that met regulatory requirements, and yet not needing users to have extensive technical backgrounds.\\n\\nUnfortunately, the software and learning resources are not readily available. To gain access to the application, aspiring REDCap developers must first belong to an institution that is willing to sign up to be a consortium partner. You can however, sign up for a one-week trial account on their demo server.\\n\\nTo get a sense of how APEX might fit the research environment, I thought it would be fitting to use REDCap\'s list of [software features](https://projectredcap.org/software/) against what the platform had to offer.\\n\\n| REDCap Feature | APEX |\\n|-|-|\\n| Project design (online) | One of APEX key strengths is its comprehensive, web-based graphical development environment, the App Builder. To get a brief overview of the UI, check out this [9-minute video](https://www.youtube.com/watch?v=q_GfZFio9qg) from Oracle. |\\n| Project design (offline) | REDCap allows users to create and edit data collection instruments (forms) using a CSV. Users can modify just about everything including field types, calculation fields and validations. Though APEX applications are declarative and has a rich API, it does not have a matching functionality for creating pages using a configuration file... or do we? Using [Application Blueprint](https://docs.oracle.com/database/apex-18.1/HTMDB/using-blueprints.htm), APEX developers can quickly create an application defined in a JSON file. [Dimitri Gielis](https://twitter.com/dgielis) demonstrates how to get this done in this short [video tutorial](https://youtu.be/bccne14Qu10).\\n| Availability | <p>REDCap is available free-of-charge, but as mentioned earlier, you will need to first sign up as an institutional consortium partner.</p><p>APEX is available to download and install at no cost as well. That\'s right, **no purchase necessary**! Oracle offers a free version of their database. Version 18c and later will even include [database features](https://twitter.com/dgielis/status/1005806235147554816) otherwise only available in Enterprise Editions of the software.</p><p>For no-fuss learning and understanding the platform, developers can create a workspace on [apex.oracle.com](https://apex.oracle.com). It\'s free and the only requirement to keep the \\"trial\\" going is to keep using it.</p> |\\n| Secure and web-based | <p>APEX runs on top of perhaps the world\'s most secure database. The Oracle database has built-in features like auditing, secure database connections, cryptographic stored procedures and functions for protecting sensitive information. Other paid options provide additional security features like Transparent Data Encryption, Data Redaction, Fine Grained Auditing (FGA) and much more!</p><p>APEX applications can also be configured to use a wide-variety of authentication methods, including enterprise systems and protocols like LDAP and OAuth2. Single-sign on (SSO) available out-of-the-box.</p> |\\n| Fast and flexible | With APEX, you can develop production-grade, enterprise-ready applications with minimum amount of coding. Users also have at their disposal, tools like [Quick SQL](https://apex.oracle.com/en/quicksql/) and Application Builder for quickly defining data structures and creating applications. Users can also transform spreadsheet data into modern, professional-looking reports and data entry forms in minutes! |\\n| Multi-site access | Mutli-tenancy support is available at many levels. For example, research groups could isolate their applications and data using APEX workspaces and database schemas. Access can be managed and federated using enterprise identity management systems cited earlier. |\\n| Autonomous utilization | Administrators within a research groups have full autonomy to manage user/developer accounts and access. |\\n| Fully customizable | APEX Applications are more than fully customizable. Users/developers can modify the look-and-feel (Themes) and behaviour (Dynamic Actions and Processes). For advanced developers, the sky\'s the limit! |\\n| Audit trails | The Oracle Database provides standard auditing features. Enterprise Edition customers have the option to also enable FGA. |\\n| Automated export procedures | The Data Workshop utility in APEX provides the ability to import and export data in XML and CSV formats. Developers can also write PL/SQL routines to generate the data in a suitable download format. Packaged as an APEX plugin, these procedures are then readily reusable in other applications. For example, Didier Bastogne created a Process Type [plugin](http://www.apex-plugin.com/oracle-apex-plugins/process-type-plugin/download-sdf_388.html) that allows users to download a table of molecules in SDF file format. |\\n| A built-in project calendar | APEX comes with a comprehensive Team Development utility that allows developers to work together. Developers can set milestones, define features, track TODO items, file bugs and manage feedback, all within the workspace environment. |\\n| A scheduling module | This module is specially geared to help with longitudinal studies. Since APEX is a multi-purpose application development platform, such modules do not exist. Developers will have to create scheduling features within a clinical research application. |\\n| Ad hoc reporting tools | User-driven reporting is one of APEX\'s greatest strengths. There is too much to write about Interactive Reports (IR) and Interactive Grids (IG) to condense in a single blog post. Suffice to say that with IRs and IGs, users can filter, sort, create computations and aggregations, and generate charts, all without developer intervention. |\\n| Branching logic | Dynamic actions and page branching in APEX provides developers declaratively define the application\'s business logic. |\\n| File uploading | The Data Load utility described earlier, facilitates the import of data into the system. Developers may also use the File Browse page item to allow users to submit files for upload. Files can be staged and manipulated in the database before it is finally stored. APEX also has a wizard to create a data-loading wizard that allows users to upload, map and import delimited data files.  |\\n| Calculated fields | Use Dynamic Actions to perform on-the-fly calculations using either JavaScript, PL/SQL or both. |\\n| A quick and easy software installation process | Installing the APEX software stack is more involved and probably requires a steeper learning curve. However, with modern DevOps tools available and a lot of helpful fellow developers, there are now [Docker images](https://github.com/oracle/docker-images), [Vagrant configuration files](https://github.com/oracle/vagrant-boxes) and [build scripts](https://github.com/OraOpenSource/OXAR/) to help setup the stack with minimal user interaction. |\\n| Regulatory compliance | The platform itself may not been certified with regulatory authorities, however, infrastructure providers could architect and have their systems validated. |\\n| Available in multiple languages | Multiple languages are [supported](https://docs.oracle.com/database/apex-18.1/HTMDB/understanding-application-translation-and-globalization-support.htm#HTMDB14001). [Community-contributed translation](http://translate-apex.com) text are also available freely for download and use. |\\n\\n## Features of Note\\n\\nThe following are three features that makes it even more compelling to take a deeper look at APEX.\\n\\n### Enterprise-ready Authentication Schemes\\n\\n> Available since version 4.0\\n\\nThe Social Sign In authentication scheme, made available with the 18.1 release, is my favourite. To me, an important consideration when introducing new software or platforms, is its ability to integrate with existing systems, most importantly, security. Let\'s face it, users do a poor job in maintaining their account credentials and so, the less username-password combos they need to come up with, the better. A SSO setup also allows administrators to introduce more secure policies like requiring the use of two-factor authentication during logins.\\n\\nThe new authentication scheme comes pre-configured to work with Google and Facebook\'s OAuth2 providers. I have written previously on [how to do this](/content/id/1709/). All you need are the correct URLs to use. No coding required. There are two additional setups for OpenID and generic OAuth2 providers like Microsoft.\\n\\nBesides OAuth2, APEX has supported LDAP and HTTP Header Variable authentication for a while now. I have also written about how SSO can be [achieved with SAML2](https://insum.ca/saml2-single-sign-on-with-oracle-application-express/), making it possible for users to seamlessly move between Google G Suite/Microsoft Office 365 environments and corporate APEX applications.\\n\\n### Quick SQL\\n\\n> Available since version 5.1\\n\\n[Quick SQL](https://apex.oracle.com/en/quicksql/) was first made available in APEX 5.1 as a packaged application. Packaged applications are fully functional applications that you can install in APEX workspaces. These applications come pre-packed with installation scripts and seed data. I digress. When version 18.1 was released, Quick SQL became a feature and is accessed during the process of creating a new application or through the SQL Scripts utility.\\n\\nUsing syntax similar to Markdown, users/developers can use a shorthand to script out a data model and then generate the SQL code needed to create these database objects.\\n\\nHere\'s a quick example:\\n\\n```markdown\\n# drop: true\\nperson /auditcols\\n  personid number /pk\\n  firstname string\\n  lastname string\\n  birthdate date\\n  height_cm number\\n  car /auditcols\\n    carid number /pk\\n    make string\\n    model string\\n    engine_capacity number\\n```\\n\\nAnd what it looks like in SQL:\\n\\n```sql\\n-- drop objects\\ndrop table person cascade constraints;\\ndrop table car cascade constraints;\\n\\n-- create tables\\ncreate table person (\\n    personid                       number not null constraint person_personid_pk primary key,\\n    firstname                      varchar2(255),\\n    lastname                       varchar2(255),\\n    birthdate                      date,\\n    height_cm                      number,\\n    created                        date not null,\\n    created_by                     varchar2(255) not null,\\n    updated                        date not null,\\n    updated_by                     varchar2(255) not null\\n)\\n;\\n\\ncreate table car (\\n    person_id                      number\\n                                   constraint car_person_id_fk\\n                                   references person on delete cascade,\\n    carid                          number not null constraint car_carid_pk primary key,\\n    make                           varchar2(4000),\\n    model                          varchar2(4000),\\n    engine_capacity                number,\\n    created                        date not null,\\n    created_by                     varchar2(255) not null,\\n    updated                        date not null,\\n    updated_by                     varchar2(255) not null\\n)\\n;\\n\\n-- triggers\\ncreate or replace trigger person_biu\\n    before insert or update\\n    on person\\n    for each row\\nbegin\\n    if :new.personid is null then\\n        :new.personid := to_number(sys_guid(), \'XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\');\\n    end if;\\n    if inserting then\\n        :new.created := sysdate;\\n        :new.created_by := nvl(sys_context(\'APEX$SESSION\',\'APP_USER\'),user);\\n    end if;\\n    :new.updated := sysdate;\\n    :new.updated_by := nvl(sys_context(\'APEX$SESSION\',\'APP_USER\'),user);\\nend person_biu;\\n/\\n\\ncreate or replace trigger car_biu\\n    before insert or update\\n    on car\\n    for each row\\nbegin\\n    if :new.carid is null then\\n        :new.carid := to_number(sys_guid(), \'XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\');\\n    end if;\\n    if inserting then\\n        :new.created := sysdate;\\n        :new.created_by := nvl(sys_context(\'APEX$SESSION\',\'APP_USER\'),user);\\n    end if;\\n    :new.updated := sysdate;\\n    :new.updated_by := nvl(sys_context(\'APEX$SESSION\',\'APP_USER\'),user);\\nend car_biu;\\n/\\n\\n-- indexes\\ncreate index car_i1 on car (person_id);\\n-- load data\\n```\\n\\nCheck out the video below that demonstrates how this is done with Quick SQL:\\n\\n<youtube youTubeId=\\"BCs2jWkdVFg\\" />\\n\\n### Web Source Modules\\n\\n> Available since version 18.1\\n\\nAPEX has had PL/SQL packages ([`apex_web_service`](https://docs.oracle.com/database/apex-18.1/AEAPI/APEX_WEB_SERVICE.htm)) to work with external REST APIs for a while now, but the new [Web Source Modules](https://docs.oracle.com/database/apex-18.1/HTMDB/managing-web-source-modules.htm#HTMDB-GUID-93D7A596-03A9-48AF-9FC9-6538BCC25DED) (WSM) feature found in version 18.1 is a game-changer. Power users and developers configure credentials and WSM references with simply the knowledge of what URLs and parameters to use. Once created, reports can be created using these WSMs as source. The framework performs any necessary authentication, REST calls, result parsing and then renders them as though they were database tables or views.\\n\\nThis feature allows developers to easily integrate and interact with external data available through REST APIs, e.g. [Bioontology](http://data.bioontology.org/documentation) and [MailChimp](https://developer.mailchimp.com/).\\n\\n## What\'s Missing\\n\\nAPEX is great, but it\'s a general purpose tool. One that we can use to create and support a wide variety of business use cases. As a platform for supporting research, here are some areas where it falls a **little** short. And I emphasised \\"little\\" because there are workarounds.\\n\\n### Validated Instruments\\n\\nREDCap maintains a [library/collection](https://projectredcap.org/resources/library/) of validated data collection instruments. These are pre-baked forms and data structures that have been used in research and peer reviewed in publications. Researchers could use these and avoid reinventing the wheel.\\n\\nWe don\'t have access to such a resource for APEX **yet**. However, with Package Applications, I am confident that a similar repository of research-focused package apps could be made available to the community. If this interests you, then ping me. Let\'s talk.\\n\\n### De-identification\\n\\nDesigned for clinical research, the REDCap Consortium has put much effort in ensuring that the software meets regulatory compliance and patient privacy is protected. Hence, you will find built-in data de-identification tools and are commonly used during data exports. APEX does not facilitate de-identification out-of-the-box. While developers can implement filters that can do the same during data exports, the preferred approach is to use features such as Data Redaction and Data Masking. These unfortunately, are paid options on top of the Enterprise Edition of the database.\\n\\n\x3c!-- ## APEX in Academic Research [<i style=\\"float:right; opacity:0.5;\\" class=\\"fa fa-toggle-up\\"></i>](#Table-of-Contents) --\x3e\\n\\nBelow is an organic list of academic institutions that I have identified through various sources, that use APEX for research, provide an APEX platform and/or services to support research. A few institutions actively use APEX administratively, but to keep the discussion in context, I opted to leave them out for now. They are however, primed to deliver APEX services for the research activities, if any, within their organization.\\n\\n| Institution/Organization | Country | Source | Date Added |\\n|-|-|-|-|\\n| [Centre for Heart Lung Innovation](https://www.hli.ubc.ca) | Canada | Personal knowledge | 2018-09-03 |\\n| [BC Children\'s Hospital Research Institute](https://bcchr.ca/) | Canada | Personal knowledge | 2018-09-03 |\\n| [BC Centre for Excellence](http://www.cfenet.ubc.ca) | Canada | [Job posting](http://www.cfenet.ubc.ca/join-us/careers) | 2018-09-03 |\\n| Indiana University | United States | [Press release](http://www.oracle.com/us/corporate/press/367029) | 2018-09-03 |\\n| [Case Western Reserve University](https://case.edu/) | United States | [Resource listing](https://case.edu/utech/help/knowledge-base/research-database-services) | 2018-09-03 |\\n| [Institute for Computer Biology](http://www.icompbio.net) | United States | [Resource listing](http://www.icompbio.net/oracle-apex/) | 2018-09-03 |\\n| [NASA Ames Research Center](https://www.nasa.gov/ames) | United States | [Presentation/Publication](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6979560) | 2018-09-03 |\\n| [Commonwealth Scientific and Industrial Research Organisation](https://www.csiro.au/) | Australia | [Resource listing](https://research.csiro.au/oa-idc/web-development/) | 2018-09-03 |\\n\\nGoogle searches only take you that far, so this is by no means a comprehensive lay of the land. If you are part of a research organization that is actively using and/or offering APEX as a development platform and service, please reach out to me.\\n\\n## Final Word\\n\\nI had intentionally left out several institutions of higher education out of the list above, as I wanted to focus solely on the use of APEX in academic research. In reality, many institutions have a site licenses and/or significant discounts for using the Oracle Database. If you are endowed with such resources, please reach out to the IT folks and do the right thing. Stop storing and hording data on spreadsheets and file-based, siloed relational databases. Secure them in enterprise-grade software and take data collection to the next level with advanced, web-based forms."},{"id":"id/1713","metadata":{"permalink":"/content/id/1713","source":"@site/posts/snap-in-storage-for-your-docker-playground/index.md","title":"Snap in Storage For Your Docker Playground","description":"Floppy Disks","date":"2018-07-11T23:24:31.000Z","formattedDate":"July 11, 2018","tags":[{"label":"docker","permalink":"/content/tags/docker"},{"label":"oracle","permalink":"/content/tags/oracle"},{"label":"devops","permalink":"/content/tags/devops"},{"label":"orclapex","permalink":"/content/tags/orclapex"},{"label":"environment","permalink":"/content/tags/environment"}],"readingTime":3.59,"hasTruncateMarker":true,"authors":[{"name":"Adrian Png","title":"Senior Cloud Solutions Architect @ Insum","url":"https://github.com/fuzziebrain","imageURL":"https://github.com/fuzziebrain.png","key":"fuzziebrain"}],"frontMatter":{"title":"Snap in Storage For Your Docker Playground","tags":["docker","oracle","devops","orclapex","environment"],"slug":"id/1713","authors":["fuzziebrain"],"categories":["Technology","DevOps"],"date":"2018-07-11T23:24:31.000Z"},"prevItem":{"title":"Research-Ready","permalink":"/content/id/1714"},"nextItem":{"title":"A Kscope to Remember","permalink":"/content/id/1712"}},"content":"![Floppy Disks](./assets/black-18320_1920.jpg)\\n\\nICYMI (in case you missed it), a team from [Oracle](https://www.oracle.com) has been hard at work these few months churning out a bunch of [Vagrant](https://www.vagrantup.com) configuration files and scripts that help you provision some really cool virtual machines (VM), powered by [Oracle Enterprise Linux](https://www.oracle.com/linux/). At the time of writing, here are some of the cool VMs you can build:\\n\\n\x3c!-- truncate --\x3e\\n\\n* Vanilla Oracle Linux\\n* Oracle Database\\n* LAMP stack\\n* Docker Container Registry\\n* Docker Host\\n* Kubernetes\\n\\nMake sure you \\"star\\" or \\"watch\\" their [repository](https://github.com/oracle/vagrant-boxes) for the latest updates.\\n\\nI was recently tasked to develop a deployment guide using [Docker](https://www.docker.com) for creating [Oracle Application Express](https://apex.oracle.com) (APEX) on the [Oracle Cloud Infrastructure](https://cloud.oracle.com). To setup my working environment, I decided to deploy an Oracle Database instance using the [Docker images](https://github.com/oracle/docker-images) that Oracle has also open sourced previously. However, during that process, I encountered the dreaded:\\n\\n{% codeblock lang:plain line_number:false %}\\ncheckSpace.sh: ERROR - There is not enough space available in the docker container.\\ncheckSpace.sh: The container needs at least 15 GB, but only 4 GB are available.\\n{% endcodeblock %}\\n\\nMy first thought was to set a larger container base size, but the VM was configured to use [Btrfs](https://btrfs.wiki.kernel.org/) as the storage driver, so that clearly wasn\'t the issue. A quick check on the filesystem quickly revealed what the root problem was: **free disk space**!\\n\\n{% codeblock lang:bash line_number:false %}\\n$ df -h\\nFilesystem                   Size  Used Avail Use% Mounted on\\ndevtmpfs                     854M     0  854M   0% /dev\\ntmpfs                        871M     0  871M   0% /dev/shm\\ntmpfs                        871M   17M  855M   2% /run\\ntmpfs                        871M     0  871M   0% /sys/fs/cgroup\\n/dev/mapper/vg_main-lv_root   32G  5.2G   27G  17% /\\n/dev/sda1                    497M   99M  398M  20% /boot\\nvagrant                      941G  172G  770G  19% /vagrant\\nvagrant_share                941G  172G  770G  19% /vagrant_share\\n/dev/sdb                      16G  3.4G   11G  25% /var/lib/docker\\ntmpfs                        175M     0  175M   0% /run/user/1000\\n{% endcodeblock %}\\n\\n![VM\'s Storage Devices](./assets/vm-storage-config.jpg)\\n\\nThe base OEL vagrant box downloaded came with two attached virtual disk. The second, was about 16 GB and was available to the system as `/dev/sdb`. This device is then used for the storage device for Docker containers.\\n\\nSee [`scripts/install.sh`](https://github.com/oracle/vagrant-boxes/blob/master/DockerEngine/scripts/install.sh), line 21:\\n\\n{% codeblock lang:bash line_number:false %}\\ndocker-storage-config -s btrfs -d /dev/sdb\\n{% endcodeblock %}\\n\\nTo solve this issue, I needed to increase the amount of storage available to Docker. Fortunately for Btrfs, this is pretty easy to do. \\n\\n1. Shutdown the VM (using the Vagrant command to halt the system) so that I could add another 16 GB virtual disk.\\n\\n![Virtual disk OL7U5_x86_64-disk3.vmdk added](./assets/vm-storage-config-new.jpg)\\n\\n2. Power on the VM.\\n3. Check that the device can be \\"seen\\" by the operating system (OS):\\n\\n{% codeblock lang:bash line_number:false %}\\n$ lsblk\\nNAME                MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT\\nsda                   8:0    0 36.5G  0 disk\\n\u251c\u2500sda1                8:1    0  500M  0 part /boot\\n\u2514\u2500sda2                8:2    0   36G  0 part\\n  \u251c\u2500vg_main-lv_root 249:0    0   32G  0 lvm  /\\n  \u2514\u2500vg_main-lv_swap 249:1    0    4G  0 lvm  [SWAP]\\nsdb                   8:16   0 15.6G  0 disk /var/lib/docker\\nsdc                   8:32   0   16G  0 disk\\n{% endcodeblock %}\\n\\n4. Add the new device to the Btrfs volume:\\n\\n{% codeblock lang:bash line_number:false %}\\n$ sudo btrfs device add /dev/sdc /var/lib/docker\\n{% endcodeblock %}\\n\\n5. Attempt to spread the data evenly ([balance](https://btrfs.wiki.kernel.org/index.php/FAQ#What_does_.22balance.22_do.3F)) across all devices:\\n\\n{% codeblock lang:bash line_number:false %}\\n$ sudo btrfs filesystem balance /var/lib/docker\\nWARNING:\\n\\n        Full balance without filters requested. This operation is very\\n        intense and takes potentially very long. It is recommended to\\n        use the balance filters to narrow down the balanced data.\\n        Use \'btrfs balance start --full-balance\' option to skip this\\n        warning. The operation will start in 10 seconds.\\n        Use Ctrl-C to stop it.\\n10 9 8 7 6 5 4 3 2 1\\nStarting balance without any filters.\\nDone, had to relocate 12 out of 12 chunks\\n{% endcodeblock %}\\n\\nFinally, check all went well and that we now have sufficient space to build our very own Oracle Database container.\\n\\n{% codeblock lang:bash line_number:false %}\\n$ df -h\\nFilesystem                   Size  Used Avail Use% Mounted on\\ndevtmpfs                     854M     0  854M   0% /dev\\ntmpfs                        871M     0  871M   0% /dev/shm\\ntmpfs                        871M   17M  855M   2% /run\\ntmpfs                        871M     0  871M   0% /sys/fs/cgroup\\n/dev/mapper/vg_main-lv_root   32G  5.2G   27G  17% /\\n/dev/sdb                      32G  3.4G   28G  11% /var/lib/docker\\n/dev/sda1                    497M   99M  398M  20% /boot\\nvagrant                      941G  175G  767G  19% /vagrant\\nvagrant_share                941G  175G  767G  19% /vagrant_share\\ntmpfs                        175M     0  175M   0% /run/user/1000\\n{% endcodeblock %}"},{"id":"id/1712","metadata":{"permalink":"/content/id/1712","source":"@site/posts/a-kscope-to-remember/index.md","title":"A Kscope to Remember","description":"Where Dreams CAME True","date":"2018-06-16T23:47:13.000Z","formattedDate":"June 16, 2018","tags":[{"label":"travel","permalink":"/content/tags/travel"},{"label":"orclapex","permalink":"/content/tags/orclapex"},{"label":"odtug","permalink":"/content/tags/odtug"},{"label":"disney","permalink":"/content/tags/disney"},{"label":"orlando","permalink":"/content/tags/orlando"},{"label":"learning","permalink":"/content/tags/learning"},{"label":"training","permalink":"/content/tags/training"},{"label":"community","permalink":"/content/tags/community"}],"readingTime":6.54,"hasTruncateMarker":true,"authors":[{"name":"Adrian Png","title":"Senior Cloud Solutions Architect @ Insum","url":"https://github.com/fuzziebrain","imageURL":"https://github.com/fuzziebrain.png","key":"fuzziebrain"}],"frontMatter":{"title":"A Kscope to Remember","tags":["travel","orclapex","odtug","disney","orlando","learning","training","community"],"slug":"id/1712","authors":["fuzziebrain"],"categories":["Technology","Conferences"],"date":"2018-06-16T23:47:13.000Z"},"prevItem":{"title":"Snap in Storage For Your Docker Playground","permalink":"/content/id/1713"},"nextItem":{"title":"APEX Social Sign-In Without a Wallet","permalink":"/content/id/1711"}},"content":"![Where Dreams CAME True](./assets/where-dreams-come-true_1280x960.jpg)\\n\\nThat\'s right! Here\'s the customary post-Kscope article. This year though, is one Kscope to remember! Along with other [Oracle Application Express](https://apex.oracle.com) (APEX) community members, I had the surprise honour of joining the ranks of the [Oracle ACE Program](http://www.oracle.com/technetwork/community/oracle-ace). And so I wanted to start off by first thanking newly-minted Oracle ACE Director [Opal Alapat](https://twitter.com/opal_EPM). Last year, I had deep suspicions that she was responsible for my ODTUG Volunteer Award. Then came the ACE Associate nomination.\\n\\n> Opal, if you are reading this: **THANK YOU**!\\n\\n\x3c!-- truncate --\x3e\\n\\n## Content and Events\\n\\nI was a little late getting to the conference again this year. I opted for a red-eye that took me from Vancouver to Orlando via Toronto, and arrived after the first Sunday Symposium. Unfortunately, I missed out on important news regarding the impending Oracle XE 18c release slated for August 2018. Thankfully, my watch buddy [Oliver Lemm](https://twitter.com/OliverLemm) captured and tweeted this slide that detailed the features of 2018\'s most-anticipated database.\\n\\n<blockquote class=\\"twitter-tweet\\" data-lang=\\"en\\"><p lang=\\"en\\" dir=\\"ltr\\">Some keyfacts about <a href=\\"https://twitter.com/hashtag/oraclexe?src=hash&amp;ref_src=twsrc%5Etfw\\">#oraclexe</a> by <a href=\\"https://twitter.com/mikehichwa1?ref_src=twsrc%5Etfw\\">@mikehichwa1</a> <a href=\\"https://twitter.com/hashtag/kscope18?src=hash&amp;ref_src=twsrc%5Etfw\\">#kscope18</a> <a href=\\"https://twitter.com/hashtag/orclapex?src=hash&amp;ref_src=twsrc%5Etfw\\">#orclapex</a> <a href=\\"https://twitter.com/hashtag/oracle?src=hash&amp;ref_src=twsrc%5Etfw\\">#oracle</a> <a href=\\"https://t.co/iqAX08ac7A\\">pic.twitter.com/iqAX08ac7A</a></p>&mdash; Oliver Lemm (@OliverLemm) <a href=\\"https://twitter.com/OliverLemm/status/1005805541086593025?ref_src=twsrc%5Etfw\\">June 10, 2018</a></blockquote>\\n<script async src=\\"https://platform.twitter.com/widgets.js\\" charset=\\"utf-8\\"><\/script>\\n\\nIf you asked me, that\'s a delicious bait that anyone would swallow whole, hook and all!\\n\\nThere were other key announcements made during [Joel Kallman](https://twitter.com/joelkallman)\'s \\"State of the Union\\" address:\\n\\n* [APEX Office Hours](https://apex.oracle.com/officehours)\\n* Updated APEX Certification; and\\n* APEX MOOC\\n\\nMore importantly though, my dear friend [Juergen Schuster](https://twitter.com/JuergenSchuster) was finally given the recognition he deserved! This man has devoted so much time and money into the APEX community, and brought everyone together from all corners of the earth. He took the community that the APEX DEV Team created and placed it on steriods! Please do read what Joel had to say about this wonderful man.\\n\\n<blockquote class=\\"twitter-tweet\\" data-lang=\\"en\\"><p lang=\\"da\\" dir=\\"ltr\\">J\xfcrgen Schuster: APEX Distinguished Community Member<a href=\\"https://twitter.com/hashtag/orclapex?src=hash&amp;ref_src=twsrc%5Etfw\\">#orclapex</a><a href=\\"https://t.co/yooqzmtRoQ\\">https://t.co/yooqzmtRoQ</a> <a href=\\"https://t.co/P4SQHzXGwb\\">pic.twitter.com/P4SQHzXGwb</a></p>&mdash; Joel R. Kallman (@joelkallman) <a href=\\"https://twitter.com/joelkallman/status/1007765438971904006?ref_src=twsrc%5Etfw\\">June 15, 2018</a></blockquote>\\n<script async src=\\"https://platform.twitter.com/widgets.js\\" charset=\\"utf-8\\"><\/script>\\n\\nSince I joined [Insum](https://insum.ca) and started presenting more regularly at Kscope, I have found less time for attending sessions. Apart from quiet times to psych and prep myself, I felt a need to be talking to people both at and away from the booth. For those sessions that I wanted to attend, there were always conflicts and decisions to be made until I can master the art of bilocation. Here are the ones that I did manage to attend this year:\\n\\n* Introduction to Hyperledger\\n* JavaScript and Offline First\\n* Exploring the Details of APEX Sessions\\n* Cross Workspace Application Authentication Using JSON Web Tokens\\n* Emerging Tech Unconference Session\\n* Automatically Deploy Schema Changes with Ansible and Liquibase\\n* Developing Secure Applications\\n* Deep Dive: [#LetsWreckThisTogether](https://twitter.com/hashtag/LetsWreckThisTogether) APEX Talks\\n\\n![Hunting Pokemons During Dimitri Gielis&apos; Emerging Tech Unconference Demo](./assets/emerging-tech_1280x960.jpg)\\n\\nThere were too many sessions that I really wanted to attend, but I guess I\'ll have to settle for the recordings when they are released later this year.\\n\\n## Presentation\\n\\n![I Am All About APEX and Blockchains](./assets/presenter-panel_1280x960.jpg)\\n\\nBlockchains took centre stage at Kscope this year for many reasons. In the past year, we have seen the great surge and grand fall of cryptocurrency prices. The blockchain buzzword appeared in nearly every tech news feed and many were capitalizing on the keyword, me included. Though what I really wanted to do was help APEX developers make sense of the frenzy and access for themselves if learning this emerging tech was a worthwhile pursuit. It is!\\n\\nIt was great that I was scheduled early this year as it took a big load off my mind. That said, I wondered if the audience would have benefited from first attending the introductory sessions by John King and Emmanuel Abiodun on blockchain and [Hyperledger](https://www.hyperledger.org/) respectively. Being on the initial line up, I had to make sure that I covered sufficient basics to provide the needed context for the two demos that I had prepared. Unfortunately, I ran out of time and was not able to do the second demo that involved [Hyperledger Fabric](https://www.hyperledger.org/projects/fabric) and a walkthrough creating a business network using [Hyperledger Composer](https://www.hyperledger.org/projects/composer).\\n\\nFor those who attended my session, thank you and my sincere apologies again for running out of time. Rest assured that I will be blogging more about the topic and demos, so that we can all continue learning and growing. Stay tuned!\\n\\n## People\\n\\n[Monty Latiolais](https://twitter.com/monty_latiolais) tells me that travelling to Kscope for him, is an annual reunion with friends. I agree wholeheartedly. Many of us interact almost exclusively through various Internet channels, and rarely have the chance to meet face-to-face. As a remote worker, this is true even amongst colleagues. Strangely though, you meet people, shake hands and share hugs as though you\'ve known them for ages.\\n\\nCongregating at tech conferences are also great opportunities to learn more about our peers outside of our profession. Here are some highlights:\\n\\n* American politics with Monty\\n* Canadian politics with [Martin D\'Souza](https://twitter.com/martindsouza)\\n* Kids 101 and growing up with [John Brock](https://twitter.com/peppertech), Martin and [Dan McGhan](https://twitter.com/dmcghan)\\n\\nThen there are the customers whom you\'ve been on conference calls with. Folks from other tracks, e.g. EPM, who wanted to know more about the APEX magic. The list goes on.\\n\\nThank you everyone, new and old friends, for the wonderful memories and conversations!\\n\\n## The Award\\n\\n![Badges and Ribbons](./assets/badge_1280x960.jpg)\\n\\nFresh out of university and sucked into the dotcom boom, I attended annual events like Sun Tech Days and Oracle Developer Days in Singapore. I remember how much I had admired and envied Sun Evangelists like [Sang Shin](https://www.linkedin.com/in/javapassion/), [Chuk-Munn Lee](https://www.linkedin.com/in/chukmunnlee/), [Simon Ritter](https://twitter.com/speakjava) and many others. They had the best job in the world: learning and toying with new tech and then travelling the globe to share their knowledge. I could only dream!\\n\\nAlmost twenty years later, I am a little closer to living that dream. Though it isn\'t a full-time job, receiving the Oracle ACE Associate award, means that I am now obliged to learn more and share more. How cool is that?\\n\\n<blockquote class=\\"twitter-tweet\\" data-lang=\\"en\\"><p lang=\\"en\\" dir=\\"ltr\\">More pics to come.... we love our <a href=\\"https://twitter.com/oracleace?ref_src=twsrc%5Etfw\\">@oracleace</a> members! <a href=\\"https://twitter.com/hashtag/Kscope18?src=hash&amp;ref_src=twsrc%5Etfw\\">#Kscope18</a> <a href=\\"https://t.co/vDXxTVfC9a\\">pic.twitter.com/vDXxTVfC9a</a></p>&mdash; Oracle ACE Program (@oracleace) <a href=\\"https://twitter.com/oracleace/status/1006703306037743617?ref_src=twsrc%5Etfw\\">June 13, 2018</a></blockquote>\\n<script async src=\\"https://platform.twitter.com/widgets.js\\" charset=\\"utf-8\\"><\/script>\\n\\nI won\'t deny that I am nervous and feel really small standing amongst giants and some of the brightest minds in the industry. I will have to carve a niche for myself and work much harder to grow in knowledge, writing and presentation skills. Fortunately for me though, I have many mentors at Insum.\\n\\n## What\'s Next?\\n\\nJavaScript seems to be taking on an increasing presence within the APEX track. We had a dedicated sub-topic for JS sessions and the closing Deep Dive session on Thursday was all about getting started and best practices with JS. Should we pay attention as APEX developers? Most definitely yes!\\n\\n![Netscape Navigator - The Old Days](./assets/Netscape_Navigator_2_Screenshot.jpg)\\n\\nI never liked JS much in the past. Growing up in the industry, it was a \\"kiddy\\" scripting language that lacked good structure, standards and compatibility across web browsers. All that has changed. For the last two years, I\'ve been rekindling my interest in JS through learning frameworks and tools such as TypeScript, Angular, Vue.js and Node.js, and will continue to do so.\\n\\nNext year, Kscope returns to Seattle and I am very much looking forward to it. To prove that, here\'s what I have started scheming for [Kscope19](https://kscope19.odtug.com):\\n\\n* Abstracts - I have two in mind and will pen them shortly before they fade from my memory. One or two more to go.\\n* Outreach - IMHO, we don\'t have a very enthusiastic Oracle fan club out here in the Pacific Northwest, and so, I\'m looking forward to opportunities for the [BCOUG](https://bcoug.org) to take an active role in drumming up interest and excitement for the June 2019 event.\\n\\nSee you next year!"},{"id":"id/1711","metadata":{"permalink":"/content/id/1711","source":"@site/posts/apex-social-sign-in-without-a-wallet/index.md","title":"APEX Social Sign-In Without a Wallet","description":"Social Media","date":"2018-05-26T04:36:44.000Z","formattedDate":"May 26, 2018","tags":[{"label":"orclapex","permalink":"/content/tags/orclapex"},{"label":"oauth2","permalink":"/content/tags/oauth-2"},{"label":"authentication","permalink":"/content/tags/authentication"},{"label":"google","permalink":"/content/tags/google"},{"label":"microsoft","permalink":"/content/tags/microsoft"},{"label":"sso","permalink":"/content/tags/sso"},{"label":"single sign-on","permalink":"/content/tags/single-sign-on"}],"readingTime":3.39,"hasTruncateMarker":true,"authors":[{"name":"Adrian Png","title":"Senior Cloud Solutions Architect @ Insum","url":"https://github.com/fuzziebrain","imageURL":"https://github.com/fuzziebrain.png","key":"fuzziebrain"}],"frontMatter":{"title":"APEX Social Sign-In Without a Wallet","tags":["orclapex","oauth2","authentication","google","microsoft","sso","single sign-on"],"slug":"id/1711","authors":["fuzziebrain"],"categories":["Technology","Programming","Oracle Application Express"],"date":"2018-05-26T04:36:44.000Z"},"prevItem":{"title":"A Kscope to Remember","permalink":"/content/id/1712"},"nextItem":{"title":"Just a Drop of Oracle APEX","permalink":"/content/id/1710"}},"content":"![Social Media](./assets/media-998990_1280x720.jpg)\\n\\n> **Updated June 4, 2018**\\n> * Added settings for social sign-in with Microsoft OAuth2.\\n\\nThe much anticipated [Oracle Application Express](https://apex.oracle.com) version 18.1 was [announced](https://blogs.oracle.com/apex/announcing-oracle-apex-181) on May 24, 2018. Since early this year [Monty Latiolais](https://twitter.com/monty_latiolais) has been posting a monthly series of blog posts on [Power Ranking Oracle APEX 18.1 New Features](https://www.insum.ca/power-ranking-oracle-apex-18-1-new-features-march/). My personal favourite is Social Login, but unfortunately it never made it to the top, even though it should!\\n\\n\x3c!-- truncate --\x3e\\n\\nWhen 4.2 was released, I was ecstatic. The introduction of the HTTP Header Variable authentication scheme allowed me to convince my boss back then, that APEX was the way to go. One critical success factor was the ease in integrating APEX application with the wider enterprise authentication framework that has already been established. However, you typically had to have a certain system architecture to capitalize on this authentication scheme.\\n\\nWith the newly released [Social Sign-in Authentication Scheme](https://docs.oracle.com/database/apex-18.1/HTMDB/establishing-user-identity-through-authentication.htm#GUID-CF10F6D7-768C-42F9-9362-4CF23BF1C4B8), just about anyone can utilize the countless social platforms that provide OAuth2 logins.\\n\\nAs you might expected, it didn\'t take long for me to upgrade my personal APEX instance that runs on top of Oracle Database 11g Express Edition (XE) that is too many years out of date now. The authentication scheme makes calls to identity providers using the [`APEX_WEB_SERVICE`](https://docs.oracle.com/database/apex-18.1/AEAPI/APEX_WEB_SERVICE.htm#AEAPI537) package and usually, that\'s over HTTPS. So yes, you will need to load up your Oracle wallets with the certificate chains. Read [Carsten Czarski\'s](https://twitter.com/cczarski) excellent explanation [here](https://blogs.oracle.com/apex/apex-https-certificates-and-the-oracle-wallet). You can get these from your browser as Carsten describes, or directly from the source such as DigiCert\'s [list](https://www.digicert.com/digicert-root-certificates.htm) of Root and Intermediate Certificates. However, the deal breaker here is that there is no convenient way to do this in XE. Even if you do successfully [create a wallet for XE](https://blog.hazrulnizam.com/openssl-workaround-oracle-xe-wallet/), being so old and unpatched, it will still have difficulty negotiating with modern day certificates. \\n\\nThank goodness for reverse proxies! There are many ways to do this, but for me, I use [Suez](/content/id/1706/).\\n\\n> **IMPORTANT SECURITY NOTE:** \\n> Even though you can bypass SSL checks, doesn\'t mean that you should. End-to-end encryption is always preferred, especially when dealing with confidential information.\\n\\n## Google\\n\\nGoogle and Facebook sign-ins are pre-configured for use, but if you are using a \\"reverse-proxied\\" approach, then it might not be what you want. At least for Google sign-in, here are my configurations and setup:\\n\\n1. Add the following to the Suez configuration:\\n\\n```json\\n{\\n    \\"service\\": {\\n        \\"port\\": 3001\\n    },\\n    \\"apiTargets\\": [\\n        {\\n            \\"name\\": \\"googleapis\\",\\n            \\"proxyTarget\\": \\"https://www.googleapis.com\\"\\n        }\\n]\\n}\\n```\\n\\n2. Ensure that `googleapis.localhost` resolves to 127.0.0.1.\\n\\n3. Follow my [previous guide](/content/id/1709/) on setting up Google Sign-in. The key difference is not to use the pre-configured Google option, but instead, set up a Generic OAuth2 Provider using the following settings:\\n    * **Authorization Endpoint URL**: https://accounts.google.com/o/oauth2/v2/auth\\n    * **Token Endpoint URL**: http://googleapis.localhost:3001/oauth2/v4/token\\n    * **User Info Endpoint URL**: http://googleapis.localhost:3001/oauth2/v2/userinfo\\n\\n![My Social Sign-in Settings for Google](./assets/authscheme-settings.png)\\n\\n## Microsoft\\n\\n1. Add the following to the Suez configuration:\\n\\n```json\\n{\\n    \\"service\\": {\\n        \\"port\\": 3001\\n    },\\n    \\"apiTargets\\": [\\n        {\\n      \\"name\\": \\"loginmicrosoftonline\\",\\n      \\"proxyTarget\\": \\"https://login.microsoftonline.com\\"\\n    },\\n    {\\n      \\"name\\": \\"graphmicrosoft\\",\\n      \\"proxyTarget\\": \\"https://graph.microsoft.com\\"\\n    }\\n]\\n}\\n```\\n\\n2. Ensure that `loginmicrosoftonline.localhost` and `graphmicrosoft.localhost` resolves to 127.0.0.1.\\n\\n3. Go to Microsoft\'s [Application Registration Portal](https://apps.dev.microsoft.com/). Add an application under \\"Converged applications\\".\\n\\n4. Enter the application name and the following information:\\n    * **Redirect URLs**: `https://my.app/ords/apex_authentication.callback`\\n    * **Home page URL**: `https://my.app/ords/f?p=myapp`\\n    * The remaining fields can be left at the defaults, but make sure that the delegated permission `User.Read` has been added under \\"Microsoft Graph Permissions\\".\\n\\n5. Under \\"Application Secrets\\", generate a new password and note both the Application Id and Password. \\n\\n6. Save the application registration information.\\n\\n7. Create the web credential using the Application Id and Password obtained from the registration portal.\\n\\n8. Create the authentication scheme using a Generic OAuth2 Provider with the following settings:\\n    * **Authorization Endpoint URL**: https://login.microsoftonline.com/common/oauth2/v2.0/authorize\\n    * **Token Endpoint URL**: http://loginmicrosoftonline.localhost:3001/common/oauth2/v2.0/token\\n    * **User Info Endpoint URL**: http://graphmicrosoft.localhost:3001/v1.0/me\\n    * **Scope**: User.Read\\n    * **Username Attribute**: userPrincipalName\\n\\n## Conclusion\\n\\nOnce you\'ve established what server URLs are called, this approach is easily replicated for other providers. I will update this post as I learn more."},{"id":"id/1710","metadata":{"permalink":"/content/id/1710","source":"@site/posts/just-a-drop-of-oracle-apex/index.md","title":"Just a Drop of Oracle APEX","description":"Photo by Johnny Brown","date":"2018-02-12T20:02:24.000Z","formattedDate":"February 12, 2018","tags":[{"label":"orclapex","permalink":"/content/tags/orclapex"},{"label":"oxar","permalink":"/content/tags/oxar"},{"label":"cloud","permalink":"/content/tags/cloud"},{"label":"hosting","permalink":"/content/tags/hosting"},{"label":"vagrant","permalink":"/content/tags/vagrant"},{"label":"devops","permalink":"/content/tags/devops"},{"label":"digitalocean","permalink":"/content/tags/digitalocean"}],"readingTime":0.97,"hasTruncateMarker":true,"authors":[{"name":"Adrian Png","title":"Senior Cloud Solutions Architect @ Insum","url":"https://github.com/fuzziebrain","imageURL":"https://github.com/fuzziebrain.png","key":"fuzziebrain"}],"frontMatter":{"title":"Just a Drop of Oracle APEX","tags":["orclapex","oxar","cloud","hosting","vagrant","devops","digitalocean"],"slug":"id/1710","authors":["fuzziebrain"],"categories":["Technology","Programming","Oracle Application Express"],"date":"2018-02-12T20:02:24.000Z"},"prevItem":{"title":"APEX Social Sign-In Without a Wallet","permalink":"/content/id/1711"},"nextItem":{"title":"Authenticate using OAuth2 - Now Available in APEX 5.2 EA","permalink":"/content/id/1709"}},"content":"![<a href=\\"https://unsplash.com/photos/V-afy242gY4\\" target=\\"_blank\\">Photo</a> by Johnny Brown](./assets/johnny-brown-1280x720.jpg)\\n\\nIn a previous [article](/content/id/cizx69jya000ra0pvv26u3cez), I had demonstrated how one could use [Vagrant](https://vagrantup.com) to deploy an [OXAR](https://github.com/OraOpenSource/OXAR) virtual machine directly to [Linode](https://www.linode.com/?r=41672b20d515344de465e9ed44c1a75356445597).\\n\\nWith [Digital Ocean](https://m.do.co/c/6f9b549ca569) (DO) lowering their prices to match Linode\'s, I thought it would help the community if I created a similar set of instructions to deploy OXAR on DO. I took this a step further and have provided a branch on the Github repository with a modified Vagrantfile.\\n\x3c!-- truncate --\x3e\\n\\n1. Clone the branch:\\n```bash\\n$ git clone -b vagrant/digitalocean https://github.com/OraOpenSource/OXAR.git\\n```\\n2. Install the DigitalOcean Vagrant Provider plugin:\\n```bash\\n$ vagrant plugin install vagrant-digitalocean\\n```\\n3. Make the installation files available over the web. For example, upload the files to Dropbox and create a shareable link.\\n4. Update the config.properties with the shareable link and any other customizations.\\n5. Generate a personal access token using the instructions available [here](https://www..com/community/tutorials/how-to-use-the-digitalocean-api-v2).\\n6. Update the Vagrantfile with your token and any Droplet customizations.\\n7. Make sure that you have your SSH keys generated, if not, follow the instructions [here](https://www.digitalocean.com/community/tutorials/how-to-set-up-ssh-keys--2).\\n7. Run Vagrant:\\n```bash\\n$ vagrant up\\n```\\n\\nIt takes about thirty minutes to provision the VM. Enjoy!"},{"id":"id/1709","metadata":{"permalink":"/content/id/1709","source":"@site/posts/authenticate-using-oauth2-now-available-in-apex-5-2-ea/index.md","title":"Authenticate using OAuth2 - Now Available in APEX 5.2 EA","description":"ICYMI, early access for Oracle Application Express was opened for workspace requests erlier this month. Like every other APEX fan out there, I was of course eager to get my hands dirty. Guess what was the first upcoming feature I took for a test drive?","date":"2018-01-01T00:52:10.000Z","formattedDate":"January 1, 2018","tags":[{"label":"orclapex","permalink":"/content/tags/orclapex"},{"label":"oauth2","permalink":"/content/tags/oauth-2"},{"label":"authentication","permalink":"/content/tags/authentication"},{"label":"preview","permalink":"/content/tags/preview"},{"label":"google","permalink":"/content/tags/google"},{"label":"single sign on","permalink":"/content/tags/single-sign-on"},{"label":"sso","permalink":"/content/tags/sso"}],"readingTime":3.665,"hasTruncateMarker":true,"authors":[{"name":"Adrian Png","title":"Senior Cloud Solutions Architect @ Insum","url":"https://github.com/fuzziebrain","imageURL":"https://github.com/fuzziebrain.png","key":"fuzziebrain"}],"frontMatter":{"title":"Authenticate using OAuth2 - Now Available in APEX 5.2 EA","tags":["orclapex","oauth2","authentication","preview","google","single sign on","sso"],"categories":["Technology","Programming","Oracle Application Express"],"slug":"id/1709","authors":["fuzziebrain"],"date":"2018-01-01T00:52:10.000Z"},"prevItem":{"title":"Just a Drop of Oracle APEX","permalink":"/content/id/1710"},"nextItem":{"title":"Write High Quality JavaScript Code for Oracle Application Express","permalink":"/content/id/1708"}},"content":"ICYMI, [early access for Oracle Application Express](https://apexea.oracle.com) was opened for workspace requests erlier this month. Like every other APEX fan out there, I was of course eager to get my hands dirty. Guess what was the first upcoming feature I took for a test drive?\\n\\n\x3c!-- truncate --\x3e\\n\\nYou may have read my recent my [blog post](https://www.insum.ca/saml2-single-sign-on-with-oracle-application-express/) on using SAML2 for enabling single sign-on with cloud platforms like Microsoft Office 365 and Google G Suite. Well, this new authentication scheme available out of the box might just make the prescribed method obsolete. If you\'re interested to try it out yourself, here\'s an account of what I needed to do to enable Google sign-on. Hopefully it will provide you some guidance.\\n\\n* The first thing you ought to do is to sign up for a Google account.\\n* Next, login to Google\'s Developers [console](https://console.developers.google.com/).\\n* Create a project in order to enable the APIs you need to perform the OAuth2 login process. Click `Create Project` (see figure 1).\\n\\n![Figure 1: Create a Project - step 1](./assets/create-project_01.jpg)\\n\\n* Click `Create` (see figure 2) to continue.\\n\\n![Figure 2: Create a Project - step 2](./assets/create-project_02.jpg)\\n\\n* Enter a project name and then click `Create` to complete the process\\n\\n![Figure 3: Create a Project - step 3](./assets/create-project_03.jpg)\\n\\n* Once the project is created, you will be returned to the console dashboard, and the new project should be selected automatically. You should see this at the top-left of the page, right next to the Google APIs page banner.\\n\\n![Figure 4: Console dashboard with project selected](./assets/create-project_04.jpg)\\n\\n* Click the third link `Credentials` and then select to create an `OAuth client ID` credential.\\n\\n![Figure 5: Create a OAuth client ID credential](./assets/create-project_05.jpg)\\n\\n* Next, click `Configure consent screen`.\\n\\n![Figure 6: Configure the consent screen first](./assets/create-project_06.jpg)\\n\\n* Enter a product name and then click `Save` on the bottom of the page.\\n\\n![Figure 7: Enter the product name](./assets/create-project_07.jpg)\\n\\n* When returned to the previous page, select `Web application`. Additional fields will be displayed.\\n* Enter the `Name` of the application and enter `https://apexea.oracle.com/pls/apex/apex_authentication.callback` under the list of `Authorized redirect URIs`.\\n* Click `Create`.\\n\\n![Figure 8: Create the client ID](./assets/create-project_08.jpg)\\n\\n* Take note of the `client ID` and `client secret` and then click `OK` to close the modal dialog.\\n* Return to the APEX App Builder and create an application.\\n* Go to `Shared Components` and then click `Credentials`.\\n\\n![Figure 9: Manage Credentials](./assets/setup-application_01.jpg)\\n\\n* Click `Create`.\\n\\n![Figure 10: Create a Credential](./assets/setup-application_02.jpg)\\n\\n* Enter a name for the credential you are about to store.\\n* Select the `OAuth2 Client Credentials Flow` for the `Authentication Type`.\\n* Enter the `Client ID` and `Client Secret` obtained from the Google Developers console.\\n* Click `Create`.\\n\\n![Figure 11: Setup the Credential](./assets/setup-application_03.jpg)\\n\\n* Create a new Authentication Scheme.\\n* Select `Social Sign-In` for the `Scheme Type`.\\n* Select the credential you created earlier and then `Google` for the `Authentication Provider`.\\n* Enter `profile,email` in the `Scope` field, and `email` for the `Username Attribute`.\\n\\n> The email address of the user will be username (a.k.a. `APP_USER`) in your APEX application.\\n\\n![Figure 12: Create the Social Sign-In authentication scheme](./assets/setup-application_04.jpg)\\n\\nRun the application. If everything was configured correctly, you will be prompted to login with your (any) Google Account.\\n\\n![Figure 13: Login with a Google Account](./assets/run-application_01.jpg)\\n\\n![Figure 14: Application Homepage](./assets/run-application_02.jpg)\\n\\nYou might observe that the application\'s logout link has no effect. However, take a closer look and you will see that the session identifier changes. Logging out only terminates your APEX session, but since your Google sign-on session is still valid, you are automatically authenticated back into the application. To ensure a proper logout, set the post-logout URL. For Google, this is `https://account.google.com/logout`.\\n\\n![Figure 15: Set the Post-Logout URL](./assets/post-logout.jpg)\\n\\nUnfortunately, Google does not allow post-logout, third-party site redirects, so users will need to re-enter your site\'s URL again to access the application.\\n\\nIn the absence of official documentation, this is my best guess on how to set this up. As we progress towards the final release, we should get better clarity on the correctness of this article.\\n\\nMeanwhile, **HAPPY NEW YEAR** and HAPPY AUTHENTICATING!\\n\\n> <u>UPDATE [May 4, 2018]</u>\\n> [apex.oracle.com](https://apex.oracle.com) was upgraded to APEX 18.1 (previously known as 5.2) last month. The instructions provided in this blog post is still relevant. Simply replace any links or references using `apexea` to `apex`. For enabling authentication with the Microsoft platform, please see Morten Braten\'s [post](https://ora-00001.blogspot.ca/2018/02/apex-authentication-with-microsoft-account.html)."},{"id":"id/1708","metadata":{"permalink":"/content/id/1708","source":"@site/posts/write-high-quality-javascript-code-for-oracle-application-express/index.md","title":"Write High Quality JavaScript Code for Oracle Application Express","description":"Old Handwritten Book by Kiwihug}","date":"2017-11-08T19:08:20.000Z","formattedDate":"November 8, 2017","tags":[{"label":"typescript","permalink":"/content/tags/typescript"},{"label":"orclapex","permalink":"/content/tags/orclapex"},{"label":"ide","permalink":"/content/tags/ide"},{"label":"quality","permalink":"/content/tags/quality"},{"label":"vscode","permalink":"/content/tags/vscode"},{"label":"atom","permalink":"/content/tags/atom"}],"readingTime":3.09,"hasTruncateMarker":true,"authors":[{"name":"Adrian Png","title":"Senior Cloud Solutions Architect @ Insum","url":"https://github.com/fuzziebrain","imageURL":"https://github.com/fuzziebrain.png","key":"fuzziebrain"}],"frontMatter":{"title":"Write High Quality JavaScript Code for Oracle Application Express","tags":["typescript","orclapex","ide","quality","vscode","atom"],"slug":"id/1708","authors":["fuzziebrain"],"categories":["Technology","Programming","Tools"],"date":"2017-11-08T19:08:20.000Z"},"prevItem":{"title":"Authenticate using OAuth2 - Now Available in APEX 5.2 EA","permalink":"/content/id/1709"},"nextItem":{"title":"Two Days of Node.js","permalink":"/content/id/1707"}},"content":"[Old Handwritten Book by Kiwihug}](./assets/kiwihug-393319_1280x720.jpg)\\n\x3c!-- source: https://unsplash.com/photos/5bzMOpMTDRM --\x3e\\n\\nIt\'s funny how JavaScript\'s 20th birthday was not as spectacular as it\'s shorter-named bretheren. When I first learned JavaScript in the late 1990\'s, I wasn\'t too excited as I was about Java, for two reasons most web developers know about JavaScript: weakly typed and non-standard browser implementation. The latter has been addressed by ECMA, but does that mean our code can now be more predictable and consistent?\\n\\n\x3c!-- truncate --\x3e\\n\\n[TypeScript](https://www.typescriptlang.org/) attempts to solve these issues by introducing static typing. This paves the way for a more structured language that results in better tooling and developer support. Think code hints. More importantly though, with compile-time error checking, we can be better assured of how the application would behave at runtime. And because we compile TypeScript to JavaScript, there is an opportunity to allow developers to work with modern ECMA specifications, but yet having the ability to compile down to earlier flavours of JS. All that without needing (not saying you shouldn\'t) to know the intricacies of transpilers and build management tools.\\n\\nTypeScript introduces the concept of a declaration file. These files are like header files in C and package specifications in PL/SQL. Declaration files are usually written in files with a `.d.ts` file extension. They do not compile, instead, simply describe the functions and variables available in a JS library. They assist in TypeScript code compilation, and with IDE support, developer productivity is enhanced with features like IntelliSense and type-checking. In Code, type-checking is also available in JS with a simple comment at the top of the file: `//@ts-check`.\\n\\nAn [Oracle Application Express](https://apex.oracle.com/) (APEX) developer is no stranger to tab-switching the APEX API documentation, browser (Page Designer) and I hope, a comparable desktop IDE. When writing the JavaScript bits of the APEX application, developers have a few choices. The best practice though, suggests that we consolidate code into JS files. This aids version control and testing. It\'s no easy feat though, ask any experience APEX developer. Fortunately, [Vincent Morneau](https://twitter.com/vincentmorneau) came to the rescue with some [(APEX) Nitro](https://github.com/OraOpenSource/apex-nitro) to boost productivity.\\n\\nVincent changed the paradigm of coding JS in APEX. We could gradually move some of our screen time away from Page Designer, and use mainstream code editors like [Visual Studio Code (Code)](https://code.visualstudio.com/), [Atom](https://atom.io) and [Sublime](https://www.sublimetext.com/), when working on JS, CSS and PL/SQL code. Without any fanfare, I had suggested and implemented TypeScript support in APEX Nitro that has been available since version 4.0.0-beta. For me, the final piece of the toolchain, would be a TS declaration file for the APEX JS API. Since there aren\'t any out there, I thought, why not write one? And so I did.\\n\\nI am very excited about what benefits this might bring to the community and so, going away from my comfort zone, I have decided to make my work public so that fellow APEX developers can have a feel of what it\'s like working with TypeScript and what benefits it brings. I won\'t claim to be an expert in TypeScript or the APEX JS API. If you do find errors, please let me know through the usual channels!\\n\\nSince TypeScript 2.0, declaration files are managed under the [DefinitelyTyped](http://definitelytyped.org/) project and made available through [npm](https://npmjs.com). You can find a list of available declaration files under the organization [@types](https://www.npmjs.com/~types), or from their Github [repository](https://github.com/DefinitelyTyped/DefinitelyTyped). For now, I have published my work at https://github.com/fuzziebrain/orclapex-js with instructions on how to use it in your environment. Eventually though, I\'d like to publish this under DefinitelyTyped repository, so keep in touch to be informed of changes.\\n\\nSome day though, I hope that the APEX JS API would proudly wear the badge:\\n\\n[TypeScript definitions on DefinitelyTyped (http://definitelytyped.org)](./assets/standard-flat.svg)\\n\\nHappy coding!"},{"id":"id/1707","metadata":{"permalink":"/content/id/1707","source":"@site/posts/two-days-of-node-js/index.md","title":"Two Days of Node.js","description":"Main Ballroom Before the Big Event Starts","date":"2017-10-22T16:23:56.000Z","formattedDate":"October 22, 2017","tags":[{"label":"nodejs","permalink":"/content/tags/nodejs"},{"label":"serverless","permalink":"/content/tags/serverless"},{"label":"typescript","permalink":"/content/tags/typescript"},{"label":"v8","permalink":"/content/tags/v-8"},{"label":"graphql","permalink":"/content/tags/graphql"},{"label":"kubernetes","permalink":"/content/tags/kubernetes"},{"label":"bots","permalink":"/content/tags/bots"},{"label":"wasm","permalink":"/content/tags/wasm"},{"label":"orclapex","permalink":"/content/tags/orclapex"}],"readingTime":6.535,"hasTruncateMarker":true,"authors":[{"name":"Adrian Png","title":"Senior Cloud Solutions Architect @ Insum","url":"https://github.com/fuzziebrain","imageURL":"https://github.com/fuzziebrain.png","key":"fuzziebrain"}],"frontMatter":{"title":"Two Days of Node.js","tags":["nodejs","serverless","typescript","v8","graphql","kubernetes","bots","wasm","orclapex"],"slug":"id/1707","authors":["fuzziebrain"],"categories":["Technology","Programming","Nodejs"],"date":"2017-10-22T16:23:56.000Z"},"prevItem":{"title":"Write High Quality JavaScript Code for Oracle Application Express","permalink":"/content/id/1708"},"nextItem":{"title":"Oracle APEX and REST Without the Pain","permalink":"/content/id/1706"}},"content":"![Main Ballroom Before the Big Event Starts](./assets/nodejs-opening.jpg)\\n\\nThe beautiful City of Vancouver (in British Columbia) played host to the North American [Node.js Interactive 2017](http://events.linuxfoundation.org/events/node-interactive) conference earlier this month. That close to my backyard, it was simply inexcusable not to attend! I wasted no time in getting the support of my employer and am glad I did. Here\'s a little summary of my experience and what I learned at the two-day conference to share.\\n\\n\x3c!-- truncate --\x3e\\n\\n## Keynotes\\nThe conference began with a series of short (10 to 20 minutes) keynote sessions on a wide spectrum of topics. Mark Hinkle, Executive Director of the Node.js Foundation, kicked things off with a \\"State of the Union\\" address on how well the technology is progressing. Also announced was the launch of the Node.js certification exam beta. Look out for that if you are keen to demonstrate your Node.js skills. I signed up a while to be notified, but haven\'t heard anything since. Hopefully in the coming weeks.\\n\\nMark made a special mention of [Node School](https://nodeschool.io/), a self-organized effort to develop curriculum and resources for running Node.js training workshops. Very similar to the work that [Software Carpentry](https://software-carpentry.org/) and [Data Carpentry](http://www.datacarpentry.org/) are doing to teach programming, Unix and data management to the research community. Given the [resources](https://blogs.oracle.com/apex/oracle-announces-oracle-application-express-curriculum) that Oracle has provided, it won\'t be difficult for the Oracle APEX community to emulate these initiatives.\\n\\n## First Class Citizen\\n![Franzi Hinkelmann (Google) on Node.js\' importance in V8](./assets/franzi-on-node-and-v8.jpg)\\n[Franzi Hinkelmann](https://twitter.com/fhinkel) flew across an ocean and continent to let everyone know that supporting Node.js is now a priority for Google\'s V8 team. In fact, the recent [delay](https://medium.com/the-node-js-collection/node-js-8-0-0-has-been-delayed-and-will-ship-on-or-around-may-30th-cd38ba96980d) in releasing Node.js version 8 was in part, due to [major changes in V8](https://v8project.blogspot.com/2017/05/launching-ignition-and-turbofan.html) (Crankshaft &rarr; TurboFan + Ignition) in the V8 engine, and the team wanted the community to receive a version of Node.js that would be more future-proof. Great news!\\n\\n## On Open Source\\n![Panel on issues sustaining an open source ecosystem. From left: Jory Burson, Erin McKean, Tracy Hinds and Maggie Pint](./assets/panel-on-sustaining-open-source-ecosystem.jpg)\\nAfter round of lightning keynotes, the welcome session ended with an all-women panel to discuss some of the major challenges maintaining open source (OS) projects. I was especially thrilled to listen to what [Maggie Pint](https://twitter.com/maggiepint) had to share. Maggie\'s the maintainer of the [Moment.js](https://momentjs.com/), a library for manipulating dates in JavaScript that I\'m sure many web developers have used in their projects. She was very candid on some of the issues she has had to deal with maintaining the project, for example, how she has to deal with snide remarks on the Internet. A good reminder for us consumers to provide constructive criticisms without needing to be mean.\\n\\nMaggie opened the panel discussion with her take on monetary rewards for working on OS projects. Funding to pay developers is nice, but having companies granting employees time towards working on OS projects is even better. Large companies like IBM and Microsoft can quite easily manage and provide such support, but it is a challenge for smaller companies with limited resources to do the same. For that reason, I am very grateful to [Insum Solutions](https://www.insum.ca) for the chance to participate and contribute to open source.\\n\\nA third takeaway that stuck with me was that **YOU** don\'t have to be an expert coder to be able to contribute to open source. Most projects have other needs like writing technical writing, project management and community engagement. If you have these or other valuable skillsets, consider lending your expertise. The community will appreciate it!\\n\\n## Sessions\\nMost, if not all sessions were about 20-30 minutes long. A little different from what ODTUGers are familiar with. This format provides attendees a lot of breath, but unfortunately, at the expense of depth.\\n\\nAs an APEX developer, who sometimes dable with Node.js, these were my picks:\\n\\n### Day 1\\n* Everything You Wanted to Know About Logging by [Charlie Robbins](https://twitter.com/indexzero)\\n* VS Code: Optimize Your Node.js Development \\"Inner Loop\\" by [Chris Dias](https://twitter.com/chrisdias)\\n* Data Science Survival Kit (workshop) by Philipp Burckhardt and [Athan Reines](https://twitter.com/kgryte)\\n* GraphQL in the Wild by [Steve Faulkner](https://twitter.com/southpolesteve)\\n* Build Interactive Workshops with Electron (workshop) by [Seth Vincent](https://twitter.com/sethdvincent)\\n* High Performance JS in V8 - [Peter Marshall](https://www.linkedin.com/in/peter-marshall-01226130)\\n\\n#### Highlights\\n* Love [VS Code](https://code.visualstudio.com/)! Chris presented a comprehensive walkthrough on writing, debugging and deploying Node.js applications. An impressive demonstration of intellisense, Docker-based deployments and working with Azure cloud, all within a **FREE** tool!\\n* Need a good use case for [WASM](http://webassembly.org/) other than game engines? Try [stdlib](https://stdlib.io/). The workshop was an eye-opener, demonstrating how you could do a good amount of data analysis and visualization all within the browser using JavaScript.\\n* [GraphQL](http://graphql.org/), cool tech but as an APEX developer, I have yet to be convinced with a good use case. Something worth exploring for data integration perhaps?\\n\\n### Day 2\\n* Break-Up with Your Server, But Don\'t Commit to a Cloud Platform by [Linda Nichols](https://twitter.com/lynnaloo)\\n* High Performance Apps with JavaScript and Rust, It\'s Easier than You Think by [Amir Yasin](https://twitter.com/ayasin)\\n* Effective Typed JavaScript by [Jeremy Morrell](https://twitter.com/jeremymorrell)\\n* New DevTools Features for JavaScript by [Yang Guo](https://twitter.com/hashseed)\\n* Using minikube (Kubernetes) for Local Node.js Development by [Troy Connor](https://twitter.com/troy0820)\\n* Serverless Bots with Node.js (workshop) by Alan Ho and [Amir Shevat](https://twitter.com/ashevat)\\n* TypeScript - A Love with JavaScript by [Bowden Kelly](https://twitter.com/bowdenk7)\\n* Grokking Asynchronous Work in Node.js by [Thorsten Lorenz](https://twitter.com/thlorenz)\\n\\n#### Highlights\\nSessions and workshops that I enjoyed on day two mostly fell into two buckets: Serverless and TypeScript.\\n\\nServerless is one of the newer buzzwords made popular by cloud vendors like Amazon ([AWS Lamda](https://aws.amazon.com/lambda/)) and Google ([Cloud Functions](https://cloud.google.com/functions/)). My interest stems from a presentation I did at Kscope17 that examined the role of [Microservices](https://martinfowler.com/articles/microservices.html) in APEX applications. Since we run most of our code/logic on the database, the challenge is to find a fit for such technologies, especially in the cloud era. I expect to spend more time looking at this in the near future.\\n\\nBowden Kelly cited an interesting [paper](http://ieeexplore.ieee.org/document/7985711) that examined the significance of applying type-checking in JavaScript development. The study examined 400 projects on Github and found that 15% of bugs identified could have been fixed before a release. I am not surprised! The lack of type-checkign was one of the major reasons why I **had** a distaste for the language, and so, dialects like TypeScript and Coffeescript are a welcomed addition. For APEX developers, you might be interested to know that [TypeScript support](https://github.com/OraOpenSource/apex-nitro/issues/200) was baked into APEX-Nitro v4.\\n\\nThe other interesting topics were on [Minikube](https://github.com/kubernetes/minikube) and Chrome\'s [DevTools](https://developer.chrome.com/devtools). Minikube allows you to run a local single-node [Kubernetes](https://kubernetes.io/) cluster. This helps with simulation and testing of Docker container deployments. Though currently you would use `kubectl` to deploy, it sounds like in upcoming releases ([see report](https://techcrunch.com/2017/10/17/docker-gives-into-invevitable-and-offers-native-kubernetes-support/)), paid versions of Docker will support pushing containers to Kubernetes clusters.\\n\\n## Conclusion\\n![Sticker Collection](./assets/sticker-collection.jpg)\\nNode.js Interactive is a great event to attend whether or not you are a hard-core Node.js programmer by trade. Would have been nice if lunch was provided so that attendees didn\'t need to compete with downtown office workers for food, but I suppose it had to do with the more affordable conference fee? And like all developer conferences, besides good content, the other benefit of attending is the opportunity to network with peers, speakers and vendors. I met some of the core people involved with Loopback project before Strongloop was acquired by IBM, and had the chance to allay my fears that Loopback was **not** going away in favour of some IBM-branded product. I was also pleasantly surprised to have met two friends at the event, and even learned a few things about GraphQL and Serverless from them!\\n\\nThe conference returns to Vancouver in October 2018, so for the folks heading to Oracle OpenWorld next year, consider planning a trip north to our beautiful city. There\'s much knowledge and scenery to soak up!"},{"id":"id/1706","metadata":{"permalink":"/content/id/1706","source":"@site/posts/oracle-apex-and-rest-without-the-pain/index.md","title":"Oracle APEX and REST Without the Pain","description":"Photo by Hisham Abo-hamad on Unsplash","date":"2017-09-04T02:05:06.000Z","formattedDate":"September 4, 2017","tags":[{"label":"orclapex","permalink":"/content/tags/orclapex"},{"label":"rest","permalink":"/content/tags/rest"},{"label":"suez","permalink":"/content/tags/suez"},{"label":"nodejs","permalink":"/content/tags/nodejs"},{"label":"proxy","permalink":"/content/tags/proxy"},{"label":"ssl","permalink":"/content/tags/ssl"},{"label":"tools","permalink":"/content/tags/tools"}],"readingTime":3.115,"hasTruncateMarker":true,"authors":[{"name":"Adrian Png","title":"Senior Cloud Solutions Architect @ Insum","url":"https://github.com/fuzziebrain","imageURL":"https://github.com/fuzziebrain.png","key":"fuzziebrain"}],"frontMatter":{"title":"Oracle APEX and REST Without the Pain","tags":["orclapex","rest","suez","nodejs","proxy","ssl","tools"],"slug":"id/1706","authors":["fuzziebrain"],"categories":["Technology","Programming","Oracle Application Express"],"date":"2017-09-04T02:05:06.000Z"},"prevItem":{"title":"Two Days of Node.js","permalink":"/content/id/1707"},"nextItem":{"title":"Real Uptime Monitoring of Oracle APEX Instances","permalink":"/content/id/1705"}},"content":"![Photo by <a href=\\"https://unsplash.com/@moudy100\\" target=\\"_blank\\">Hisham Abo-hamad</a> on Unsplash](./assets/ismalia-egypt.jpg)\\n\\nEarlier this year (2017), I had the privilege to share with the community, some of the work and experiences I gained helping to build some of the blocks that form the foundation of [apex.world](https://apex.world/). In my presentations, I had shared some of the challenges I had integrating RESTful web services with [Oracle Application Express](https://apex.oracle.com) (APEX). A crippling issue involved the use of [Oracle Database Express Edition](http://www.oracle.com/technetwork/database/database-technologies/express-edition/) (Oracle XE) and communicating with web services over a secured protocol. The by-product of that experience led to the development of a simple [Node.js](https://nodejs.org/) application, which I named [Suez](https://github.com/fuzziebrain/suez).\\n\\n\x3c!-- truncate --\x3e\\n\\nTraffic flowing through the Internet can be sniffed and used for nefarious purposes. Therefore we are encouraged, whenever possible, to use a secured network transport protocol to safeguard against these attacks. It is almost mandatory when the communication between client and server contains secrets or personally identifiable information (PII), e.g passwords, secure tokens and users\' credit card details.\\n\\nFor content served over web protocols, we use Transport Layer Security (TLS) or Secure Sockets Layer (SSL). These involve the use of digitally-signed certificates belonging to a trust chain that allows systems to verify identity and provides keys to communicate securely using encryption. Most Certification Authorities (CA) offer these for an annual fee, but more recently, a consortium led by Mozilla and a few other partners, created a new CA that made SSL certificates available to everyone. [Let\'s Encrypt](https://letsencrypt.org/) is an open CA that provides services and tools to automatically verify and sign certificate requests for **free**!\\n\\nMany API providers offer web services over HTTPS and consumers are highly encouraged, and sometimes forced to use this protocol for accessing content. The Oracle Wallet is used to store and manage SSL certificates that are required for establishing connections (SSL handshakes). For Oracle Database Standard and Enterprise Editions, a tool known as the Oracle Wallet Manager (OWM) is used to conveniently add and remove certificates. Not surprisingly, this is absent in the free Express Edition. A workaround using OpenSSL was prescribed by Hazrul Nizam\'s [blog post](https://blog.hazrulnizam.com/openssl-workaround-oracle-xe-wallet/). Good read!\\n\\nHowever, like any other web technology, SSL is prone to weaknesses and vulnerabilities and hence, it continues to evolve. For example, older SSL certificates that were signed with a weaker hashing algorithm SHA-1, have been made obsolete as it was been proven to be [prone to collisions](https://en.wikipedia.org/wiki/Collision_attack). For older versions of the Oracle DB, an active Oracle Support contract allows you access to patches that upgrade the wallet to handle newer hashes. However, if you are developing using Oracle XE, then you are out of luck. Well... kinda.\\n\\nWithout any upgrades to support newer hash algorithms, we were forced to improvise a solution for integrating Slack and apex.world. This is done through a proxy that accepts communications on a non-secure HTTP channel, relay the request and then pipe the response back to the caller. However, rather than write code for every API that needed to be accessed, I wrote Suez.\\n\\n![Suez usage example](./assets/suez.png)\\n\\nSuez relies on [Steven Chim\'s](https://twitter.com/chimurai) [work](https://github.com/chimurai/http-proxy-middleware) for creating simple web proxies using Node.js, and that in turn is powered by Nodejitsu\'s [node-http-proxy](https://github.com/nodejitsu/node-http-proxy). It uses the request\'s hostname value, looks up the configuration and then routes the call to the intended API target. Instructions on how to configure and deploy the application is detailed in the project\'s [README](https://github.com/fuzziebrain/suez/blob/master/README.md).\\n\\nI am hoping that this small utility would help developers get started on integrating cool APIs and build even cooler APEX applications. That said, I would end by emphasizing that orthdox approaches to security are always preferred. If budget allows, continue to rely on SSL certificates not only for securing end-to-end communications, but also, verifying that your application is talking to authorized servers."},{"id":"id/1705","metadata":{"permalink":"/content/id/1705","source":"@site/posts/real-uptime-monitoring-of-oracle-apex-instances/index.md","title":"Real Uptime Monitoring of Oracle APEX Instances","description":"All Systems Go!","date":"2017-08-26T13:41:15.000Z","formattedDate":"August 26, 2017","tags":[{"label":"monitoring","permalink":"/content/tags/monitoring"},{"label":"sysadmin","permalink":"/content/tags/sysadmin"},{"label":"orclapex","permalink":"/content/tags/orclapex"},{"label":"oxar","permalink":"/content/tags/oxar"},{"label":"devops","permalink":"/content/tags/devops"}],"readingTime":4.56,"hasTruncateMarker":true,"authors":[{"name":"Adrian Png","title":"Senior Cloud Solutions Architect @ Insum","url":"https://github.com/fuzziebrain","imageURL":"https://github.com/fuzziebrain.png","key":"fuzziebrain"}],"frontMatter":{"title":"Real Uptime Monitoring of Oracle APEX Instances","tags":["monitoring","sysadmin","orclapex","oxar","devops"],"slug":"id/1705","authors":["fuzziebrain"],"categories":["Technology","Programming","Oracle Application Express"],"date":"2017-08-26T13:41:15.000Z"},"prevItem":{"title":"Oracle APEX and REST Without the Pain","permalink":"/content/id/1706"},"nextItem":{"title":"Mail Dropped","permalink":"/content/id/1704"}},"content":"![All Systems Go!](./assets/all-systems-go.png)\\n\\nRunning a production application on an Oracle APEX instance? Then read on!\\n\\nMaking sure your application is up and running 24/7 doesn\'t need to be a daunting task, plus it may not even cost you a dime! [UptimeRobot](https://uptimerobot.com/) provides **50 free monitors** that can poll your servers using the smallest interval of 5 minutes. You can easily monitor websites and receive multi-channel notifications of downtime. No excuse!\\n\\n\x3c!-- truncate --\x3e\\n\\nThere are several solutions out there for server monitoring including the ever popular, open-sourced [Nagios](https://www.nagios.org/). However, build-your-own solutions often require more resources to set up. It only makes sense to host the monitoring software on an independent server - possibly even network/infrastructure - that your APEX instance uses. Not forgetting the much needed knowledge and time spent to set up the systems.\\n\\nUptimeRobot provides monitoring services for free or for fee. You can choose four monitoring types:\\n\\n### 1.Ping\\nMonitors servers by sending [ICMP](https://en.wikipedia.org/wiki/Internet_Control_Message_Protocol) packets and waiting for a response. Sending ping requests are not logged, so that can be a good thing. However, for web servers, a ping response is not a reliable measure of whether the required service is operational. A server can respond to pings even though the server application, e.g. Apache HTTPD server, is not running.\\n\\nTo configure a Ping Monitor:\\n1. Select the monitor type *Ping*.\\n2. Enter a friendly name.\\n3. Enter the *IP address* or *fully-qualified domain name* to monitor, e.g. *111.111.111.111*, *websvr.contoso.com*.\\n4. Set the monitoring interval.\\n5. Select the contact(s) to be notified.\\n6. Click *Create Monitor*.\\n\\n### 2. Port\\nPort monitoring allows you to monitor services through the ports that they use. For web servers, this is usually HTTP (80) or HTTPS (443), but one could also monitor other services like FTP and mail services.\\n\\nTo configure a Port Monitor:\\n1. Select the monitor type *Port*.\\n2. Enter a friendly name.\\n3. Enter the *IP address* or *fully-qualified domain name* or URL to monitor.\\n4. Select the port to monitor. Choose from:\\n   * HTTP (80)\\n   * HTTPS (443)\\n   * SMTP (25)\\n   * POP3 (110)\\n   * IMAP (143); or\\n   * Custom Port\\n5. Specify a port number if *Custom Port* was chosen in step (4).\\n6. Set the monitoring interval.\\n7. Select the contact(s) to be notified.\\n8. Click *Create Monitor*.\\n\\n### 3. HTTP(s)\\n![HTTP(s) Monitor Configuration](./assets/https-monitor-configuration.png)\\n\\nThis monitor type uses HTTP status codes returned by the specified URL or IP address to indicate whether the service is up (`200`) or down (anything else, e.g. `404`, `500`, `504` etc.).\\n\\nTo configure a HTTP(s) Monitor:\\n1. Select the monitor type *HTTP(s)*.\\n2. Enter a friendly name, e.g. *Contoso APEX Instance*.\\n3. Enter the URL or IP address to monitor, e.g.  *https://contoso.com/ords/f?p=STORE:HOME:0*.\\n4. Set the monitoring interval. __For free accounts, the range is from 5 minutes to 24 hours.__\\n5. If authentication is required, then click ont he *Advanced Settings*, *Authentication Settings* and then enter the user credentials and authentication type.\\n6. Select the contact(s) to be notified.\\n7. Click *Create Monitor*.\\n\\n### 4. Keyword\\nKeyword monitoring provides the ability for more fine-grain monitoring.\\n\\nTo configure a Keyword Monitor:\\n1. Select the monitor type *Keyword*.\\n2. Enter a friendly name, e.g. *Contoso Homepage*.\\n3. Enter the URL or IP address to monitor, e.g.  *https://contoso.com/ords/f?p=STORE:HOME:0*.\\n4. Enter the keyword to scan for, e.g. *Welcome to Contoso Store*. __Note that the keyword entered is case-sensitive.__\\n5. Set to alert when either *Keyword Exists* or *Keyword Not Exists*\\n6. Enter authentication information if required.\\n7. Select the contact(s) to be notified.\\n8. Click *Create Monitor*.\\n\\n**NOTEWORTHY** Web servers will log access by both HTTP(s) and Keyword monitor types. Keep that in mind when setting up monitors with short intervals.\\n\\n## Notifications\\n\\n![Service Down Email Notification](./assets/downtime-notification-email.png)\\n\\nWhen setting up monitors, you need to specify which contact(s) to notify when things go wrong. This is set up under *My Settings*. You may add contacts using either of the following technologies:\\n* Email\\n* Pro SMS (for paid accounts, receive up to 10 messages included in the subscription, otherwise SMS credits are required)\\n* [Twitter](https://twitter.com/)\\n* [Webhook](https://en.wikipedia.org/wiki/Webhook)\\n* Email-to-SMS (uses service providers\' free email-to-SMS gateways and may not always be stable)\\n* [Boxcar 2](https://boxcar.io/)\\n* [Pushbullet](https://www.pushbullet.com/)\\n* [Pushover](https://pushover.net/)\\n* [HipChat](https://www.hipchat.com/)\\n* [Slack](https://slack.com/)\\n\\nThere is also an option of adding a public status page or using RSS feeds to display server statuses on your website. These are nice to haves, but not always necessary.\\n\\n## Summary\\nFor APEX applications, I would generally prefer using either *HTTP(s)* or *Keyword* monitoring as they provide the most reliable method for detecting if the web server (reverse proxy), JavaEE Web Container (hosting [ORDS](http://www.oracle.com/technetwork/developer-tools/rest-data-services/)) and/or Oracle Database had gone down. All three are important services responsible for delivering APEX applications. Any of them going down will trigger an alert.\\n\\n![APEX Error Page](./assets/apex-error-page.png)\\n\\nBetween the two though, I feel *Keyword* stands out. It really depends on what your definition of \\"uptime\\" is. To me, that includes having the APEX instance deliver the expected outcomes. Any errors as a result of application runtime execution, should also classify as downtime. Often, APEX would still return a HTTP status code `200` even if an error had occured. Either we trap these events during error handling and then return the appropriate status code, or create a strategy involving keywords and then monitor these terms to signal service status for better insights to availability.\\n\\nThat said, regardless of what you choose, anything is better than nothing. So get monitored now!"},{"id":"id/1704","metadata":{"permalink":"/content/id/1704","source":"@site/posts/mail-dropped/index.md","title":"Mail Dropped","description":"Screen capture of http://www.mail-tester.com/ Results","date":"2017-08-20T12:50:06.000Z","formattedDate":"August 20, 2017","tags":[{"label":"email","permalink":"/content/tags/email"},{"label":"smtp","permalink":"/content/tags/smtp"},{"label":"spf","permalink":"/content/tags/spf"},{"label":"orclapex","permalink":"/content/tags/orclapex"},{"label":"oxar","permalink":"/content/tags/oxar"},{"label":"devops","permalink":"/content/tags/devops"},{"label":"linode","permalink":"/content/tags/linode"}],"readingTime":9.165,"hasTruncateMarker":true,"authors":[{"name":"Adrian Png","title":"Senior Cloud Solutions Architect @ Insum","url":"https://github.com/fuzziebrain","imageURL":"https://github.com/fuzziebrain.png","key":"fuzziebrain"}],"frontMatter":{"title":"Mail Dropped","tags":["email","smtp","spf","orclapex","oxar","devops","linode"],"slug":"id/1704","authors":["fuzziebrain"],"categories":["Technology","Programming","Oracle Application Express"],"date":"2017-08-20T12:50:06.000Z"},"prevItem":{"title":"Real Uptime Monitoring of Oracle APEX Instances","permalink":"/content/id/1705"},"nextItem":{"title":"The Community is One of Oracle APEX\'s Best Features","permalink":"/content/id/1703"}},"content":"![Screen capture of http://www.mail-tester.com/ Results](./assets/before.png)\\n\\nBefore I begin, I just wanted to be absolutely clear that this post has nothing to do with Apple\'s nifty [tool](https://support.apple.com/kb/ph2629) for sending out large email attachments.\\n\\nIn my previous [post](/content/id/cizx69jya000ra0pvv26u3cez/), I shared an approach for creating your very own [Oracle Application Express](https://apex.oracle.com/) (APEX) instance on a small budget. Briefly, the solution involves using [Vagrant](https://www.vagrantup.com/) to create a very affordable virtual machine (VM) on [Linode\'s](https://www.linode.com/?r=41672b20d515344de465e9ed44c1a75356445597) pool of servers and run the [OXAR](https://github.com/OraOpenSource/OXAR) build script, to setup the APEX instance very quickly and painlessly.\\n\\nHowever, all that power (and savings) comes with great responsibilty. \\n\\n\x3c!-- truncate --\x3e\\n\\nWithout the aid of managed services, the passionate APEX developer will have to handle the day-to-day administration of the system and database. One of the common tasks required of a developer is to ensure emails can be sent from APEX appplications successfully. This not only involves understanding how to use the APIs to generate and send emails, but also knowledge on maximizing the chance that emails are delivered successfully to the intended audience and do not end up in the spam bin.\\n\\nSo what\'s involved in getting email setup on your OXAR instance?\\n\\n## Part One\\nEnsure that a suitable mail daemon is running on your server. If you used the method that I prescribed in my earlier post, then you would be running a [CentOS](https://www.centos.org/) 7 server with [Postfix](http://www.postfix.org/) installed, but not running on boot. Let\'s be sure by executing the following command: \\n`> sudo rpm -q postfix`\\n\\nIf Postfix is installed, you should see an output with the full package name, e.g. `postfix-2.10.1-6.el7.x86_64`.\\n\\nIf absent, install it using yum by executing the command: \\n`> sudo yum install -y postfix`\\n\\nNext, check that the service is running by executing the command: \\n`> sudo systemctl is-active postfix`\\n\\nThe screen output should provide some clues about whether the service has been started and/or enabled to start on boot:\\n* `active` &rArr; service is running\\n* `inactive` &rArr; service is not running but enabled\\n* `unknown` &rArr; service is not running and not enabled\\n\\nIf the service is in `active` state, then check that it is also enabled to start on boot by executing the command: \\n`> sudo systemctl is-enabled postfix`\\n\\nThe system should respond with either `enabled` or `disabled`.\\n\\nTo enable the service to start on boot, execute:\\n`> sudo systemctl enable postfix`\\n\\nTo start the service:\\n`> sudo systemctl start postfix`\\n\\nNext, configure Postfix if necessary. In most situations, you can use Postfix to send emails directly. If that is the case, the default configuration should generally work just fine. \\n\\n### Email Relay\\n\\nSome developers have access to SMTP servers and might like to use them to circumvent spam-related issues. It would be prudent to assume that these servers, if exposed to remote requests, will require authentication. And because it is the Internet, using a secured network protocol to authenticate is highly recommended. If you have no problems setting up SSL/TLS support for SMTP connections, then you might not want to use Postfix at all. You can configure the SMTP host, port and credentials in the APEX Instance Administration. this is addressed in Part Two.\\n\\nHowever, if you are unable to set up a secure connection, then consider using Postfix by configuring it as a secure relay. I will use Gmail as an example. Here\'s what you need to do:\\n1. Install Cyrus [SASL](https://en.wikipedia.org/wiki/Simple_Authentication_and_Security_Layer) plain authentication package:\\n   `> sudo yum install -y cyrus-sasl-plain`\\n2. Execute the command: \\n   `> sudo echo \\"smtp.gmail.com myaccount@gmail.com:mysecurepassword\\" > /etc/postfix/sasl_passwd\\"`\\n3. Execute the next command to hash the file to protect it from prying eyes: \\n   `> sudo postmap hash:/etc/postfix/sasl_passwd`\\n4. Don\'t leave the plain text file lying around: \\n   `> sudo rm -f /etc/postfix/sasl_passwd`\\n5. Configure Postfix stored in the configuration file `/etc/postfix/main.cf`. Place the following lines under the section *INTERNET OR INTRANET*:\\n```bash\\nsmtp_sasl_auth_enable = yes\\nsmtp_sasl_password_maps = hash:/etc/postfix/sasl_passwd\\nsmtp_sasl_security_options = noanonymous\\nsmtp_tls_security_level = secure\\nsmtp_tls_mandatory_protocols = TLSv1\\nsmtp_tls_mandatory_ciphers = high\\nsmtp_tls_secure_cert_match = nexthop\\nsmtp_tls_CAfile = /etc/pki/tls/certs/ca-bundle.crt\\nrelayhost = smtp.gmail.com:587\\n```\\n6. Restart Postfix:\\n   `> sudo systemctl restart postfix`\\n\\n**IMPORTANT:** To send emails using Gmail\'s SMTP servers, you may need to change your account access to [allow access from less secure apps](https://support.google.com/accounts/answer/6010255). \\n\\n*Attribution: The method for configuring Postfix to relay emails through Gmail was adapted from Charles Auer\'s [post](https://charlesauer.net/tutorials/centos/postfix-as-gmail-relay-centos.php).*\\n\\n### IPv6\\n\\nIf you are not prepared to deal with IPv6 configuration and issues, then you might want to also limit the network protocol that Postfix uses. Update this configuration in `/etc/postfix/main.cf` and then restart the service.\\n```\\ninet_protocols = ipv4\\n```\\n\\n## Part Two\\nConfiguring the APEX instance for email is simple. The documentation for how to do this is well documented in the [docs](https://docs.oracle.com/database/apex-5.1/AEADM/configuring-instance-settings.htm#GUID-FEC98D5E-3C89-423E-9603-4CD2E553BA34) so I will not explain at length. In a nutshell:\\n1. Login to the Instance Administration page\\n2. Click Manage Instance\\n3. Click Instance Settings\\n4. Click the Email tab\\n5. Enter the following:\\n  a. SMTP Host Address - set to `localhost`\\n  b. SMTP Host Port - set to `25` or whatever port you used in the Postfix configuration\\n6. Click Apply Changes to save\\n7. Ensure the proper network ACL has been set to allow the \\"APEX schema\\" to communicate with `localhost`.\\n\\n![Instance Administration - Instance Settings - Email](./assets/apex-email-settings.png)\\n\\nAn easy way to test the service is to run the following procedure block using SQL Workshop\'s [SQL Commands](https://docs.oracle.com/database/apex-5.1/AEUTL/SQL-commands.htm#AEUTL220) interface:\\n```sql\\nbegin\\n  apex_mail.send(\\n    p_to => \'myfriend@email.com\'\\n    , p_from => \'me@myselfandi.com\'\\n    , p_subj => \'Popularity of Semantic Web Today\'\\n    , p_body => q\'[\\n        The 2017 Gartner Hype Cycle indicates that Semantic Web-related \\n        technologies are heading past the trough of disillusionment. \\n        We should investigate.\\n]\'\\n    , p_body_html => q\'[\\n        <p>\\n          The 2017 Gartner Hype Cycle indicates that Semantic Web-related \\n          technologies are heading past the trough of disillusionment. We \\n          should investigate.\\n        </p>\\n]\'\\n  );\\n  apex_mail.push_queue;\\nend;\\n/\\n```\\n\\n## Part Three \\nThe third and final piece that I\'d like to talk about in this post is to get pass spam filters and allowing emails to arrive safely in your intended receipients\' inbox.\\n\\nThe following are a list of checks that spam filters typically use to assess an incoming email.\\n* HELO Greeting Check\\n* Reverse DNS Check\\n* [Sender Policy Framework](http://www.openspf.org/) (SPF)\\n* Domain Keys ([DKIM](http://www.dkim.org/))\\n* [SpamAssassin](http://spamassassin.apache.org/) content checks\\n* [BATV](https://en.wikipedia.org/wiki/Bounce_Address_Tag_Validation) (Bounce Address Tag Validation)\\n* Blacklisting\\n  * [DNSBL](http://www.dnsbl.info/) \\n  * [URIBL](http://uribl.com/)\\n\\nThe easiest checks to get past is ensuring that your DNS records are correct. The HELO greeting simply checks that your domain has a valid forward and reverse entry. The latter might be a little harder to achieve since PTR records, reverse DNS mappings, are usually managed by the organization that issued the IP address. Most spam filters recognize this and would accept the email as long as there is one. \\n\\nThe single most important factor seems to be how the SPF records are set for your domain. It\'s not difficult for anyone to setup a SMTP server and with an appropriate email api/client, spoof a domain that he/she does not own and send out unauthorized emails. For example, if your APEX application attempts to use an *@yahoo.com* or *@gmail.com* email address sent from an unauthorized SMTP server, then these emails will likely be flagged as spam. Servers with strict policies might even outrightly reject these emails.\\n\\nSPF is a **single** [TXT](https://en.wikipedia.org/wiki/TXT_record) record in the DNS that uses [mechanisms](http://www.openspf.org/Mechanisms) to help verify if the incoming email originated from an authorized source. Commonly used mechanisms include: `a`, `mx`, `include`, `ip4` and `all`. Each mechanism in the record can have one of four qualifiers (`-` &rArr; fail, `~` &rArr; softfail, `+` &rArr; pass or `?` &rArr; neutral) that email filters could then use to decide on whether to flag as spam. \\n\\nBelow is the SPF record for the domain `bcoug.org`. It begins with the SPF version number and then six mechanisms. Five of them state different ways to resolve an authorized source. The last is a catch-all to state that the email source is unauthorized.\\n\\n```bash\\nv=spf1 a mx include:_spf.google.com include:servers.mcsv.net ip4:204.244.186.184/32 ~all\\n```\\n\\nBefore adding a SPF record to your DNS, test it first using the [tool](http://www.kitterman.com/spf/validate.html) provided by Kitterman. Add it to your DNS only when the SPF record validates successfully and can correctly identify your SMTP server as an authorized server for the specified domain. \\n\\n![Inputs to test a what-if email was sent from my personal APEX instance](./assets/spf-validate-form.png)\\n\\n![Results](./assets/spf-validate-results.png)\\n\\nAfter adding the SPF record, check to see if the record is being read correcly. You can use [MxToolBox](https://mxtoolbox.com/) for this. Here\'s what the SPF record for the domain `bcoug.org` looks like:\\n\\n| Prefix | Type | Value | PrefixDesc | Description |\\n|-|-|-|-|-|\\n| v | version | spf1 | | The SPF record version |\\n| + | a | | Pass | Match if IP has a DNS \'A\' record in given domain |\\n| + | mx | | Pass | Match if IP is one of the MX hosts for given domain name |\\n| + | include | _spf.google.com | Pass | The specified domain is searched for an \'allow\'. |\\n| + | include | servers.mcsv.net | Pass | The specified domain is searched for an \'allow\'. |\\n| + | ip4 | 204.244.186.184/32 | Pass | Match if IP is in the given range |\\n| ~ | all | | SoftFail | Always matches. It goes at the end of your record. |\\n\\n**NOTE:** Since `bcoug.org` resolves to IPv4 `204.244.186.184`, the ip4 mechanism can be omitted.\\n\\n### Evaluate Your Email\'s Spamminess\\n\\nHere\'s a bunch of tools I gathered from websites and [stackoverflow](https://stackoverflow.com/) posts that are useful for evaluating your emails (content, SPF records etc.) and creating an actionalble checklist. They have helped me quite a bit, so I\'m counting on them to do the same for you!\\n\\n| Link | Description |\\n| - | - |\\n| http://www.mail-tester.com/ | Create an email and send it through your designated SMTP server. |\\n| http://www.allaboutspam.com/ | Like Mail Tester, send an email through your designated SMTP server. |\\n| http://spamcheck.postmarkapp.com/ | Copy and paste an email with its headers to get a SpamAssassin score. |\\n\\n### Outcome\\n\\nAfter taking my APEX instance through cycles of corrective actions, I was finally able to achieve a score of 9/10. Where I fell short of a perfect score, is the absence of [DomainKeys](https://en.wikipedia.org/wiki/DomainKeys) (DKIM) information. I have yet to implement DKIM support for my APEX instance, so stay tuned, and you can expect a follow up howto post.\\n\\n![Results after the appropriate SPF records are added](./assets/after.png)\\n\\n## Last Words\\n\\nI hope this information here saves you some time figuring out what you need to do to stay out of the junk mail folder. Before I end though, here are a few more notes: \\n* Watch your content, even when performing checks on the validity of your emails.\\n* Include HTML content and make sure there are no broken links if any.\\n* Do your best to stay off any blacklists. It\'s a huge challenge getting your mail server delisted."},{"id":"id/1703","metadata":{"permalink":"/content/id/1703","source":"@site/posts/the-community-is-one-of-oracle-apex-s-best-features/index.md","title":"The Community is One of Oracle APEX\'s Best Features","description":"Photo by Ben Duchac on Unsplash","date":"2017-07-10T17:44:51.000Z","formattedDate":"July 10, 2017","tags":[{"label":"orclapex","permalink":"/content/tags/orclapex"},{"label":"community","permalink":"/content/tags/community"},{"label":"rstats","permalink":"/content/tags/rstats"},{"label":"r","permalink":"/content/tags/r"},{"label":"integration","permalink":"/content/tags/integration"},{"label":"research data management","permalink":"/content/tags/research-data-management"}],"readingTime":3.435,"hasTruncateMarker":true,"authors":[{"name":"Adrian Png","title":"Senior Cloud Solutions Architect @ Insum","url":"https://github.com/fuzziebrain","imageURL":"https://github.com/fuzziebrain.png","key":"fuzziebrain"}],"frontMatter":{"title":"The Community is One of Oracle APEX\'s Best Features","date":"2017-07-10T17:44:51.000Z","slug":"id/1703","authors":["fuzziebrain"],"tags":["orclapex","community","rstats","r","integration","research data management"],"categories":["Technology","Programming","Oracle Application Express"]},"prevItem":{"title":"Mail Dropped","permalink":"/content/id/1704"},"nextItem":{"title":"San Antonio Revisited","permalink":"/content/id/1702"}},"content":"![Photo by Ben Duchac on Unsplash](./assets/ben-duchac-66002.jpg)\\n\\nAfter reading the article \\"[The R community is one of R\'s best features](http://blog.revolutionanalytics.com/2017/06/r-community.html)\\" by [David Smith](https://twitter.com/revodavid), I felt there were a lot of similarities between the R and Oracle APEX communities, and hence, was inspired to pen a copycat post. So here it is:\\n\x3c!-- truncate --\x3e\\n\\n* [#orclapex](https://twitter.com/hashtag/orclapex) hashtag - \\n* Oracle APEX Ladies - none yet, but [ODTUG](http://www.odtug.com/) has a strong community of women and some of them are top APEX developers. I\'m sure we\'ll see a similar group forming in the near future! (/me nudge [@McilroyJackie](https://twitter.com/McilroyJackie))\\n* [Local Oracle APEX Meetup Groups](https://apex.world/ords/f?p=APEX_WORLD:MEETUPS) - we have meetup groups all over the world, many of them supported by ODTUG\'s [#GetInvolved](http://www.odtug.com/page/meetup) program. The link brings you to a only list of meetups organized through [Meetup.com](https://meetup.com/). I hear there are more groups, especially in Asia (good number in Japan) that use a different platform for organizing their meetings.\\n* Oracle APEX Newsletters - some evangelists maintain their own aggregate of the week\'s best news on APEX, e.g. [APEX Weekly](http://paper.li/RoelH/1333719828) by [Roel Hartman](https://twitter.com/RoelH). Others like [apex.world](https://apex.world) publishes a monthly newsletter, and ODTUG has their [blog aggregator](https://twitter.com/orclapexblogs) that tweets out new posts as they bake fresh, so take your pick!\\n* [Oracle APEX Blogs](https://twitter.com/orclapexblogs) - oops, I jumped the gun on this one.\\n* The official [Oracle Application Express Curriculum](https://apex.oracle.com/education) from the source - it\'s free and open sourced too. Enough said!\\n* Oracle APEX Conferences - take your pick (in order of when they usually happen)\\n  * APEX World\\n  * APEX Connection\\n  * Kscope (My Choice Award ;-) )\\n  * And a few smaller events. Use this [link](https://apex.world/ords/f?p=100:230:::NO:::) to track when and where they happen every year. Also, not to forget that there are some (a few) APEX sessions at [Oracle OpenWorld](https://www.oracle.com/openworld/).\\n* Github - R code is typically shared in the form of installable packages, and very similar to [npm packages](https://npmjs.com) for [Node.js](https://nodejs.org). For Oracle APEX and PLSQL developers, we usually share code either in the form of plugins, PLSQL packages or tools. Here are some examples:\\n  * [Plugins] On [apex.world](https://apex.world/), click on \\"Plug-ins\\" > \\"Plug-ins\\".\\n  * [Plugins] http://www.apex-plugin.com/ (some contribution might have the source code in Github repositories)\\n  * [PLSQL] https://github.com/OraOpenSource/oos-utils\\n  * [PLSQL] https://github.com/OraOpenSource/Logger\\n  * [PLSQL] https://github.com/mortenbra/alexandria-plsql-utils\\n  * [PLSQL] https://github.com/utPLSQL/utPLSQL\\n* Consortium? Hmm... I guess the closest would ODTUG\'s [Application Express Community](http://odtug.com/apex). The community has otherwise been very spontaneous. Everyone is and can be involved, and that includes members of the Oracle APEX development team.\\n\\nBut there\'s more:\\n* [Slack](https://orclapex.slack.com) - grab your invite by signing up as an apex.world member, and chat realtime with other members of the community. \\n* [OTN Community Forums](https://community.oracle.com/community/database/developer-tools/application_express) - probably the oldest social platform to have your questions answered. \\n* [Twitter bot](https://twitter.com/oapexquestions) that monitors and tweets [StackOverflow\'s](https://stackoverflow.com/) Oracle APEX questions so that they get maximum reach.\\n\\nI happen to think that there\'s much synergy between these two powerful platforms. Why? Let me attempt to convince you why Oracle APEX makes a great ~~Research~~ Data Management platform in a few bullet points:\\n* It allows ~~research~~ groups to quickly create, collect and securely manage data.\\n* Standards-based development without the fuss.\\n* Install the [Quick SQL](https://apex.oracle.com/en/quicksql/) package app, and use markdown-like text to generate data models, and even sample data!\\n* Start from low code and advance to lots-of-code if required.\\n* Integrates readily with other systems and technologies through the use of web services, e.g. R, which I discussed in this earlier [blog post](https://www.insum.ca/oracle-apex-statistics-r-u-serious/).\\n* Analyze the data exclusively in R using a package like [ROracle](https://cran.r-project.org/web/packages/ROracle/)\\n* Start for free (forever) or scale up later by paying for more advanced features.\\n\\nThat\'s a lot of bullet points. Just one more. Much like R:\\n> \\"[The] community is one of greatest assets of [Oracle APEX], and is an essential component of what makes [Oracle APEX] useful, easy, and fun to use. And you couldn\'t find a nicer and more welcoming group of people to be a part of.\\"\\n\\nLast but not least, if you are an R enthusiast who is curious to find out more about Oracle APEX, please feel free to reach out to [me](https://twitter.com/fuzziebrain)."},{"id":"id/1702","metadata":{"permalink":"/content/id/1702","source":"@site/posts/san-antonio-revisited/index.md","title":"San Antonio Revisited","description":"San Antonio Sunset","date":"2017-06-29T18:28:01.000Z","formattedDate":"June 29, 2017","tags":[{"label":"Kscope17","permalink":"/content/tags/kscope-17"},{"label":"odtug","permalink":"/content/tags/odtug"},{"label":"orclapex","permalink":"/content/tags/orclapex"},{"label":"community","permalink":"/content/tags/community"}],"readingTime":4.64,"hasTruncateMarker":true,"authors":[{"name":"Adrian Png","title":"Senior Cloud Solutions Architect @ Insum","url":"https://github.com/fuzziebrain","imageURL":"https://github.com/fuzziebrain.png","key":"fuzziebrain"}],"frontMatter":{"title":"San Antonio Revisited","date":"2017-06-29T18:28:01.000Z","slug":"id/1702","authors":["fuzziebrain"],"tags":["Kscope17","odtug","orclapex","community"],"categories":["Technology","Programming","Oracle Application Express"]},"prevItem":{"title":"The Community is One of Oracle APEX\'s Best Features","permalink":"/content/id/1703"},"nextItem":{"title":"APEX and the Affordable Cloud!","permalink":"/content/id/cizx69jya000ra0pvv26u3cez"}},"content":"![San Antonio Sunset](./assets/san_antonio_sunset.jpg)\\n\\nYet another Kscope journey has ended. This year\'s event was held in the beautiful JW Marriott Hill Country Resport and Spa, San Antonio, Texas. This is my fourth Kscope, third as a speaker, second at San Antonio, and first time taking home an award!\\n\x3c!-- truncate --\x3e\\n\\nKscope has always been about content and this year was no exception. Though unfortunately, I did not get to attend as many sessions as I would have liked to. This year, I spent more time at our company\'s exhibitor booth, as well as hiding in my hotel room, preparing myself for the two sessions I had to deliver. Stakes were high, as many speakers before me have set the standards very high. \\n\\nEven though I could not learn as much as previous years, I appreciated very much the increased amount of contact time I had with partners and fellow attendees, sharing some of the stuff I had worked on in the year passed. Learning doesn\'t only come through listening. You gain much by sharing too!\\n\\n## Food and Swag\\n![Healthy, tasty breakfast!](./assets/quinoa.jpg)\\n\\nSomeone claimed that food at Kscope was one of the best, and I agree. Having attended several other conferences before, I am confident to say that ODTUG serves up some of the most delicious cuisine. I was never really hungry as there was always ample to eat and enjoy!\\n\\n![San Antonio Sunset](./assets/swag.jpg)\\n\\nSwag-wise, ODTUG always seems to hit it right with a high quality bag containing lots of goodies. That includes the indispensible power bank that kept my phone alive. Allowing me to keep in constant communication with the team and of course, tweeting during the day uninterrupted. \\n\\nThis year however, it was Insum\'s swag that stood out. I know, I\'m biased! The company\'s cowboy hat, fidget spinners and stickers! It wasn\'t just our friends and partners that enjoyed these wonderful collectibles, but I got to enjoy them too!\\n\\n## On Sessions and Sharing\\nPreparing and delivering a successful presentation is tough, and I have a long road ahead to become as season and polished speaker like some of my peers at  Insum. However, as the wise [Joel Kallman](https://twitter.com/joelkallman) and [Juergen Schuster](https://twitter.com/JuergenSchuster) would advise me, I need to persevere, find opportunities and keep honing those presentation skills!\\n\\nI have presented a few times now at different conferences, but this year, something felt different! It was the happiness that I felt when attendees came up to me  after the session to let me know they had faced similar challenges, and that your sharing\'s gonna help in some little way!\\n\\n## It\'s All About People\\nBesides great food and content, the other huge advantage for being at Kscope in person is the people! It\'s a fantastic opportunity to meet people you hear about on Twitter, webinars and books! Back at SAT, I reminisced the first time I met [Patrick Cimolini](https://twitter.com/PatrickCimolini) at the hotel restaurant. I was estatic and went over to introduce myself. I must have surprised him! And who would have thought that four years later, he\'d be the dedicated sheperd, who made sure I got to work okay, during my two-week induction at the Montreal office.\\n\\nThis year, I had the pleasure of meeting [Maxime](https://twitter.com/mtremblaydev) and [Daniel Hochleitner](https://twitter.com/Dani3lSun) for the first time. And then the great conversations with [Doug](https://twitter.com/dmelx) and [Michael Potter](https://twitter.com/milobandit). Michael makes this trip across the Pacific annually, so no excuse for those of you living on the North American Plate not to join us! It\'s also wonderful to catch up with fellow Vancouverites Mirek, Nada and Liron, who made his debut appearance at Kscope. I\'m sure we\'re gonna see him around the ODTUG community a lot more going forward!\\n\\n![Working with the best crew](./assets/vr.jpg)\\n\\nLast but not least, I work remote and alone across the continent and so, it always brings joy meeting up with the Insum crew. This year, we had Michel, [Francis](https://twitter.com/Fr4ncis), [Monty](https://twitter.com/monty_latiolais), [Dave](https://twitter.com/dschleis), [Martin](https://twitter.com/martindsouza) and [Jorge](https://twitter.com/rimblas) my roommate. And I\'m especially delighted that we had our Kscope newbies Daniel and [Jackie](https://twitter.com/McilroyJackie) join in the fun!\\n\\n![Winner of the ODTUG Innovation Award 2017](./assets/nitro.jpg)\\n\\nThen the guy who made us all really proud. Scoring the ODTUG Innovation Award 2017 win and the much covetted Oracle ACE title, [Vincent Morneau](https://twitter.com/vincentmorneau)! Vincent won the award for his outstanding work on [APEX Nitro](https://github.com/OraOpenSource/apex-nitro). Again, if you are an Oracle APEX developer working mostly on the frontend, then this is a *FREE* product you don\'t wanna miss! It\'s gonna ease the pain of coding JavaScript and CSS considerably, improve your productivity and be a hero! Just like Vincent!\\n\\n![Not about the breakfast, but the bacon.](./assets/breakfast.jpg)\\n\\nAnd not to forget our troops back in homebase who cover our absence and are readily on standby to facilitate our sales and marketing efforts. It helps so much to have Marie and [Jason](https://twitter.com/jasonctquach) ensure our flights, accommodation and schedules are organized to make our conference travel such a breeze!\\n\\n## Gratitude\\n![ODTUG Volunteer Award](./assets/volunteer_award.jpg)\\n\\nSAT was not only my first Kscope, but it marked the beginning of my ODTUG journey. This year, I am extremely grateful for the recognition of my contributions to the community. It marks the beginning and not the end of my desire to serve in whatever capacity I can.\\n\\nI would like to end by thanking again the person who played the most significant role in influencing my career path and involvement with community: [Martin GiffyD\'Souza](https://twitter.com/martindsouza).\\n\\n![Standing Ovation for Martin on his last term as a ODTUG Board Member](./assets/grand_applause.jpg)\\n\\nThank you Martin! Had you not invited me to Kscope12, I might not have achieved this much!"},{"id":"id/cizx69jya000ra0pvv26u3cez","metadata":{"permalink":"/content/id/cizx69jya000ra0pvv26u3cez","source":"@site/posts/apex-and-the-affordable-cloud/index.md","title":"APEX and the Affordable Cloud!","description":"Rivadeneira by Cesar Lopez","date":"2017-03-04T11:28:18.000Z","formattedDate":"March 4, 2017","tags":[{"label":"orclapex","permalink":"/content/tags/orclapex"},{"label":"oxar","permalink":"/content/tags/oxar"},{"label":"cloud","permalink":"/content/tags/cloud"},{"label":"hosting","permalink":"/content/tags/hosting"},{"label":"vagrant","permalink":"/content/tags/vagrant"},{"label":"devops","permalink":"/content/tags/devops"},{"label":"linode","permalink":"/content/tags/linode"}],"readingTime":3.025,"hasTruncateMarker":true,"authors":[{"name":"Adrian Png","title":"Senior Cloud Solutions Architect @ Insum","url":"https://github.com/fuzziebrain","imageURL":"https://github.com/fuzziebrain.png","key":"fuzziebrain"}],"frontMatter":{"title":"APEX and the Affordable Cloud!","date":"2017-03-04T11:28:18.000Z","slug":"id/cizx69jya000ra0pvv26u3cez","authors":["fuzziebrain"],"tags":["orclapex","oxar","cloud","hosting","vagrant","devops","linode"],"categories":["Technology","Programming","Oracle Application Express"]},"prevItem":{"title":"San Antonio Revisited","permalink":"/content/id/1702"},"nextItem":{"title":"Hexo - Blogging Without Disks","permalink":"/content/id/cizx69jyk0015a0pvten1mngo"}},"content":"![Rivadeneira by Cesar Lopez](./assets/cesar-lopez-rivadeneira-6088-1280.jpg)\\n\\nLove [Oracle Application Express](https://apex.oracle.com/) (APEX), but having trouble finding a decent, affordable home for your killer application? Here\'s one option you can consider that should not burn your wallet.\\n\x3c!-- truncate --\x3e\\n\\n## Before We Begin\\nThe following guide assumes the following:\\n1. [Vagrant](https://www.vagrantup.com/) installed. The version I used for this article was 1.9.2.\\n2. [OXAR](https://github.com/OraOpenSource/OXAR) cloned.\\n3. Installation files required by OXAR have been downloaded and made available as a web resource. One option is to upload the files to [Dropbox](https://www.dropbox.com/) and then create a [shared link](https://www.dropbox.com/en/help/167).\\n4. The `config.properties` files is customized to requirements. The file URLs should point to the relevant installation files as per [OXAR instructions](https://github.com/OraOpenSource/OXAR/blob/master/README.md).\\n5. SSH keys generated. If not execute the command: `ssh-keygen`.\\n6. All commands are executed from the project\'s root directory.\\n\\n### OS Support\\nAt this time, I have only been able to demonstrate the remote build using Linux, though I believe Mac users should not have any problems doing the same. I will update or write a new post when I am able to accomplish the same tasks in Windows.\\n\\n## Linode\\nSign up for an account with [Linode](https://www.linode.com/?r=41672b20d515344de465e9ed44c1a75356445597). The smallest-sized Linode that you can create has sufficient resources to get a basic OXAR node running. Available to you at an affordable monthly fee of **USD 5**, it\'s probably as good as it gets!\\n\\n## Install the Linode Provider Plugin\\nExecute:\\n`vagrant plugin install vagrant-linode`\\n\\n## Prepare the Vagrantfile\\n### Synced Folders\\n\\nForce Vagrant to sync files using `rsync`, otherwise Vagrant might attempt to use NFS and the process will most likely fail as it did for me initially.\\n\\n```\\n  config.vm.synced_folder \\".\\", \\"/vagrant\\", type: \\"rsync\\"\\n    , rsync__exclude: [\\".git/\\", \\".vagrant/\\"]\\n```\\n\\n### Configure the Linode Provider\\n\\nAdd the Linode provider block:\\n\\n```\\nconfig.vm.provider :linode do |provider, override|\\n  override.ssh.private_key_path = \'~/.ssh/id_rsa\'\\n\\n  provider.api_key = \'<LINODE API KEY>\'\\n  provider.label = \'<LABEL FOR LINODE>\'\\n  provider.distribution = \'CentOS 7\'\\n  provider.datacenter = \'<DATACENTER>\'\\n  provider.planid = <PLAN ID>\\nend\\n```\\n\\nA list of valid Plan IDs and data centers can be obtained from the Linode Vagrant Provider plugin [documentation](https://github.com/displague/vagrant-linode/blob/master/README.md). Note that Plan ID is probably enough for a start.\\n\\n### Set the Timezone\\nDuring my first build, I encountered issues with ORDS not installing correctly because Java was not able to recognize the system\'s timezone correctly, similar to what had previously been [reported](https://github.com/OraOpenSource/OXAR/issues/194).\\n\\nThe Centos 7 image provisioned by Linode has the timezone set to UTC and that seems to be problem. The workaround for me was to set the timezone prior to calling OXAR\'s `build.sh` script. This is done towards the end of the `Vagrantfile`, within the SHELL sub-section. Probably something that we should consider adding to the main OXAR build process.\\n\\n```\\n    ...\\n\\n    rsync -rtv --exclude=\'files\' --exclude=\'.*\' /vagrant/ /tmp/vagrant-deploy\\n\\n    cd /tmp/vagrant-deploy\\n\\n    # Set the timezone\\n    timedatectl set-timezone America/Vancouver\\n\\n    ./build.sh\\n  SHELL\\nend\\n```\\n\\nTo get a list of valid timezones, execute the command `timedatectl list-timezones` on a similar Centos 7 instance.\\n\\n## Linode API Key\\n\\nLogin to your Linode account and then click on \\"my profile\\", found on the top-right corner of the page. Provide your password again when prompted. This will allow you access to the API Keys sub-page.\\n\\n![Create an API Key](./assets/linode-create-apikey.png)\\n\\nCreate a new key. I strongly recommend using a suitable label and setting an expiry date for the generated key.\\n\\n![Note the API Key](./assets/linode-note-apikey.png)\\n\\nCopy and past the API key into the `Vagrantfile`. As warned, the key will only be displayed once!\\n\\n## Deploy\\nWhen you are ready, execute the command:\\n\\n```bash\\nvagrant up --provider=linode\\n```\\n\\nSit back, wait for the OXAR system reboot countdown and then pop the champagne. Magic!\\n\\nThanks for reading!"},{"id":"id/cizx69jyk0015a0pvten1mngo","metadata":{"permalink":"/content/id/cizx69jyk0015a0pvten1mngo","source":"@site/posts/hexo-blogging-without-disks/index.md","title":"Hexo - Blogging Without Disks","description":"I feel like I have gone a full circle from my early days writing my blogs (or weblogs) in flat files as opposed to storing content in a database through a content management system (CMS). Prior to using Wordpress, I was heavily invested in developing with Drupal, and consider myself privileged to have used the knowledge of both platforms for work. All of sudden now, I have an urge to go back to my roots and focusing on creating blog posts without the bells and whistles.","date":"2016-03-02T19:43:10.000Z","formattedDate":"March 2, 2016","tags":[{"label":"blog","permalink":"/content/tags/blog"},{"label":"blogging","permalink":"/content/tags/blogging"},{"label":"platform","permalink":"/content/tags/platform"},{"label":"nodejs","permalink":"/content/tags/nodejs"},{"label":"markdown","permalink":"/content/tags/markdown"},{"label":"bootstrap","permalink":"/content/tags/bootstrap"}],"readingTime":4.715,"hasTruncateMarker":true,"authors":[{"name":"Adrian Png","title":"Senior Cloud Solutions Architect @ Insum","url":"https://github.com/fuzziebrain","imageURL":"https://github.com/fuzziebrain.png","key":"fuzziebrain"}],"frontMatter":{"title":"Hexo - Blogging Without Disks","date":"2016-03-02T19:43:10.000Z","slug":"id/cizx69jyk0015a0pvten1mngo","authors":["fuzziebrain"],"comments":false,"categories":["Technology","Others"],"tags":["blog","blogging","platform","nodejs","markdown","bootstrap"]},"prevItem":{"title":"APEX and the Affordable Cloud!","permalink":"/content/id/cizx69jya000ra0pvv26u3cez"},"nextItem":{"title":"That Time of the Year","permalink":"/content/id/498"}},"content":"I feel like I have gone a full circle from my early days writing my blogs (or weblogs) in flat files as opposed to storing content in a database through a content management system (CMS). Prior to using [Wordpress](https://www.wordpress.org/), I was heavily invested in developing with [Drupal](https://www.drupal.org), and consider myself privileged to have used the knowledge of both platforms for work. All of sudden now, I have an urge to go back to my roots and focusing on creating blog posts without the bells and whistles.\\n\x3c!-- truncate --\x3e\\nSo instead of manipulating raw HTML tags, I found a neat little tool written in [Node.js](https://nodejs.org) that allows me to write posts and pages in Markdown, while making use templates and plugins to generate the ubiquitous Web 2.0 technologies with minimal effort. It is called [Hexo](https://hexo.io/). I am not going through the basics of how to install and configure a blog site using Hexo as I find the documentation concise but comprehensive enough to get anyone started.\\n\\nHowever, what I think might be helpful is what I did to further customize the size to my taste.\\n\\n#### 1. Migrating Posts from the Old WordPress Website\\nI have a few posts on my previous blog and wanted to move over as many posts and pages as I could. The simple procedure from the [section](https://hexo.io/docs/migration.html) on migrating from other blog platforms is easy to follow. For WordPress though, You do have a choice on what you want to export and you can opt to export only posts if that is all that you needed to move over. Once you have a copy of the export XML file, copy that over to your site folder (`<site-folder>`) and run the command `$ hexo migrate wordpress my_wordpress_site.xml`. That will convert your posts (in XML format) to individual Markdown files. Posts and drafts are placed in separate folders and you can find them in `<site-folder>/source/_posts`  and `<site-folder>/source_drafts` respectively. If you need to move them around, here\'s your chance!\\n\\n#### 2. Theming\\nThis is probably one of the most important tasks!\\n\\nOut of the box, Hexo provides you with the Landscape theme. You might also have realized that the box titles on the sidebar are written in Chinese. If you are reading this post in English, I\'m gonna guess that this isn\'t what you want. The quick fix is to copy the file `<site-folder>/themes/landscape/languages/defaut.yml` and name it `en.yml`.\\n\\nFor me though, it\'s a no-brainer to seek out an alternative theme. I started out with the [theme](https://github.com/chrisjlee/hexo-theme-zurb-foundation) based on Zurb\'s [Foundation](http://foundation.zurb.com/), but found myself wandering back to [Bootstrap](http://getbootstrap.com/) as that is a framework that I am currently more familiar with. So I decided to grab a copy of the Bootstrap-based [theme](https://github.com/cgmartin/hexo-theme-bootstrap-blog) instead. Be sure to follow the instructions on what else to install and configure to get the theme running correctly for your website.\\n\\n#### 3. Plugins\\nThat\'s easy to share. Here\'s a list that wasn\'t too hard for me to compile:\\n```\\n{\\n  ...\\n  \\"dependencies\\": {\\n    \\"hexo\\": \\"^3.1.1\\",\\n    \\"hexo-front-matter-excerpt\\": \\"^0.2.0\\",\\n    \\"hexo-generator-archive\\": \\"^0.1.4\\",\\n    \\"hexo-generator-category\\": \\"^0.1.3\\",\\n    \\"hexo-generator-index\\": \\"^0.2.0\\",\\n    \\"hexo-generator-sitemap\\": \\"^1.1.2\\",\\n    \\"hexo-generator-tag\\": \\"^0.2.0\\",\\n    \\"hexo-migrator-wordpress\\": \\"^0.1.2\\",\\n    \\"hexo-renderer-ejs\\": \\"^0.1.1\\",\\n    \\"hexo-renderer-marked\\": \\"^0.2.9\\",\\n    \\"hexo-renderer-stylus\\": \\"^0.3.0\\",\\n    \\"hexo-server\\": \\"^0.1.3\\",\\n    \\"hexo-tag-bootstrap\\": \\"0.0.8\\",\\n    \\"hexo-tag-fontawesome\\": \\"^1.0.0\\"\\n  }\\n}\\n```\\nBear in mind though, that this is a dynamic compilation and will more than likely change in the near future! There are several [plugins](https://hexo.io/plugins/) available to you depending on your development style. So pick and choose what you feel is necessary and install/uninstall with simple `npm` commands. Just remember to include the `--save` parameter to persist your changes.\\n\\n#### 4. Version Control\\nIt is almost always necessary to store versions of your work for the purpose of backup and guarding against accidental disasters. The preferred choice for me is to use [Git](https://git-scm.com). For the purpose of working with my personal website, I opted to use [BitBucket](https://www.bitbucket,org) as they give me a much needed option of keeping my repositories private.\\n\\nI created two for my website. One for storing content, and the other for my theme of choice. I added the theme as a submodule in the main site repository. This allows me to work on both content and theme concurrently. Any change to the theme (as a submodule) can then be pushed and committed to the repository, almost at the same time. Take note that you might have to first commit the submodule before updating the repository with changes in the main project.\\n\\n#### 5. Tooling\\nThere is certainly no dearth of Markdown editors out there. Just take a look at this [list][8715fe38] of 78 compiled by Mashable in 2013! Since I do most of my development in [Atom](https://atom.io) these days, I had to find a package I could use to make it easier to edit my posts. I have barely started using [markdown-writer](https://atom.io/packages/markdown-writer) and it is already making my life a lot easier with shortcuts and widgets. The other neat feature is its support for [Hexo](https://hexo.io)! It actually understands the concept of draft and post in Hexo context so in theory, you would never have to use the commandline... well... not much at least.\\n\\nThe one thing I desire most in any kind of web editing tool is the ability to take a screenshot and paste it into the active document. If you come across such a plugin or tool, please don\'t hesitate to put a comment below or tweet me!\\n\\n#### Conclusion\\nI love Hexo. I love Markdown. I love Atom. Most of all, I love simplicity! Hope you\'ve enjoyed reading this post as much as I have writing it. This is after all, my virgin post using Hexo!\\n\\nHAPPY BLOGGING!\\n\\n  [8715fe38]: http://mashable.com/2013/06/24/markdown-tools \\"78 Tools for Writing and Previewing Markdown by Mashable\\""},{"id":"id/498","metadata":{"permalink":"/content/id/498","source":"@site/posts/that-time-of-the-year/index.md","title":"That Time of the Year","description":"Chicago Skyline","date":"2015-10-09T06:05:27.000Z","formattedDate":"October 9, 2015","tags":[{"label":"career","permalink":"/content/tags/career"},{"label":"blogging","permalink":"/content/tags/blogging"},{"label":"odtug","permalink":"/content/tags/odtug"},{"label":"knowledge sharing","permalink":"/content/tags/knowledge-sharing"}],"readingTime":1.025,"hasTruncateMarker":true,"authors":[{"name":"Adrian Png","title":"Senior Cloud Solutions Architect @ Insum","url":"https://github.com/fuzziebrain","imageURL":"https://github.com/fuzziebrain.png","key":"fuzziebrain"}],"frontMatter":{"title":"That Time of the Year","slug":"id/498","authors":["fuzziebrain"],"comment":false,"categories":["Technology","Programming","Oracle Application Express"],"date":"2015-10-09T06:05:27.000Z","tags":["career","blogging","odtug","knowledge sharing"]},"prevItem":{"title":"Hexo - Blogging Without Disks","permalink":"/content/id/cizx69jyk0015a0pvten1mngo"},"nextItem":{"title":"Thinking Back, Looking Forward","permalink":"/content/id/490"}},"content":"![Chicago Skyline](./assets/chicago-150543_640.png)\\nWe now have less than a week before [Kscope 16](http://www.kscope16.com) abstract submissions close. Have you submitted yours yet? If not, please spend a few minutes to consider the following reasons why you should:\\n\x3c!-- truncate --\x3e\\n1.  Sharing is good\\n2.  The best way to know a topic is to teach/present it\\n3.  A ~~complimentary~~\xa0free ticket to visit [Chicago](https://youtu.be/XpEGxHLJpK4)\\n4.  An opportunity to mix with the best and brightest, and as they say in Chinese: \u8fd1\u6731\u8005\u8d64,\u8fd1\u58a8\u8005\u9ed1 ![English translation of \u8fd1\u6731\u8005\u8d64,\u8fd1\u58a8\u8005\u9ed1](./assets/chinese_proverb.png)\\n5.  Collectibles\xa0galore\\n\\nAnd for the\xa0Oracle Application Express ([#orclapex](https://twitter.com/hashtag/orclapex)) crowd:\\n1.  Get to meet the\xa0APEX development team in-person to <del>report a bug</del> say: \\"Thank You!\\"\\n2.  Attend, or better still, take part in APEX Open Mic Night\\n3.  ~~Drink beer and play poker~~\\nI have been submitting abstracts each year\xa0since I was first introduced to the conference in 2011\\\\. Haven\'t had much success, but as with every life lesson... it\'s not only about the result, the process leading to it is equally as important. Each year, the submission process challenges me to think of innovative ways to build better APEX applications.\\n\\nStop hesitating, start dreaming and then\xa0[click that button](http://kscope16.com/content/submit-an-abstract)!"},{"id":"id/490","metadata":{"permalink":"/content/id/490","source":"@site/posts/thinking-back-looking-forward/index.md","title":"Thinking Back, Looking Forward","description":"After more than five years at the UBC James Hogg Research Centre (now also known as the Centre for Heart Lung Innovation), I am now ready to take the plunge and explore a familiar yet uncharted path. Familiar because I will continue working on a technology that I am very passionate about, though in a very different industry.","date":"2015-09-21T09:12:48.000Z","formattedDate":"September 21, 2015","tags":[{"label":"insum","permalink":"/content/tags/insum"},{"label":"orclapex","permalink":"/content/tags/orclapex"},{"label":"career","permalink":"/content/tags/career"},{"label":"change is constant","permalink":"/content/tags/change-is-constant"}],"readingTime":0.825,"hasTruncateMarker":true,"authors":[{"name":"Adrian Png","title":"Senior Cloud Solutions Architect @ Insum","url":"https://github.com/fuzziebrain","imageURL":"https://github.com/fuzziebrain.png","key":"fuzziebrain"}],"frontMatter":{"title":"Thinking Back, Looking Forward","slug":"id/490","authors":["fuzziebrain"],"comment":false,"categories":["Life"],"date":"2015-09-21T09:12:48.000Z","tags":["insum","orclapex","career","change is constant"]},"prevItem":{"title":"That Time of the Year","permalink":"/content/id/498"},"nextItem":{"title":"An Even Faster Guide to Build A Personal Oracle APEX Instance","permalink":"/content/id/447"}},"content":"After more than five years at the [UBC James Hogg Research Centre](https://www.hli.ubc.ca/) (now also known as the Centre for Heart Lung Innovation), I am now ready to take the plunge and explore a familiar yet uncharted path. Familiar because I will continue working on a technology that I am very passionate about, though in a very different industry.\\n\x3c!-- truncate --\x3e\\n![Photo by Dan Fador](./assets/dawn-190055_1280-1024x682.jpg)\\n\\nI am grateful for the many opportunities to have helped design and implement technical solutions that improved the Research Data Management capabilities at the centre. These include the use of Oracle, IBM, Novell, Talend and several open source products that have helped enhanced how we performed Research Data Capture, high-performance data analysis and supported operational workflows at the centre.\\n\\nWhile it\'s hard to leave my \\"baby\\" behind, as I start my new life chapter with [Insum](http://insum.ca/) tomorrow, I look forward to bringing with me my passion for technology, the desire to help customers excel and contributing to the company\'s success."},{"id":"id/447","metadata":{"permalink":"/content/id/447","source":"@site/posts/an-even-faster-guide-to-build-a-personal-oracle-apex-instance/index.md","title":"An Even Faster Guide to Build A Personal Oracle APEX Instance","description":"ORCLAPEX Meetup Logo","date":"2015-06-21T22:47:56.000Z","formattedDate":"June 21, 2015","tags":[{"label":"DevOps","permalink":"/content/tags/dev-ops"},{"label":"Linux","permalink":"/content/tags/linux"},{"label":"Oracle","permalink":"/content/tags/oracle"},{"label":"Oracle Application Express","permalink":"/content/tags/oracle-application-express"},{"label":"orclapex","permalink":"/content/tags/orclapex"},{"label":"Vagrant","permalink":"/content/tags/vagrant"},{"label":"VirtualBox","permalink":"/content/tags/virtual-box"}],"readingTime":2.14,"hasTruncateMarker":true,"authors":[{"name":"Adrian Png","title":"Senior Cloud Solutions Architect @ Insum","url":"https://github.com/fuzziebrain","imageURL":"https://github.com/fuzziebrain.png","key":"fuzziebrain"}],"frontMatter":{"title":"An Even Faster Guide to Build A Personal Oracle APEX Instance","tags":["DevOps","Linux","Oracle","Oracle Application Express","orclapex","Vagrant","VirtualBox"],"slug":"id/447","authors":["fuzziebrain"],"comment":false,"categories":["Technology","Programming","Oracle Application Express"],"date":"2015-06-21T22:47:56.000Z"},"prevItem":{"title":"Thinking Back, Looking Forward","permalink":"/content/id/490"},"nextItem":{"title":"Preparing for the Hands-on Event","permalink":"/content/id/440"}},"content":"![ORCLAPEX Meetup Logo](./assets/orclapex-meetup-logo.jpeg)\\n\\nWant to get started on APEX development but don\'t want to muck around with Linux too much, then this guide\'s for you! As a follow-up to our\xa0[second ORCLAPEX-YVR meetup](http://www.meetup.com/orclapex-YVR/events/223156789/) on **June 17, 2015**, and to supplement [Trent Schafer](https://twitter.com/trentschafer)\'s excellent [post](http://apextips.blogspot.com.au/2015/06/building-oracle-11g-xe-server-with-apex.html) on how you could build an Oracle APEX instance, here\'s a simple procedure that you can use to stand up your personal APEX development environment in probably much less than 30 minutes depending on your Internet connection bandwidth!\\n\\n\x3c!-- truncate --\x3e\\n\\nThis method involves the use of [Vagrant](https://www.vagrantup.com/) and\xa0[Oracle VirtualBox](https://www.virtualbox.org/), and will work on any host operating system (OS) that both software supports. So let\'s get started!\\n\\n1.  Download and save the following installation packages\xa0on your computer:\\n  * [Oracle Database Express Edition 11g Release 2](http://www.oracle.com/technetwork/database/database-technologies/express-edition/downloads/index.html)\\n  * [Oracle Application Express 5.0](http://www.oracle.com/technetwork/developer-tools/apex/downloads/index.html)\\n  * [Oracle REST Data Services 3.0](http://www.oracle.com/technetwork/developer-tools/rest-data-services/downloads/index.html)\\n2.  Download and install\xa0[Git](https://git-scm.com/)\xa0(you may use other Git clients if preferred, e.g. [SourceTree](https://www.sourcetreeapp.com/))\\n3.  Download and install\xa0[Oracle](https://www.virtualbox.org/)[VirtualBox](https://www.virtualbox.org/) (version 5.0.x)\\n4.  Download and install\xa0[Vagrant](https://www.vagrantup.com/) (version 1.7.x)\\n5.  Start a terminal session or the Command Prompt for the next few\xa0steps. **Note**:\xa0\\"vagrant\\" should have been added to your system path during the installation.\\n6.  Clone the [oraclexe](https://github.com/OraOpenSource/oraclexe-apex)[-apex](https://github.com/OraOpenSource/oraclexe-apex) project:\\n`$ git clone https://github.com/OraOpenSource/OXAR.git`\\n7.  Change your working directory to oraclexe-apex\\n`$ cd OXAR`\\n8.  Copy the installation files into files subdirectory. ![OXAR project folder structure](./assets/oraclexe-apex-project-dir-structure-112x300.png)\\n9.  Update the config.properties. Specify the location and filenames of the installation files. ![OXAR Project Configuration Changes]./assets/oraclexe-apex-project-config-changes-300x74.png)\\n10.  Execute the command:\\n`$ vagrant up`\\n11.  Done! Wait for the build process to complete and then test your instance using your browser and point to [http://localhost:50080/ords/](http://localhost:50080/ords/). If you see the APEX login\xa0page, then you are good to go!\\nBy default, the Vagrant is configured to use the [Centos](http://centos.org/) 7 base box provided by\xa0[Puppet Labs](https://puppetlabs.com/). If [Ubuntu](http://www.ubuntu.com/) is preferred, then try switching the base box [ubuntu/trusty64](https://atlas.hashicorp.com/ubuntu/boxes/trusty64). To do so, update the vagrantfile and set the desired base box.\\n\\n![Change the base box used by the oraclexe-apex project](./assets/oraclexe-apex-project-box-change-300x32.png)\\nFor the trust64 base box provided by Ubuntu, the Oracle REST Data Services (ORDS) application may\xa0not run out-of-the-box (pun intended). If the\xa0time-zone for the guest OS has not been defined,\xa0then that might result\xa0in a JDBC connection error ([ORA-01882](http://docs.oracle.com/cd/E11882_01/server.112/e17766/e1500.htm#ORA-01882)). If you encounter a similar issue, please perform the following steps to fix the problem:\\n\\n1.  Login to the guest OS using SSH. The username and password are both \\"vagrant\\".\\n`$ ssh -l vagrant -p 2222 localhost`\\n2.  Execute the command and then use the wizard to set the correct timezone for your system:\\n`$ sudo dpkg-reconfigure tzdata`\\n3.  Reboot the server:\\n`$ sudo /sbin/reboot`"},{"id":"id/440","metadata":{"permalink":"/content/id/440","source":"@site/posts/preparing-for-the-hands-on-event/index.md","title":"Preparing for the Hands-on Event","description":"Here\'s a little weekend activity for those of you planning to attend our second ORCLAPEX-YVR meetup on June 17, 2015. To avoid network congestion during the event, please perform the following preparatory steps:","date":"2015-06-12T21:27:44.000Z","formattedDate":"June 12, 2015","tags":[{"label":"vagrant","permalink":"/content/tags/vagrant"},{"label":"OXAR","permalink":"/content/tags/oxar"},{"label":"devops","permalink":"/content/tags/devops"},{"label":"workshop","permalink":"/content/tags/workshop"},{"label":"training","permalink":"/content/tags/training"}],"readingTime":0.805,"hasTruncateMarker":true,"authors":[{"name":"Adrian Png","title":"Senior Cloud Solutions Architect @ Insum","url":"https://github.com/fuzziebrain","imageURL":"https://github.com/fuzziebrain.png","key":"fuzziebrain"}],"frontMatter":{"title":"Preparing for the Hands-on Event","slug":"id/440","authors":["fuzziebrain"],"comment":false,"categories":["Technology","Programming","Oracle Application Express"],"date":"2015-06-12T21:27:44.000Z","tags":["vagrant","OXAR","devops","workshop","training"]},"prevItem":{"title":"An Even Faster Guide to Build A Personal Oracle APEX Instance","permalink":"/content/id/447"},"nextItem":{"title":"Oracle APEX Meetup in YVR","permalink":"/content/id/426"}},"content":"Here\'s a little weekend activity for those of you planning to attend our [second ORCLAPEX-YVR meetup](http://www.meetup.com/orclapex-YVR/events/223156789/) on **June 17, 2015**. To avoid network congestion during the event, please perform the following preparatory steps:\\n\x3c!-- truncate --\x3e\\n1. Download and install\xa0[Git](https://git-scm.com/)\xa0(you may use other Git clients if preferred, e.g. [SourceTree](https://www.sourcetreeapp.com/))\\n2. Download and install\xa0[Oracle](https://www.virtualbox.org/)[VirtualBox](https://www.virtualbox.org/) (version 4.3.28)\\n3. Download and install\xa0[Vagrant](https://www.vagrantup.com/) (version 1.7.2)\\n4. Start a terminal session or the Command Prompt for the next few\xa0steps. **Note**:\xa0\\"vagrant\\" should have been added to your system path during the installation.\\n5. Add the box \\"chef/centos-7.0\\":\\n`$ vagrant box add chef/centos-7.0`\\n6. Clone the [oraclexe](https://github.com/OraOpenSource/oraclexe-apex)[-apex](https://github.com/OraOpenSource/oraclexe-apex) project:\\n`$ git clone https://github.com/OraOpenSource/oraclexe-apex.git`\\n7. Change your working directory to oraclexe-apex\\n`$ cd oraclexe-apex`\\n8. Checkout the last release version:\\n`$ git checkout tags/v0.3.0`\\n9. Download and save the following installation packages\xa0on your computer:\\n  * [Oracle Database Express Edition 11g Release 2](http://www.oracle.com/technetwork/database/database-technologies/express-edition/downloads/index.html)\\n  * [Oracle Application Express 5.0](http://www.oracle.com/technetwork/developer-tools/apex/downloads/index.html)\\n  * [Oracle REST Data Services 2.0.10](http://www.oracle.com/technetwork/developer-tools/rest-data-services/downloads/ords-downloads-2010-2528184.html)\\n\\nThat\'s it! See you at the meetup!"},{"id":"id/426","metadata":{"permalink":"/content/id/426","source":"@site/posts/oracle-apex-meetup-in-yvr/index.md","title":"Oracle APEX Meetup in YVR","description":"My first adventure in organizing meetups started late last year with the creation of the Vancouver Research Data Management Roundtable. The goal of the group was to gather members of both academia and industry, and engage in discussion on topics, issues and solutions in managing research data. We have since found a new home for the group. If\xa0RDM\xa0excites you, then I warmly welcome you to sign up for our mailing list\xa0kindly hosted by The University of British Columbia. Though it does help that you live around the area! We have a monthly \\"Data Cafe\\" that is co-organized with the UBC library folks.","date":"2015-02-17T19:32:07.000Z","formattedDate":"February 17, 2015","tags":[{"label":"meetup","permalink":"/content/tags/meetup"},{"label":"user groups","permalink":"/content/tags/user-groups"},{"label":"Vancouver","permalink":"/content/tags/vancouver"},{"label":"orclapex","permalink":"/content/tags/orclapex"}],"readingTime":1.155,"hasTruncateMarker":true,"authors":[{"name":"Adrian Png","title":"Senior Cloud Solutions Architect @ Insum","url":"https://github.com/fuzziebrain","imageURL":"https://github.com/fuzziebrain.png","key":"fuzziebrain"}],"frontMatter":{"title":"Oracle APEX Meetup in YVR","slug":"id/426","authors":["fuzziebrain"],"comment":false,"categories":["Technology","Programming","Oracle Application Express"],"date":"2015-02-17T19:32:07.000Z","tags":["meetup","user groups","Vancouver","orclapex"]},"prevItem":{"title":"Preparing for the Hands-on Event","permalink":"/content/id/440"},"nextItem":{"title":"Post-KScope Blues","permalink":"/content/id/391"}},"content":"My first adventure in organizing [meetups](http://www.meetup.com/) started late last year with the creation of the Vancouver Research Data Management Roundtable. The goal of the group was to gather members of both academia and industry, and engage in discussion on topics, issues and solutions in managing research data. We have since found a new home for the group. If\xa0[RDM](http://www2.le.ac.uk/services/research-data/rdm/what-is-rdm)\xa0excites you, then I warmly welcome you to sign up for our [mailing list](https://lists.ubc.ca/scripts/wa.exe?A0=VANRDM-ROUNDTABLE)\xa0kindly hosted by [The University of British Columbia](http://www.ubc.ca/). Though it does help that you live around the area! We have a monthly \\"Data Cafe\\" that is co-organized with the UBC library folks.\\n\x3c!-- truncate --\x3e\\nWith that move, the opportunity opened up for me to start yet another meetup to make sure the [Oracle APEX](https://apex.oracle.com/) community in Canada\xa0had adequate representation on both the east and west coasts! Notice has been served to current VanRDM members, asking them to cross over to the new mailing list before I overhaul the website. The link however, has been secured.\\n\\nI will not be pruning the members list come **February 28**, choosing instead to let members decide whether to stay or leave. After all, APEX is one of the best tools out there to help facilitate best practices in data management. So if you are like me: (a) living in Vancouver and (b) passionate about Oracle APEX, then I strongly encourage\xa0you to [sign up](http://www.meetup.com/orclapex-YVR/join/) immediately!"},{"id":"id/391","metadata":{"permalink":"/content/id/391","source":"@site/posts/post-kscope-blues/index.md","title":"Post-KScope Blues","description":"I traditionally write a summary of what I learned and experienced at each technical conference I attend. I do this in order to help both my sponsors\xa0and I to\xa0take stock of how much value was created by attending a training\xa0event. I find this a good practice and give credit to my previous employer Nanyang Polytechnic for instilling this culture in me. This year, I decided to do this as a blog post as well and hopefully, provide others an opportunity to decide if KScope15 should be on your training calendar next year.","date":"2014-06-29T23:06:08.000Z","formattedDate":"June 29, 2014","tags":[{"label":"review","permalink":"/content/tags/review"},{"label":"ODTUG","permalink":"/content/tags/odtug"},{"label":"orclapex","permalink":"/content/tags/orclapex"},{"label":"training","permalink":"/content/tags/training"},{"label":"learning","permalink":"/content/tags/learning"}],"readingTime":5.135,"hasTruncateMarker":true,"authors":[{"name":"Adrian Png","title":"Senior Cloud Solutions Architect @ Insum","url":"https://github.com/fuzziebrain","imageURL":"https://github.com/fuzziebrain.png","key":"fuzziebrain"}],"frontMatter":{"title":"Post-KScope Blues","tags":["review","ODTUG","orclapex","training","learning"],"slug":"id/391","authors":["fuzziebrain"],"comment":false,"categories":["Technology","Conferences"],"date":"2014-06-29T23:06:08.000Z"},"prevItem":{"title":"Oracle APEX Meetup in YVR","permalink":"/content/id/426"},"nextItem":{"title":"Generating XSL-FO Layouts with Microsoft Word and Apache POI","permalink":"/content/id/332"}},"content":"I traditionally write a summary of what I learned and experienced at each technical conference I attend. I do this in order to help both my sponsors\xa0and I to\xa0take stock of how much value was created by attending a training\xa0event. I find this a good practice and give credit to my previous employer [Nanyang Polytechnic](http://www.nyp.edu.sg/) for instilling this culture in me. This year, I decided to do this as a blog post as well and hopefully, provide others an opportunity to decide if [KScope15](http://www.kscope15.com/) should be on your training calendar next year.\\n\x3c!-- truncate --\x3e\\n# Sessions\\n![\\"Space Needle\\"](./assets/kscope14_spaceneedle.jpg)\\nKScope typically begins with a community-based symposium during which representatives from [Oracle](http://oracle.com \\"Oracle Corporation\\") provide a day-long marathon of sessions that usually gives quite a good insight into what\'s on the horizon. This year, for [Oracle APEX](http://apex.oracle.com/) developers, our eyes are on what to expect with the upcoming - game changing - features of APEX 5.0\\\\. Not surprisingly, Joel Kallman ([@joelkallman](https://twitter.com/joelkallman)) and his talented team of developers did not disappoint and provided us hours of woos and wows. The chant \\"[Everything is Awesome](http://youtu.be/StTqXEQ2l-Y)\\" was soon on everyone\'s slides, minds and lips.\\n\\nWell, I didn\'t attend all the APEX symposium sessions. Steven Feuerstein ([@sfonplsql](https://twitter.com/sfonplsql)) spoke about the \\"Best PL/SQL Features You Never Heard Of\\" and I was glad I attended.\\n\\nThere were quite a few other APEX sessions that caught my attention this year. John Scott ([@aejes](https://twitter.com/aejes)) and Dan McGhan ([@dmcghan](https://twitter.com/dmcghan)) presented very interesting use cases for Javascript frameworks [Node.js](http://www.nodejs.org/) and [AngularJS](http://www.angularjs.org/ \\"AngularJS\\"). Peter Raganitsch ([@PeterRaganitsch](https://twitter.com/PeterRaganitsch)) and Christian Rokitta ([@crokitta](https://twitter.com/crokitta)) focused on important issues and lessons to learn from Oracle Packaged Applications when making APEX applications public. Kris Rice ([@krisrice](https://twitter.com/krisrice)) talked about the best practices and considerations when deploying Oracle RESTful Data Services (ORDS, a.k.a. APEX listener) for production, as well as some interesting tricks and extensibility features. And I attended a hands-on session that provided examples on how you can setup [Continuous Integration](http://en.wikipedia.org/wiki/Continuous_integration \\"Continuous integration\\") for your APEX development environment. Lessons/tips that I could definitely apply back at work.\\n\\nAt heart, I am still very much a Java developer. So this year, I decided forgo a couple of novice-level APEX sessions to learn some basics about ADF. I attend two sessions. Luc Bors ([@lucb_](https://twitter.com/lucb_)) presented on ADF Essentials and Fr\xe9d\xe9ric Desbiens ([@BlueberryCoder](https://twitter.com/BlueberryCoder)) provided a lively one-hour ADF quick start session. These were fruitful and I found ADF to be Java\'s equivalent to APEX in developing web applications more productive.\\n\\nI attended a few sessions on data modelling using Oracle SQL Developer Data Modeler that turned out to be a good decision. Learned a few tips and tricks from Kent Graziano ([@KentGraziano](https://twitter.com/KentGraziano)) on how to use SDDM a little better. There\'s really a lot of features in SDDM that can help improve the process and working with clients better.\\n\\nThere were many others I would have liked to attend, including freshly minted Oracle ACE Jorge Rimblas\' ([@rimblas](https://twitter.com/rimblas)) session on \\"Creating a Business-Oriented UI in APEX\\", but he was competing against Tom Kyte\'s ([@OracleAskTom](https://twitter.com/OracleAskTom)) \\"Five Best Things to Happen to SQL\\". Woes of a conference attendee!\\n\\nIf I had to pick sessions that made the biggest impact, they would be:\\n1.  \\"Application Express: Fast = True\\" by Joel Kallman. What did I learn? Knowing APEX isn\'t quite enough to build good scalable applications. We need to have and invest in Oracle Database skills.\\n2.  \\"The Database and Developer\'s Toolkit: How to Find and Fix Your (APEX, Java, .NET, OBIEE, etc.) Application Performance Problem\\" by Cary Millsap ([@CaryMillsap](https://twitter.com/CaryMillsap)). That\'s a very long title for a talk, and so was the session - two hours! Amazingly though, Cary was able to take us through the entire session with ease. He captured my attention throughout. I liked how his slides were concise, well laid out and used pleasant pastel colours. I definitely have lots to learn, besides measuring application metrics and troubleshooting performance issues, how to present well!\\n\\n# Competition Finale\\nLate last year, I was approached by Roel Hartman ([@RoelH](https://twitter.com/RoelH)) and Martin D\'Souza ([@martindsouza](https://twitter.com/martindsouza)) to help organize the [APEX Theming Competition 2014](http://competition.odtug.com/) as a prelude to KScope. It wasn\'t too hard to persuade me to take up this challenge and I\'m glad I did. Learned a little bit more about APEX theming myself. The competition ended with\xa0the results announced at the annual APEX Open Mic Night.\xa0I was definitely nervous (as Joel picked out)\u2026 who wouldn\'t be. I was speaking to well-attended community event with some of the best APEX brains\xa0in the world! So please excuse my jitters if you were there that night.\\n\\nMy heartiest congratulations to Robert Schaefer ([@SchaeferRob](https://twitter.com/SchaeferRob)) who won the competition with this [entry](http://apex-plugin.com/oracle-apex-plugins/odtug-theming-competition/contribution-by-r.schaefer_385.html). It\'s a pity though that I wasn\'t able to meet him in person and present him with the grand prize.\\n\\n# Should You Attend KScope15?\\n![\\"ODTUG Rocks at the Experience Music Project\\"](./assets/kscope14_emp.jpg)\\n\\nIn a nutshell... **YES!!!**\\n\\n[ODTUG](http://www.odtug.com/) always puts up a great show. Check out my previous blog [post](/content/id/274)\xa0on my KScope12 experience. They didn\'t disappoint again this year with a great selection of topics, swag, venue, hotel and FOOD! You can also almost always expect a unique experience at their special events night. I rarely attend such events when I\'m at conferences, but this year, I made an exception and was glad\xa0I did.\\n\\nBecause ODTUG is home to several technology-specific communities, it is easy to pick up a breadth of new skills and knowledge at one conference. For me, I certainly benefited from attending topics from the DBA, Development, ADF and APEX tracks. This is useful for me as I undertake several complimentary roles in my current job.\\n\\nLast but not least, ODTUG is the COMMUNITY, where members know each other by name. Every time this community meets, it\'s like a family reunion. While there might be sibling (but constructive) rivalry at times, there remains a strong bond between members that allows everyone to share each other\'s successes and pains. My job situation is far from ideal at the moment, but I certainly hope to stay working with Oracle technologies and continue growing with this community in the years to come. And maybe, I\'ll get to meet you in [Hollywood, Florida](http://maps.google.com/maps?ll=26.0213888889,-80.175&spn=0.1,0.1&q=26.0213888889,-80.175) for KScope15 next year!"},{"id":"id/332","metadata":{"permalink":"/content/id/332","source":"@site/posts/generating-xsl-fo-layouts-with-microsoft-word-and-apache-poi/index.md","title":"Generating XSL-FO Layouts with Microsoft Word and Apache POI","description":"The objective of this post is to describe how to create XSL-FO Layouts with little or no costs! These can then be used\xa0for custom PDF documents\xa0printing in Oracle Application Express (APEX).","date":"2013-05-31T20:53:46.000Z","formattedDate":"May 31, 2013","tags":[{"label":"Apache POI","permalink":"/content/tags/apache-poi"},{"label":"Java","permalink":"/content/tags/java"},{"label":"Microsoft Word","permalink":"/content/tags/microsoft-word"},{"label":"Oracle Application Express","permalink":"/content/tags/oracle-application-express"},{"label":"Oracle BI Publisher","permalink":"/content/tags/oracle-bi-publisher"},{"label":"PDF","permalink":"/content/tags/pdf"},{"label":"reporting","permalink":"/content/tags/reporting"},{"label":"XPath","permalink":"/content/tags/x-path"},{"label":"XSL-FO","permalink":"/content/tags/xsl-fo"}],"readingTime":2.015,"hasTruncateMarker":true,"authors":[{"name":"Adrian Png","title":"Senior Cloud Solutions Architect @ Insum","url":"https://github.com/fuzziebrain","imageURL":"https://github.com/fuzziebrain.png","key":"fuzziebrain"}],"frontMatter":{"title":"Generating XSL-FO Layouts with Microsoft Word and Apache POI","tags":["Apache POI","Java","Microsoft Word","Oracle Application Express","Oracle BI Publisher","PDF","reporting","XPath","XSL-FO"],"slug":"id/332","authors":["fuzziebrain"],"comment":false,"categories":["Technology","Programming","Oracle Application Express"],"date":"2013-05-31T20:53:46.000Z"},"prevItem":{"title":"Post-KScope Blues","permalink":"/content/id/391"},"nextItem":{"title":"My KScope Experience (and Desires)","permalink":"/content/id/274"}},"content":"The objective of this post is to describe how to create XSL-FO Layouts with little or no costs! These can then be used\xa0for custom [PDF documents](http://www.slideshare.net/ \\"SlideShare\\")\xa0printing in [Oracle Application Express](http://apex.oracle.com \\"Oracle Application Express\\") (APEX).\\n\\nLet\'s get started...\\n\\n\x3c!-- truncate --\x3e\\n\\nTools that you will need:\\n\\n1. [Microsoft Word](http://office.microsoft.com/en-us/word/ \\"Microsoft Word\\") 97 and up.\\n1. [Apache POI](http://poi.apache.org \\"Apache POI\\") binaries.\\n\\n### Step 1\\nDesign your report template using Microsoft Word and populate the document with variable placeholders.\\n![Design the form/report](./assets/apex_custom_pdf_report_06.png)\\n\\n### Step 2\\nSave this template as a <u>Word 97-2003</u> document.\\n\\n### Step 3\\nDownload the latest Apache POI binaries. At the time of writing, the latest version is 3.9 and the utility needed is still under development. The two [Java](http://www.oracle.com/technetwork/java/ \\"Java (programming language)\\") library (jar) files are highlighted.\\n![Files in Apache POI](./assets/apex_custom_pdf_report_07.png)\\n\\n### Step 4\\nExtract the package and change the current directory to the folder the files were extracted to.\\n\\n### Step 5\\nFor convenience, copy/move the template document to the same directories as the Java libraries.\\n\\n### Step 6\\nPerform the Word to FO conversion. Enter the command as highlighted.\\n![Executing the Word-to-FO Conversion Process](./assets/apex_custom_pdf_report_09.png)\\n\\n### Step 7\\nThe contents of the FO file should be something similar to this:\\n![Sample Contents](./assets/apex_custom_pdf_report_10.png)\\n\\n### Step 8\\nPlace the contents of this file - stuff between the &lt;fo:root&gt; element - within a transformation template.\\n![XSLT Template](./assets/apex_custom_pdf_report_11.png)\\n\\nNote: If [Oracle BI Publisher](http://www.oracle.com/technology/products/xml-publisher/index.html \\"Oracle BI Publisher\\") is used to generate the PDFs, then remove the and any child elements within. Consulting the [docs](http://docs.oracle.com/cd/E28280_01/bi.1111/e22254/toc.htm) for supported elements would be helpful.\\n\\n### Step 9\\nReplace the placeholders defined in the Word document with elements.\\n![Placeholders to be replaced](./assets/apex_custom_pdf_report_15.png)\\n\\nUse [XPath](http://en.wikipedia.org/wiki/XPath \\"XPath\\") to select the values from the [XML](http://en.wikipedia.org/wiki/XML \\"XML\\") data file generated by APEX.\\n![Embedding Query Results using XPath](./assets/apex_custom_pdf_report_17.png)\\n\\n### Step 10\\nImages would be removed and replaced by a placeholder comment tag.\\n![Image Placeholder](./assets/apex_custom_pdf_report_13.png)\\n\\n### Step 11\\nInclude the image file as &lt;fo:external-graphic&gt; elements.\\n![Adding an External Graphic](./assets/apex_custom_pdf_report_14.png)\\n\\n### Step 12\\nSave the file with an XML extension. To see this template in action, click on the \\"Print\\" button [here](http://apex.oracle.com/pls/apex/f?p=69146:301:0::::P301_EMPNO:7782).\\n\\nKnown Issue: The graphics element renders correctly when using Oracle APEX Listener 2.0 embedded Apache FOP print rendering engine, but not on apex.oracle.com, which uses Oracle BI Publisher. I am still trying to figure out why and if you do know the reason, I\'d be happy if you could share it with me. Thanks! :-)"},{"id":"id/274","metadata":{"permalink":"/content/id/274","source":"@site/posts/my-kscope-experience-and-desires/index.md","title":"My KScope Experience (and Desires)","description":"My room view at the JW Marriott Hill Country Resort","date":"2013-04-27T22:42:34.000Z","formattedDate":"April 27, 2013","tags":[{"label":"travel","permalink":"/content/tags/travel"},{"label":"orclapex","permalink":"/content/tags/orclapex"},{"label":"ODTUG","permalink":"/content/tags/odtug"},{"label":"San Antonio","permalink":"/content/tags/san-antonio"},{"label":"learning","permalink":"/content/tags/learning"},{"label":"training","permalink":"/content/tags/training"}],"readingTime":3.41,"hasTruncateMarker":true,"authors":[{"name":"Adrian Png","title":"Senior Cloud Solutions Architect @ Insum","url":"https://github.com/fuzziebrain","imageURL":"https://github.com/fuzziebrain.png","key":"fuzziebrain"}],"frontMatter":{"title":"My KScope Experience (and Desires)","slug":"id/274","authors":["fuzziebrain"],"comment":false,"categories":["Technology","Conferences"],"tags":["travel","orclapex","ODTUG","San Antonio","learning","training"],"date":"2013-04-27T22:42:34.000Z"},"prevItem":{"title":"Generating XSL-FO Layouts with Microsoft Word and Apache POI","permalink":"/content/id/332"},"nextItem":{"title":"Spring into Spring Roo","permalink":"/content/id/216"}},"content":"![My room view at the JW Marriott Hill Country Resort](./assets/marriott_room_view.jpg)\\nThe much anticipated\xa0[Kscope 2013](http://kscope13.com/)\xa0conference will be starting soon in June this year. I felt it would be befitting to share my first-time experience at this event especially with anyone still procrastinating about signing up. This is after all, what Kscope is all about - sharing knowledge!\\n\x3c!-- truncate --\x3e\\nKscope 2012 was for me, a trip with many firsts. It was my first visit to [San Antonio, Texas](http://maps.google.com/maps?ll=29.4166666667,-98.5&spn=0.1,0.1&q=29.4166666667,-98.5) and I was extremely impressed. I was of course very delighted to have had a couple of hours in [Houston](http://maps.google.com/maps?ll=29.7627777778,-95.3830555556&spn=0.1,0.1&q=29.7627777778,-95.3830555556) prior to arriving at my final destination. Just so I could have a quick lunch of BBQ pulled pork and a loaded potato. Tummy well-stocked for the next five days of intensive knowledge infusion.\\n\\n![\\"Front entrance to the resort\\"](./assets/marriott_lobby_view.jpg)\\nThe organizers at ODTUG certainly have great tastes when choosing a\xa0location to host the event. Last year, Kscope was held at the magnificent JW Marriott San Antonio Hill Country. The views and ambience were breathtaking. Rooms were clean and service was top notch! Thank goodness I arrived a day early and managed to squeeze a quick tour of the compound and soaked up the warmth on a [Segway](http://www.segway.com/ \\"Segway PT\\")!\\n\\nThe conference was well-packed with technical sessions covering a broad range of Oracle products and technologies. I really only had time to focus on topics on Oracle Application Express (APEX) and had hard enough time trying to figure out my schedule. Much of the APEX content focused on howtos and case studies. The [Oracle APEX](http://apex.oracle.com \\"Oracle Application Express\\") team were also present in near fullforce to give you a glimpse of what is to come, and it was very useful to me in my career to plan ahead and build for the future.\\n\\nThere were also daily activities to keep you on your feet, for example, [Kent Graziano](http://kentgraziano.com/)\'s morning Chi Gung ([Qi Gong](http://en.wikipedia.org/wiki/Qigong \\"Qigong\\")) sessions. Another personal favourite was the APEX Open Mic Night that was an hour-long (or more) session where the stage was open for any courageous APEX developer to showcase his/her current work. It\'s a real eye-opener for an APEX noob like me to learn about the exciting and creative ways that APEX has been used to deliver real-world apps.\\n\\n![\\"The signage outside the ballroom where I presented my session\\"](./assets/door_signage.jpg)\\nI was fortunate to have attended both Kscope and [Oracle OpenWorld](http://www.oracle.com/openworld \\"Oracle OpenWorld\\") and here\'s a quick comparison I\'d like to make. The two\xa0events have a very different climate/atmosphere. OpenWorld felt like a huge roadshow and festival, whereas Kscope was pretty much a hobbyist\'s bootcamp. While both events provided an opportunity to meet key players in the Oracle world, Kscope was a much cosier setting and placed producers and consumers in close quarters.\\n\\nLast but not least, I had the privilege to present at Kscope 2012 and was glad I did. I am an introvert and pessimist. Even though I had presented at conferences in the past, I still had the jitters as I knew I was in the company of\xa0intellectuals\xa0and experts. However the warmth of the organizers, attendees and community eased my fears knowing that everyone always had something good to share. It may not have been a full-house that day, but I was glad to be in the company of peers, colleagues and a very special guest. More importantly, I now feel a much greater desire to contribute to the developer community.\\n\\n![\\"A little competition always helps...\\"](./assets/kscope_fun_n_games.jpg)\\nUnfortunately, I would not be able to attend Kscope this year for varying reasons. If you can, then you should.I wish I could have written this sooner so that readers may take advantage of the early bird pricing. But better late than never. Besides, the friends you will make and knowledge gained will more than compensate the extra couple of hundred bucks. And if you do go, please consider volunteering either to assist at sessions or at the [Community Service Day](http://kscope13.com/events/community-service-day). Last but not least, make lots of new friends, have great fun and I hope to meet you in-person at Kscope 14!"},{"id":"id/216","metadata":{"permalink":"/content/id/216","source":"@site/posts/spring-into-spring-roo/index.md","title":"Spring into Spring Roo","description":"I was very excited to receive my copy of Spring Roo in Action. This is a fairly recent MEAP offering from Manning and is scheduled to be ready by the end of this year (2011). A 500-page quick-start guide, and IMHO, the missing manual for anyone interested in learning agile and rapid prototyping using Spring Roo.","date":"2011-08-30T22:54:31.000Z","formattedDate":"August 30, 2011","tags":[{"label":"spring framework","permalink":"/content/tags/spring-framework"},{"label":"spring roo","permalink":"/content/tags/spring-roo"},{"label":"springsource","permalink":"/content/tags/springsource"},{"label":"java","permalink":"/content/tags/java"}],"readingTime":3.47,"hasTruncateMarker":true,"authors":[{"name":"Adrian Png","title":"Senior Cloud Solutions Architect @ Insum","url":"https://github.com/fuzziebrain","imageURL":"https://github.com/fuzziebrain.png","key":"fuzziebrain"}],"frontMatter":{"title":"Spring into Spring Roo","tags":["spring framework","spring roo","springsource","java"],"slug":"id/216","authors":["fuzziebrain"],"comment":false,"categories":["Reviews","Books"],"date":"2011-08-30T22:54:31.000Z"},"prevItem":{"title":"My KScope Experience (and Desires)","permalink":"/content/id/274"},"nextItem":{"title":"The Elephant Rider in Action","permalink":"/content/id/185"}},"content":"I was very excited to receive my copy of [Spring Roo in Action](http://www.manning.com/rimple/). This is a fairly recent MEAP offering from Manning and is scheduled to be ready by the end of this year (2011). A 500-page quick-start guide, and IMHO, the missing manual for anyone interested in learning agile and rapid prototyping using Spring Roo.\\n\\n\x3c!-- truncate --\x3e\\n\\nBefore I continue, some brief background about me and my experiences working with the [Spring Framework](http://www.springsource.org \\"Spring Framework\\"). I took an interest in Spring a couple of years ago while working on a web application to manage administrative data about a large-scale biotechnology research project. Not too long after starting on my project, I heard about Spring Roo. At that time, it was a rather new offering from [SpringSource](http://www.springsource.org \\"SpringSource\\"), before version 1.0 was ready for prime time. It was and still is a great tool for very quickly developing straight-forward [CRUD](http://en.wikipedia.org/wiki/Create%2C_read%2C_update_and_delete \\"Create, read, update and delete\\") applications. However, my interest soon drained as I was concerned that the ease of development would compromise my ability to customise applications for customers. That opinion changed soon after reading this book. I\'ll explain later.\\n\\nTen chapters have been written so far, providing a comprehensive coverage of key component technologies that make the Spring Framework such a great technology for developing web and enterprise applications - database, web, messaging, integration and testing. Even with the ease of designing and configuring software using Spring - as the authors aptly point out - developers are often caught in a conumdrum of chosing the right set of frameworks. Often losing sight of our primary goals: building usable software that meets clients\' expectations (and schedules).\\n\\nHowever, as mentioned earlier, I was concerned that the Spring Roo was generating a lot of artifacts that are deeply intertwined with the tools, such that it becomes very difficult to customise without breaking any code. I liken this to a beginner .NET developer experience. [Visual Studio](http://www.microsoft.com/visualstudio \\"Microsoft Visual Studio\\") was a very powerful [IDE](http://en.wikipedia.org/wiki/Integrated_development_environment \\"Integrated development environment\\") that allows anyone (well almost) to create usable applications in a short amount of time. That is definitely useful during early stages of development, where it was important to quickly prototype and impress your prospective customers. As time goes by, as we learn more and more about the user\'s requirements, we begin to realise that there\'s a fair amount of tweaking to be done. Then you\'re hooped.\\n\\nI feel the authors were very conscious of this issue, and throughout the early chapters, assured me that there was a not-too-difficult exit strategy when the project needed more intricate plumbing: Push-In Refactoring. There is an entire chapter devoted to this topic and I certainly look forward to reading it when ready.\\n\\nThere\'s so much to say about this book, but I think my remaining thoughts are best condensed into these key points:\\n*   The book is well written with the developer at heart. The authors exhibit great empathy with the many issues that developers face daily.\\n*   Well organised and covers all layers of the development cake.\\n*   Clear concise instructions, great visuals and easy to follow. All you need is a computer with tools installed to follow along.\\n*   Talks about IDE integration: SpringSource Tool Suite (STS) and IDEA\'s [IntelliJ](http://www.jetbrains.com/idea/ \\"IntelliJ IDEA\\"). For STS, they don\'t just stop at the basics, there are some tips and tricks that could be very helpful in your development experience.\\n*   The authors don\'t just talk about all things Roo. There are some good pointers and guidance for software development and architecture considerations and best practices.\\n*   Chapter 11 is about [Cloud Computing](http://en.wikipedia.org/wiki/Cloud_computing \\"Cloud computing\\"). I\'m gonna guess that it has to do with CloudFoundry and the newly released MicroCloud. As developers and solution providers, as much as we hate buzz words, Cloud Computing is here to stay and I see this as a very important technology in the year ahead.\\n\\nPrior knowledge about the Spring Framework would be good, but definitely not a prerequisite. If Spring Roo is a beginner\'s portal into the world of Spring, then this book is your wormhole. If only I could give this must-have handbook a 5.01 rating!"},{"id":"id/185","metadata":{"permalink":"/content/id/185","source":"@site/posts/the-elephant-rider-in-action/index.md","title":"The Elephant Rider in Action","description":"I was very excited to get my hands on a copy of Mahout in Action. Artificial Intelligence and Machine Learning are my most favourite subjects in Computer Science. Back during my school days, AI and ML were primarily used in research, and sometimes banks use them for fraud detection. I am glad that it is gradually being adopted in mainstream businesses with the help of easy to use frameworks like Mahout, backed by commodity distributing platforms like Hadoop.","date":"2011-02-08T07:11:32.000Z","formattedDate":"February 8, 2011","tags":[{"label":"mahout","permalink":"/content/tags/mahout"},{"label":"artificial intelligence","permalink":"/content/tags/artificial-intelligence"},{"label":"computer science","permalink":"/content/tags/computer-science"},{"label":"hadoop","permalink":"/content/tags/hadoop"},{"label":"machine learning","permalink":"/content/tags/machine-learning"},{"label":"mapreduce","permalink":"/content/tags/mapreduce"}],"readingTime":2.6,"hasTruncateMarker":true,"authors":[{"name":"Adrian Png","title":"Senior Cloud Solutions Architect @ Insum","url":"https://github.com/fuzziebrain","imageURL":"https://github.com/fuzziebrain.png","key":"fuzziebrain"}],"frontMatter":{"title":"The Elephant Rider in Action","tags":["mahout","artificial intelligence","computer science","hadoop","machine learning","mahout","mapreduce"],"slug":"id/185","authors":["fuzziebrain"],"comment":false,"categories":["Reviews","Books"],"date":"2011-02-08T07:11:32.000Z"},"prevItem":{"title":"Spring into Spring Roo","permalink":"/content/id/216"},"nextItem":{"title":"Powershell in Action (Second Edition)","permalink":"/content/id/162"}},"content":"I was very excited to get my hands on a copy of [Mahout in Action](http://www.manning.com/owen/). [Artificial Intelligence](http://en.wikipedia.org/wiki/Artificial_intelligence \\"Artificial intelligence\\") and [Machine Learning](http://en.wikipedia.org/wiki/Machine_learning \\"Machine learning\\") are my most favourite subjects in [Computer Science](http://en.wikipedia.org/wiki/Computer_science \\"Computer science\\"). Back during my school days, AI and ML were primarily used in research, and sometimes banks use them for fraud detection. I am glad that it is gradually being adopted in mainstream businesses with the help of easy to use frameworks like [Mahout](http://lucene.apache.org/mahout/ \\"Apache Mahout\\"), backed by commodity distributing platforms like [Hadoop](http://hadoop.apache.org/ \\"Hadoop\\").\\n\x3c!-- truncate --\x3e\\nThe book is broadly organized into three main categories of algorithms currently implemented in Mahout: Recommendations, Clustering and Classification. The [authors](http://en.wikipedia.org/wiki/Author \\"Author\\") take a single strategy with each category. They begin with a general introduction, the algorithms available and basic examples. Later chapters go in depth on each group of algorithms with even more examples and measuring outcomes.\\n\\nIn many tutorial books and articles, we often find examples where data is magically pre-formatted for use and the outcome is pretty much orchestrated. I like the fact that the authors bother to go through the rigor of obtaining and preparing your data that includes cleaning, transformation etc. real-world data. Learning to pre-process your data is crucial for any successful ML application. Data sources are often \\"dirty\\". For ML algorithms to be useful, we must learn to deal with the imperfect world of [data collection](http://en.wikipedia.org/wiki/Data_collection \\"Data collection\\").\\n\\nMany of the examples presented in the book are also familiar with most people, e.g. classifying news articles, recommending products, clustering Twitter users. I find real-world examples and common knowledge a great way to learn a new subject. Some of these examples might even be useful and can be applied to existing projects. Bear in mind that many [data sets](http://en.wikipedia.org/wiki/Data_set \\"Data set\\") that they point you to are actual data collected from familiar websites and applications.\\n\\nThe authors also examine and provide examples on using Hadoop to support your Mahout implementation. As pointed out in the text, using ML techniques over large datasets often puts a strain on your computing resources. Mahout was designed from ground up to use Hadoop for [High Performance Computing](http://en.wikipedia.org/wiki/High-performance_computing \\"High-performance computing\\"). You could buy a focused book to read about Hadoop (e.g. Hadoop in Action), but I find the coverage is sufficient enough to get you started. In the examples, again you will learn to prepare your data, not only for the analytics, but formulating a [data structure](http://en.wikipedia.org/wiki/Data_structure \\"Data structure\\") that is compatible with Hadoop.\\n\\nLearning about Machine Learning can be very daunting. It sometimes is dry and confusing for me, especially when books focus a lot on the Mathematics involved. I have found the authors approach to this field of Computer Science well balanced. There is enough details to get you started, but a whole lot more hand-holding, through examples, to demonstrate how ML can be useful in your projects.\\n\\nIn summary, this another great book to add to my fairly large collection of Machine Learning textbooks. Here\'s why:\\n*   Authors are major contributors\\n*   Examples examples examples\\n*   Practical examples\\n*   Two books for the price of one"},{"id":"id/162","metadata":{"permalink":"/content/id/162","source":"@site/posts/powershell-in-action-second-edition/index.md","title":"Powershell in Action (Second Edition)","description":"There are many compelling reasons to get a copy of Powershell in Action (2nd Edition) but I will cover the important few. This is a great book currently in the MEAP process at Manning, and I believe, is pretty much near completion.","date":"2010-12-21T12:09:04.000Z","formattedDate":"December 21, 2010","tags":[{"label":"microsoft","permalink":"/content/tags/microsoft"},{"label":"system administration","permalink":"/content/tags/system-administration"},{"label":"powershell","permalink":"/content/tags/powershell"}],"readingTime":2.8,"hasTruncateMarker":true,"authors":[{"name":"Adrian Png","title":"Senior Cloud Solutions Architect @ Insum","url":"https://github.com/fuzziebrain","imageURL":"https://github.com/fuzziebrain.png","key":"fuzziebrain"}],"frontMatter":{"title":"Powershell in Action (Second Edition)","slug":"id/162","authors":["fuzziebrain"],"comment":false,"categories":["Reviews","Books"],"date":"2010-12-21T12:09:04.000Z","tags":["microsoft","system administration","powershell"]},"prevItem":{"title":"The Elephant Rider in Action","permalink":"/content/id/185"},"nextItem":{"title":"Exclude Rules for Eclipse+SVN","permalink":"/content/id/165"}},"content":"There are many compelling reasons to get a copy of [Powershell in Action (2nd Edition)](http://www.manning.com/payette2/) but I will cover the important few. This is a great book currently in the [MEAP](http://en.wikipedia.org/wiki/Michigan_Educational_Assessment_Program \\"Michigan Educational Assessment Program\\") process at Manning, and I believe, is pretty much near completion.\\n\x3c!-- truncate --\x3e\\nBefore I talk about the book, a brief technical background about my interest in the subject matter. I am currently with an IT group focusing mostly on software development and [High Performance computing](http://en.wikipedia.org/wiki/High-performance_computing \\"High-performance computing\\"). We are starting to deploy quite a few [Windows 2008 R2](http://www.microsoft.com/windowsserver2008/en/us/R2.aspx \\"Windows Server 2008 R2\\") servers and gradually rolling out [Windows](http://www.microsoft.com/WINDOWS \\"Windows\\") 7\\\\. We manage desktops outside of [Microsoft](http://maps.google.com/maps?ll=47.6395972222,-122.12845&spn=1.0,1.0&amp;q=47.6395972222,-122.12845) suite of system administration capabilities and rely mostly on [Novell](http://www.novell.com \\"Novell\\") products. So I quite often get requests from my colleagues to write scripts to help configure desktop systems and remotely deploy applications.\\n\\nSo, the first reason to buy this book. It is written by Bruce Payette, a key person in the Powershell Team. Bruce is a great writer. The text is fluid, detailed and at times, humourous. You will find many useful quotes and codes! Well, the latter is more useful. I especially like the breakdowns telling you what each segment of a command does. You\'ll see a lot of this in DB2 and [Oracle](http://oracle.com \\"Oracle Corporation\\") documentation that describe intricate [SQL](http://en.wikipedia.org/wiki/SQL) statements.\\n\\nOn to the second reason. Like many Manning ebooks that have successive editions, you get free access to the older versions. You might wonder how useful that is, but consider this: [Windows Server 2008](http://www.microsoft.com/windowsserver2008/en/us/default.aspx) (the first release) and [Vista](http://www.microsoft.com/windows/windows-vista/default.aspx) still use Powershell v1\\\\. Hence if you are managing a heterogeneous environment containing both Windows 2008 and Windows 2008 R2, you might still want to keep the 1st edition for reference... just in case.\\n\\nNow the meat... the book is divided into two parts. The first part covers most of the fundamentals, similar to how most other introductory books are structured. The first seven chapters are probably sufficient for you to write your first useful script (after [Helloworld](http://en.wikipedia.org/wiki/Hello_world_program \\"Hello world program\\")). If you have more demanding needs, Bruce looks at how you can distribute code and perform magical tasks remotely. Chapters 14 and 15 provides you the necessary skills to not wanna tear out your hair when your script misbehaves.\\n\\nAs mentioned earlier, the author is very careful to describe textually, graphically and by example how things work in [PowerShell](http://www.microsoft.com/powershell \\"Windows PowerShell\\"). The amount of detail can sometimes make it rather dry to read, and that\'s why I particularly appreciate the occasional interjection of humour. Details are good.\\n\\nPart 2 will probably be more useful to most, compared to earlier advanced topics discussed in Chapters 8 to 13... well, at least for me. Oh, and please read Chapter 21 on Security before you question and complain about why your scripts ain\'t running.\\n\\nLast but not least, the Appendices. I have yet to receive them, but certainly looking forward to the much needed code samples (Appendix B) and quick reference sheets (Appendix C).\\n\\nThis review should come timely. During this holiday season, it is almost customary for Manning to be handing out free ebooks or at the very least, great discounts. This book deserves a 5/5 rating, so if you are keen on making Windows work harder for you, then this is a MUST read!"},{"id":"id/165","metadata":{"permalink":"/content/id/165","source":"@site/posts/exclude-rules-for-eclipsesvn/index.md","title":"Exclude Rules for Eclipse+SVN","description":"More like a \\"note to self\\", but I\'m sure others might find useful. Here\'s a list of excludes to include that will prevent Eclipse artifacts from synchronizing on your Subversion server.","date":"2010-12-13T16:22:39.000Z","formattedDate":"December 13, 2010","tags":[{"label":"Eclipse","permalink":"/content/tags/eclipse"},{"label":"plugin","permalink":"/content/tags/plugin"},{"label":"solutions","permalink":"/content/tags/solutions"},{"label":"spring framework","permalink":"/content/tags/spring-framework"},{"label":"springsource","permalink":"/content/tags/springsource"},{"label":"springsource tool suite","permalink":"/content/tags/springsource-tool-suite"},{"label":"sts","permalink":"/content/tags/sts"},{"label":"subclipse","permalink":"/content/tags/subclipse"},{"label":"Subversion","permalink":"/content/tags/subversion"},{"label":"tips","permalink":"/content/tags/tips"}],"readingTime":0.255,"hasTruncateMarker":false,"authors":[{"name":"Adrian Png","title":"Senior Cloud Solutions Architect @ Insum","url":"https://github.com/fuzziebrain","imageURL":"https://github.com/fuzziebrain.png","key":"fuzziebrain"}],"frontMatter":{"title":"Exclude Rules for Eclipse+SVN","tags":["Eclipse","plugin","solutions","spring framework","springsource","springsource tool suite","sts","subclipse","Subversion","tips"],"slug":"id/165","authors":["fuzziebrain"],"comment":false,"categories":["Technology","Programming","Tools"],"date":"2010-12-13T16:22:39.000Z"},"prevItem":{"title":"Powershell in Action (Second Edition)","permalink":"/content/id/162"},"nextItem":{"title":"Location Aware Applications (Part 2)","permalink":"/content/id/108"}},"content":"More like a \\"note to self\\", but I\'m sure others might find useful. Here\'s a list of excludes to include that will prevent Eclipse artifacts from synchronizing on your [Subversion](http://subversion.apache.org/ \\"Apache Subversion\\") server.\\n*   .project\\n*   .classpath\\n*   .settings (added on 17-Jan-11)\\n*   .springBeans (added on 14-Dec-10)\\n*   target (and/or bin)"},{"id":"id/108","metadata":{"permalink":"/content/id/108","source":"@site/posts/building-location-aware-applications-part-2/index.md","title":"Location Aware Applications (Part 2)","description":"This post follows my earlier review of the first six chapters of \\"Location Aware Applications\\". I recently received an update containing two new chapters covering topics on connectivity issues and how to monetize your LBS applications.","date":"2010-10-01T11:53:43.000Z","formattedDate":"October 1, 2010","tags":[{"label":"geolocation","permalink":"/content/tags/geolocation"},{"label":"gps","permalink":"/content/tags/gps"},{"label":"iphone","permalink":"/content/tags/iphone"},{"label":"java","permalink":"/content/tags/java"},{"label":"lbs","permalink":"/content/tags/lbs"},{"label":"location-based services","permalink":"/content/tags/location-based-services"},{"label":"monetization","permalink":"/content/tags/monetization"}],"readingTime":1.085,"hasTruncateMarker":true,"authors":[{"name":"Adrian Png","title":"Senior Cloud Solutions Architect @ Insum","url":"https://github.com/fuzziebrain","imageURL":"https://github.com/fuzziebrain.png","key":"fuzziebrain"}],"frontMatter":{"title":"Location Aware Applications (Part 2)","tags":["geolocation","gps","iphone","java","lbs","location-based services","monetization"],"slug":"id/108","authors":["fuzziebrain"],"comment":false,"categories":["Reviews","Books"],"date":"2010-10-01T11:53:43.000Z"},"prevItem":{"title":"Exclude Rules for Eclipse+SVN","permalink":"/content/id/165"},"nextItem":{"title":"Location Aware Applications (Part 1)","permalink":"/content/id/88"}},"content":"This post follows my [earlier](/content/id/88) review of the first six chapters of \\"Location Aware Applications\\". I recently received an update containing two new chapters covering topics on connectivity issues and how to monetize your LBS applications.\\n\x3c!-- truncate --\x3e\\nChapter 7 briefly looks at issues affecting mobile devices that affect connectivity, e.g. acquisition rate of the [GPS](http://en.wikipedia.org/wiki/Global_Positioning_System \\"Global Positioning System\\") and battery consumption. A few [code snippets](http://en.wikipedia.org/wiki/Snippet_%28programming%29 \\"Snippet (programming)\\") in this chapter also demonstrates to the reader how you can obtain [geolocation](http://en.wikipedia.org/wiki/Geolocation \\"Geolocation\\") data from the different devices: [iPhones](http://www.apple.com/iphone \\"iPhone\\"), Androids, [Java ME](http://en.wikipedia.org/wiki/Java_Platform%2C_Micro_Edition \\"Java Platform, Micro Edition\\") and [Palm WebOS](http://developer.palm.com/ \\"WebOS\\") devices.\\n\\nI like Chapter 8 the most so far. Well, who wouldn\'t? Software [metrics](http://en.wikipedia.org/wiki/Software_metric \\"Software metric\\") has always been a challenging art - yes ART! So a  chapter that provides pointers on how to commercialize your application  is certainly welcomed!\\n\\nThe authors describe several ways of turning your application usage into cash churning machine. Several options on how to charge consumers for the use of your software are presented and includes many considerations - the pros and cons - for each method. I especially like Figure 8.2, a matrix for helping you decide which charging model best fits the type of application you have. Learned a few new keywords like \\"Long Tail Kings\\" and \\"Boot Campers\\"."},{"id":"id/88","metadata":{"permalink":"/content/id/88","source":"@site/posts/building-location-aware-applications/index.md","title":"Location Aware Applications (Part 1)","description":"The Manning Early Access Program, or MEAP, is a program that allows you to purchase an electronic copy of a book even before it is published. This allows readers to participate in the development of the book\'s content. You will of course receive the final version of the book when it goes gold. I recently got my hands on a copy of the MEAP book, Location Aware Applications. At the time of this post, only 50% of the book has been completed. So here are my early impression of this upcoming title.","date":"2010-09-20T22:50:40.000Z","formattedDate":"September 20, 2010","tags":[{"label":"api","permalink":"/content/tags/api"},{"label":"gis","permalink":"/content/tags/gis"},{"label":"gps","permalink":"/content/tags/gps"},{"label":"lbs","permalink":"/content/tags/lbs"},{"label":"linked data","permalink":"/content/tags/linked-data"},{"label":"semantic web","permalink":"/content/tags/semantic-web"}],"readingTime":2.08,"hasTruncateMarker":true,"authors":[{"name":"Adrian Png","title":"Senior Cloud Solutions Architect @ Insum","url":"https://github.com/fuzziebrain","imageURL":"https://github.com/fuzziebrain.png","key":"fuzziebrain"}],"frontMatter":{"title":"Location Aware Applications (Part 1)","tags":["api","gis","gps","lbs","linked data","semantic web"],"slug":"id/88","authors":["fuzziebrain"],"comment":false,"categories":["Reviews","Books"],"date":"2010-09-20T22:50:40.000Z"},"prevItem":{"title":"Location Aware Applications (Part 2)","permalink":"/content/id/108"}},"content":"The [Manning Early Access Program](http://manning.com/about/meap.html), or MEAP, is a program that allows you to purchase an electronic copy of a book even before it is published. This allows readers to participate in the development of the book\'s content. You will of course receive the final version of the book when it goes gold. I recently got my hands on a copy of the MEAP book, Location Aware Applications. At the time of this post, only 50% of the book has been completed. So here are my early impression of this upcoming title.\\n\x3c!-- truncate --\x3e\\n[GPS](http://en.wikipedia.org/wiki/Global_Positioning_System \\"Global Positioning System\\") and many other devices are becoming ubiquitous features in [cellular phones](http://en.wikipedia.org/wiki/Mobile_phone \\"Mobile phone\\"). The availability of maps and [APIs](http://en.wikipedia.org/wiki/Application_programming_interface \\"Application programming interface\\") to access geospatial [data](http://en.wikipedia.org/wiki/Data \\"Data\\") are plenty. There is also increasing effort in making data open and linked, providing a multitude of information that can be consumed, meshed and presented to the mobile user. Thus it is no surprise that a lot of [software](http://en.wikipedia.org/wiki/Computer_software \\"Computer software\\") developers (or not) are looking into making use of location-based information, hoping to spin some big bucks out of [technology](http://en.wikipedia.org/wiki/Technology \\"Technology\\").\\n\\nIf you are one of them, then this book might be for you. The authors provide a very good introduction for someone looking for headway into developing location-based, [location-aware](http://en.wikipedia.org/wiki/Location_awareness \\"Location awareness\\") applications. Part 1 is a comprehensive overview of the history and development of LBS technologies and is very informative for newcomers. The beginning four chapters will also be a good refresher for experienced [GIS](http://en.wikipedia.org/wiki/Geographic_information_system \\"Geographic information system\\") developers or geeks, so you might want to quickly glance through so that you are kept at the forefront of development. In chapters 3 and 4, the authors survey a broad range of content and mapping APIs with sample codes to showcase how they can be used. And this is a first I have seen: at the end of each code sample, the authors provide somewhat like a pseudocode to describe the program [flow](http://en.wikipedia.org/wiki/Control_flow \\"Control flow\\"). This is very effective and helpful in understanding how the code works.\\n\\nI do hope though that the authors might consider adding some introduction to [LinkedData](http://en.wikipedia.org/wiki/Linked_Data \\"Linked Data\\") and the [Semantic Web](http://semanticweb.org \\"Semantic Web\\"), and how these novel developments might help spur more LBS applications.\\n\\nThat\'s it for now. I will update this post as more chapters are completed in the coming weeks/months. Stay tuned, or if you can\'t wait, grab a MEAP copy now! This book is available either from [Manning](http://manning.com/ferraro) or [Amazon](http://www.amazon.com/gp/product/1935182331?ie=UTF8&tag=fuzziebrain-20&linkCode=as2&camp=1789&creative=390957&creativeASIN=1935182331)."}]}')}}]);