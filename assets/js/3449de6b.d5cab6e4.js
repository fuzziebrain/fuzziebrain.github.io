"use strict";(self.webpackChunknewsite=self.webpackChunknewsite||[]).push([[76058],{91513:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>h,frontMatter:()=>s,metadata:()=>r,toc:()=>c});var a=t(85893),i=t(11151);const s={title:"Generate Art Using Latent Diffusion Models and NFC Tags",description:"Learn how to host the Stable  for performing text-to-image generation tasks on the Oracle Cloud Infrastructure (OCI). Then, build an Oracle APEX application that uses NFC tags to receive text prompts and use it to generate an image.",image:"./assets/image20231119221347.png",tags:["latent diffusion model","ldm","machine learning","oci","python","conda","oracle apex","generative ai","ai-generated art","mlops","nfc","text-to-image generation"],categories:["Technology"],authors:["fuzziebrain"],date:new Date("2023-11-19T21:00:00.000Z")},o=void 0,r={permalink:"/content/generate-art-using-diffusion-models-and-nfc-tags",source:"@site/posts/generate-art-using-diffusion-models-and-nfc-tags/index.md",title:"Generate Art Using Latent Diffusion Models and NFC Tags",description:"Learn how to host the Stable  for performing text-to-image generation tasks on the Oracle Cloud Infrastructure (OCI). Then, build an Oracle APEX application that uses NFC tags to receive text prompts and use it to generate an image.",date:"2023-11-19T21:00:00.000Z",formattedDate:"November 19, 2023",tags:[{label:"latent diffusion model",permalink:"/content/tags/latent-diffusion-model"},{label:"ldm",permalink:"/content/tags/ldm"},{label:"machine learning",permalink:"/content/tags/machine-learning"},{label:"oci",permalink:"/content/tags/oci"},{label:"python",permalink:"/content/tags/python"},{label:"conda",permalink:"/content/tags/conda"},{label:"oracle apex",permalink:"/content/tags/oracle-apex"},{label:"generative ai",permalink:"/content/tags/generative-ai"},{label:"ai-generated art",permalink:"/content/tags/ai-generated-art"},{label:"mlops",permalink:"/content/tags/mlops"},{label:"nfc",permalink:"/content/tags/nfc"},{label:"text-to-image generation",permalink:"/content/tags/text-to-image-generation"}],readingTime:10.62,hasTruncateMarker:!0,authors:[{name:"Adrian Png",title:"Senior Cloud Solutions Architect @ Insum",url:"https://github.com/fuzziebrain",imageURL:"https://github.com/fuzziebrain.png",key:"fuzziebrain"}],frontMatter:{title:"Generate Art Using Latent Diffusion Models and NFC Tags",description:"Learn how to host the Stable  for performing text-to-image generation tasks on the Oracle Cloud Infrastructure (OCI). Then, build an Oracle APEX application that uses NFC tags to receive text prompts and use it to generate an image.",image:"./assets/image20231119221347.png",tags:["latent diffusion model","ldm","machine learning","oci","python","conda","oracle apex","generative ai","ai-generated art","mlops","nfc","text-to-image generation"],categories:["Technology"],authors:["fuzziebrain"],date:"2023-11-19T21:00:00.000Z"},unlisted:!1,prevItem:{title:"A Yearful of AI But What's Next?",permalink:"/content/a-yearful-of-ai-but-whats-next"},nextItem:{title:"Deploy and Use Fine-Tuned LLMs in Oracle APEX",permalink:"/content/deploy-and-use-fine-tuned-llms-in-oracle-apex"}},l={image:t(39448).Z,authorsImageUrls:[void 0]},c=[{value:"Deploying the Model",id:"deploying-the-model",level:2},{value:"Setup the Server Environment",id:"setup-the-server-environment",level:3},{value:"Write the Server-Side Code",id:"write-the-server-side-code",level:3},{value:"Run the Server",id:"run-the-server",level:3},{value:"Deploy an OCI API Gateway",id:"deploy-an-oci-api-gateway",level:3},{value:"Testing the Model",id:"testing-the-model",level:3},{value:"The NFC-Enabled Web Application",id:"the-nfc-enabled-web-application",level:2},{value:"Writing to NFC Tags",id:"writing-to-nfc-tags",level:2},{value:"Summary",id:"summary",level:2}];function d(e){const n={a:"a",blockquote:"blockquote",code:"code",em:"em",h2:"h2",h3:"h3",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.a)(),...e.components},{Youtube:s}=n;return s||function(e,n){throw new Error("Expected "+(n?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}("Youtube",!0),(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.p,{children:(0,a.jsx)(n.img,{alt:"Photo of a bird perched on a tree, generated using a Stability Diffusion model.",src:t(57699).Z+"",width:"1280",height:"720"})}),"\n",(0,a.jsxs)(n.p,{children:["In my previous ",(0,a.jsx)(n.a,{href:"/content/deploy-and-use-fine-tuned-llms-in-oracle-apex",children:"post"}),", you read about how I performed fine-tuning and deployed a large language model (LLM) on the ",(0,a.jsx)(n.a,{href:"https://oracle.com/cloud",children:"Oracle Cloud Infrastructure"})," (OCI) ",(0,a.jsx)(n.a,{href:"https://www.oracle.com/artificial-intelligence/data-science/",children:"Data Science"})," service. Here, I was hoping to do the same with the ",(0,a.jsx)(n.a,{href:"https://stability.ai/stable-diffusion",children:"Stable Diffusion XL"}),", a ",(0,a.jsx)(n.em,{children:"Latent Diffusion Model"})," (LDM), but unfortunately, the platform currently does not support a ",(0,a.jsx)(n.a,{href:"https://huggingface.co",children:(0,a.jsx)(n.em,{children:"Hugging Face"})})," pipeline using this model. I found an alternative, and you will read about it later in this article."]}),"\n",(0,a.jsxs)(n.p,{children:["To demonstrate its utility, I create a simple ",(0,a.jsx)(n.a,{href:"https://apex.oracle.com",children:"Oracle APEX"})," application that reads NFC tags using the ",(0,a.jsx)(n.strong,{children:"experimental"})," ",(0,a.jsx)(n.a,{href:"https://w3c.github.io/web-nfc/",children:(0,a.jsx)(n.em,{children:"Web NFC"})})," API, and then displays an image generated by the deployed machine learning (ML) model."]}),"\n",(0,a.jsxs)(n.p,{children:['I called this toy application "Amiibo Art". The ',(0,a.jsx)(n.a,{href:"https://www.nintendo.com/amiibo/",children:"Amiibo"})," was created by ",(0,a.jsx)(n.em,{children:"Nintendo"})," to bridge games with the real world using toys, figurines, and collectibles that embedded a radio-frequency tag. I wanted this APEX application to interact with the real world, and then generate a collectible. If you missed it, here's the screen recording of the application in action:"]}),"\n",(0,a.jsx)(s,{videoId:"aozXI0NWlKs"}),"\n",(0,a.jsx)(n.h2,{id:"deploying-the-model",children:"Deploying the Model"}),"\n",(0,a.jsxs)(n.p,{children:["As mentioned earlier, the OCI Data Science platform did not provide me an easy way to deploy the Stable Diffusion XL model. I found an open-source project called ",(0,a.jsx)(n.a,{href:"https://pinferencia.underneathall.app/",children:(0,a.jsx)(n.em,{children:"Pinferenia"})})," that I could use to deploy a Hugging Face pipeline to perform the text-to-image generation task. The code below can be executed and deployed on an OCI Compute, and a GPU, while not required, is desired."]}),"\n",(0,a.jsx)(n.h3,{id:"setup-the-server-environment",children:"Setup the Server Environment"}),"\n",(0,a.jsxs)(n.p,{children:["The first task involved setting up a new ",(0,a.jsx)(n.a,{href:"https://conda.io",children:"Conda"})," environment with the required dependencies using the ",(0,a.jsx)(n.a,{href:"https://yaml.org/",children:"YAML"})," file below:"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-yaml",children:"channels:\n    - conda-forge\n    - pytorch\n    - nvidia\n\ndependencies:\n    - python=3.9\n    - pytorch\n    - pytorch-cuda\n    - transformers\n    - pillow\n    - numpy\n    - safetensors\n    - diffusers\n    - accelerate\n    - pip\n    - pip:\n        - invisible_watermark\n        - oracle-ads\n        - pinferencia\n"})}),"\n",(0,a.jsxs)(n.p,{children:["If you are new to Conda, write the YAML contents into a file name ",(0,a.jsx)(n.code,{children:"amiibo-art.yaml"}),", install the Conda software, and then execute the command:"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"conda env create -n amiibo-art -f amiibo-art.yaml\n"})}),"\n",(0,a.jsx)(n.p,{children:"Then, as instructed, activate the environment before running the subsequent code and commands. To activate the newly created environment, execute the command:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"conda activate amiibo-art\n"})}),"\n",(0,a.jsx)(n.h3,{id:"write-the-server-side-code",children:"Write the Server-Side Code"}),"\n",(0,a.jsxs)(n.p,{children:["Create the Python file ",(0,a.jsx)(n.code,{children:"app.py"})," with the code provided below. Follow the embedded comments to understand what it is doing."]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'from diffusers import DiffusionPipeline\nfrom pinferencia import Server\nimport torch\nimport datetime\nimport os\nimport oci.object_storage\n\ndef get_current_timestamp():\n    """\n    Return a timestamp for file naming purposes.\n    """\n    now = datetime.datetime.now()\n    return now.strftime(\'%Y%m%d%H%M%S\')\n\ndef save_object(bucketName, objectName, content):\n    """\n    Save the generated image in an Object Storage bucket for retrieval later.\n    """\n    mimeType = "image/png"\n\n    object = client.put_object(\n        namespace_name = namespace\n        , bucket_name = bucketName\n        , object_name = objectName\n        , content_type = mimeType\n        , put_object_body = content\n    )\n\ndef generate(prompt):\n    """\n    This function is registered to the Pinferencia as the model "genai". It\n    generates the image using the Hugging Face pipeline, save the output as a\n    file, then push it to the Object Storage bucket using the function\n    "save_object".\n    """\n    bucketName = "amiibo"\n    objectName = f"image{get_current_timestamp()}.png"\n    image = pipe(prompt=prompt, width=1280, height=720).images[0]\n    image.save(objectName)\n    content = open(objectName, "rb")\n    save_object(bucketName=bucketName, objectName=objectName, content=content)\n    os.remove(objectName)\n    return {"bucket_name": bucketName, "object_name": objectName}\n\n"""\nWe will use an instance principal to sign the request. Be sure to:\n1. Create a dynamic group that includes the Compute instance\'s OCID.\n2. Create an IAM policy that has the necessary statements to allow the dynamic\n   group to write to the bucket.\n"""\nsigner = oci.auth.signers.InstancePrincipalsSecurityTokenSigner()\nclient = oci.object_storage.ObjectStorageClient(config={"region":"***",\n    "tenancy":signer.tenancy_id}, signer=signer)\nnamespace = client.get_namespace().data\n\n"""\nCreate a Hugging Face pipeline using the Stable Diffusion XL model hosted on the\nplatform. We will use the quantized model.\n"""\npipe = DiffusionPipeline.from_pretrained("stabilityai/stable-diffusion-xl-base-1.0",\n        use_safetensors=True, torch_dtype=torch.float16, variant="fp16")\n\n"""\nUse a Nvidia GPU, if available.\n"""\npipe.to("cuda" if torch.cuda.is_available() else "cpu")\n\n"""\nInstantiate the Pinferencia server.\n"""\nservice = Server()\n\n"""\nRegister the "generate" function as the model "genai".\n"""\nservice.register(model_name="genai", model=generate,\n    metadata={"platform":"Linux", "device": "GPU"})\n'})}),"\n",(0,a.jsx)(n.h3,{id:"run-the-server",children:"Run the Server"}),"\n",(0,a.jsxs)(n.p,{children:["The Pinferencia package includes executables for running a server instance. For simplicity, I ran the server in the background using ",(0,a.jsx)(n.a,{href:"https://github.com/tmux/tmux",children:"tmux"})," (think ",(0,a.jsx)(n.em,{children:"screen"}),"), and executing the command:"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"pinfer --backend-host 0.0.0.0 --mode backend app:service --reload\n"})}),"\n",(0,a.jsxs)(n.blockquote,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"NOTE"})}),"\n",(0,a.jsxs)(n.p,{children:["By default, the server only binds and listens to the local interface, i.e., ",(0,a.jsx)(n.code,{children:"localhost"})," or IP address ",(0,a.jsx)(n.code,{children:"127.0.0.1"}),". Hence, the command includes the parameter ",(0,a.jsx)(n.code,{children:"--backend-host 0.0.0.0"}),"."]}),"\n"]}),"\n",(0,a.jsx)(n.p,{children:"The deployed model is accessible at an endpoint URL with a format that looks like this:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:"http://{{HOST_IP}}:8000/v1/models/{{MODEL_NAME}}/predict\n"})}),"\n",(0,a.jsx)(n.p,{children:"For example:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:"http://10.30.4.23:8000/v1/models/genai/predict\n"})}),"\n",(0,a.jsx)(n.h3,{id:"deploy-an-oci-api-gateway",children:"Deploy an OCI API Gateway"}),"\n",(0,a.jsxs)(n.p,{children:["As I am calling this web service from an Oracle Autonomous Database that is ",(0,a.jsx)(n.strong,{children:"not"})," using a private endpoint, I needed to proxy the service over HTTPS. The fastest approach to achieve this was to create an OCI API Gateway and route as show below."]}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.img,{alt:"OCI API Gateway route for the Pinferencia web service endpoint.",src:t(51918).Z+"",width:"2560",height:"1440"})}),"\n",(0,a.jsx)(n.p,{children:"This serves the web service publicly using an endpoint address that looks like this:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:"https://{{UNIQUE_IDENTIFIER}}.apigateway.eu-frankfurt-1.oci.customer-oci.com/v1/genai/texttoimage\n"})}),"\n",(0,a.jsx)(n.h3,{id:"testing-the-model",children:"Testing the Model"}),"\n",(0,a.jsxs)(n.p,{children:["Pinferencia provides a ",(0,a.jsx)(n.a,{href:"https://swagger.io/",children:"Swagger"})," UI that documents all the endpoints it is serving. You can use it to try out the models served. In this diagram, I am testing the image generation model."]}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.img,{alt:"Swagger UI showing all the available endpoints and a way to try them.",src:t(95604).Z+"",width:"2560",height:"1440"})}),"\n",(0,a.jsx)(n.p,{children:"The input is a JSON object containing the prompt:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-json",children:'{\n    "id": "string",\n    "parameters": {},\n    "data": "A cartoon image of a cat chasing a mouse."\n}\n'})}),"\n",(0,a.jsxs)(n.p,{children:["After the image has been placed in the bucket, it returns a HTTP response containing a JSON payload with the ",(0,a.jsx)(n.code,{children:"bucket_name"})," and ",(0,a.jsx)(n.code,{children:"object_name"})," that we can use to retrieve the object."]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-json",children:'{\n    "id": "string",\n    "model_name": "genai",\n    "model_version": "default",\n    "data": {\n        "bucket_name": "amiibo",\n        "object_name": "image20231113223047.png"\n    }\n}\n'})}),"\n",(0,a.jsx)(n.h2,{id:"the-nfc-enabled-web-application",children:"The NFC-Enabled Web Application"}),"\n",(0,a.jsx)(n.p,{children:"I created an Oracle APEX application with the following features:"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Page 1."})," On the home page, the user scans a NFC tag containing a single text record that stores the prompt to submit to the deployed model."]}),"\n",(0,a.jsxs)(n.li,{children:["An administrative tool that allows an administrator to:","\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Page 2"}),". List tags in the inventory with buttons to manage or write prompts to a tag."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Page 3"}),". Register and manage a tag."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Page 4"}),". Write prompts to a tag."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(n.p,{children:"To keep this blog post readable, I will focus on two key aspects that allow the user to scan and generate an image."}),"\n",(0,a.jsxs)(n.p,{children:["First, the ",(0,a.jsx)(n.em,{children:"Ajax Callback"})," process that is called when an NFC tag containing a valid prompt is scanned successfully. Again, follow the embedded comments to understand what this code does."]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-sql",children:"declare\n    /**\n     * The collection name used for temporary storing the images.\n     */\n    c_collection_name apex_collections.collection_name%type := 'AMIIBO_ART';\n\n    /**\n     * The JavaScript event listener will read the tag and supply prompt using\n     * the x01 variable.\n     */\n    l_prompt apex_application.g_x01%type := apex_application.g_x01;\n\n    /**\n     * Utility variables for reading and parsing the results.\n     */\n    l_response clob;\n    l_content blob;\n    l_json_response apex_json.t_values;\nbegin\n    /**\n     * Call the deployed model's endpoint through the API gateway. The payload\n     * contains the prompt read from the NFC tag.\n     */\n    l_response := apex_web_service.make_rest_request(\n        p_url => 'https://{{UNIQUE_IDENTIFIER}}.apigateway.eu-frankfurt-1.oci.customer-oci.com/v1/genai/texttoimage'\n        , p_http_method => 'POST'\n        , p_body => json_object(\n            key 'data' value l_prompt\n        )\n    );\n\n    if apex_web_service.g_status_code = 200 then\n        /**\n         * If all goes well, parse the HTTP response's JSON payload to read the\n         * generated image's bucket and object name.\n         */\n        apex_json.parse(l_json_response, l_response);\n\n        /**\n         * Retrieve the object using the DBMS_CLOUD.GET_OBJECT function. Be sure\n         * to also create an IAM user, set up the OCI API credentials, and\n         * create the necessary policy to allow the IAM user to access the\n         * object.\n         */\n        l_content := dbms_cloud.get_object(\n            credential_name => 'MY_API_CREDENTIALS'\n            , object_uri => 'https://objectstorage.eu-frankfurt-1.oraclecloud.com/n/***/b/amiibo/o/'\n                ||  apex_json.get_varchar2(\n                        p_path => 'data.object_name'\n                        , p_values => l_json_response\n                    )\n        );\n\n        /**\n         * Not the best code here, but I am simply pushing the image into the\n         * an APEX collection.\n         */\n        apex_collection.create_or_truncate_collection(c_collection_name);\n        apex_collection.add_member(\n            p_collection_name => c_collection_name\n            , p_c001 => l_prompt\n            , p_blob001 => l_content\n        );\n\n        sys.htp.p('{\"message\":\"success\", \"prompt\": \"'||l_prompt||'\"}');\n    else\n        sys.htp.p('{\"message\":\"failed\"}');\n    end if;\nend;\n"})}),"\n",(0,a.jsx)(n.p,{children:"This can be a long running process, and better strategies should be considered if larger images, the non-quantized model is used, or the pipeline is not running on a server with a GPU with sufficient memory. With a single Nvidia A10, I was able to get images to be generated in less than 20 seconds. While possible, using a CPU took more than 12 minutes in one attempt I made."}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.img,{alt:"Elapsed time using only CPU resources for the pipeline.",src:t(85352).Z+"",width:"2560",height:"1440"})}),"\n",(0,a.jsxs)(n.p,{children:["On page 1, an Oracle APEX ",(0,a.jsx)(n.em,{children:"Card Region"})," is added to render the image using the source query:"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-sql",children:"select\n    seq_id\n    , 'My Amiibo Art' as title\n    , c001 as prompt\n    , blob001 as content_blob\nfrom apex_collections\nwhere collection_name = 'AMIIBO_ART'\n"})}),"\n",(0,a.jsx)(n.p,{children:"And a button that calls a dynamic action that executes the following JavaScript code:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-javascript",children:'apex.message.alert(\n    "Close this alert, and then scan your Amiibo.",\n    null,\n    { style: "information" }\n);\n\ntry {\n    /**\n     * Instantiate the NFC reader object when the scan button is clicked.\n     */\n    const ndef = new NDEFReader();\n\n    /**\n     * Begin scanning.\n     */\n    ndef.scan();\n\n    /**\n     * Attach an event handler when there is an error reading the tag.\n     */\n    ndef.addEventListener(\n        "readingerror",\n        () => {\n            apex.message.alert(\n                "Failed to read data from the Amiibo. Please try again.",\n                null,\n                { style: "warning" }\n            );\n        }\n    );\n\n    /**\n     * Attach an event handler to handle the incoming data when a tag is scanned\n     * successfully. The data contains the serial number and its message\n     * payload.\n     */\n    ndef.addEventListener(\n        "reading",\n        ({ message, serialNumber }) => {\n            if(message.records.length > 0) {\n                /**\n                 * Get the first record.\n                 */\n                record = message.records[0];\n\n                /**\n                 * If it is of the expected record type, then process it.\n                 */\n                if(record.recordType === "text") {\n                    /**\n                     * First, instantiate a text decoder.\n                     */\n                    const decoder = new TextDecoder();\n\n                    let spinner = apex.util.showSpinner($("body"));\n\n                    /**\n                     * Call the Ajax callback with the text contained in the\n                     * tag\'s message payload.\n                     */\n                    apex.server.process(\n                        "TEXT_TO_IMAGE",\n                        {\n                            x01: decoder.decode(record.data)\n                        },\n                        {\n                            success: function( data )  {\n                                /**\n                                 * TODO:\n                                 * A page submit here because the Card region\n                                 * was not refreshing with the new image. Needs\n                                 * improvement.\n                                 */\n                                apex.page.submit();\n                            },\n                            error: function( jqXHR, textStatus, errorThrown ) {\n                                // handle error\n                            }\n                        }\n                    );\n                }\n            }\n        }\n    );\n} catch (error) {\n    apex.message.alert(`General error: {error}`, null, { style: "warning" });\n}\n'})}),"\n",(0,a.jsx)(n.p,{children:"After the tag is scan, on the backend, you can see Pinferencia hard at work, generating images typically under 20 seconds."}),"\n",(0,a.jsx)(s,{videoId:"uheDoKlAFdU"}),"\n",(0,a.jsx)(n.p,{children:"And if all goes well, the page should be refreshed with the image generated by the model."}),"\n",(0,a.jsx)(n.h2,{id:"writing-to-nfc-tags",children:"Writing to NFC Tags"}),"\n",(0,a.jsx)(n.p,{children:'At the time of writing, the Web NFC API allows developers to read, write, and set a tag to "read-only" mode. In the application, I had created functionality for an administrator to maintain a collection of NFC tags, and to write prompts to them. On page 4, a button is attached to a dynamic action that runs JavaScript code to write to the tag. A sample of the code is as follows:'}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-javascript",children:'try {\n    ...\n\n    ndef.addEventListener(\n        "reading",\n        ({ message, serialNumber }) => {\n            if(serialNumber === apex.item("P4_TAG_ID").getValue()) {\n                ndef.write(apex.item("P4_PROMPT").getValue());\n                apex.message.alert(\n                    "Prompt written to tag.",\n                    null,\n                    {style: "information"}\n                );\n            } else {\n                apex.message.alert(\n                    "Failed to write to tag. Tag ID mismatch.",\n                    null,\n                    {style: "danger"}\n                );\n            }\n        }\n    );\n} catch (error) {\n    apex.message.alert(`General error: {error}`, null, { style: "warning" });\n}\n'})}),"\n",(0,a.jsx)(n.p,{children:"Easy eh?"}),"\n",(0,a.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,a.jsx)(n.p,{children:"I worked on this fun project with two objectives in mind:"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsx)(n.li,{children:"How can I work with NFC tags in an Oracle APEX application."}),"\n",(0,a.jsx)(n.li,{children:"How can I host a Stable Diffusion model in the OCI."}),"\n"]}),"\n",(0,a.jsx)(n.p,{children:"Despite the limitations, I have learned quite a bit during this adventure."}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsx)(n.li,{children:'Support for NFC tags is currently limited. Only Google Chrome on an Android device supports the experimental Web NFC API. And even so, we seem to be limited to reading, writing, and setting a tag to "read-only" mode. Hopefully, there will be future support password-lock a tag as well.'}),"\n",(0,a.jsx)(n.li,{children:"Not all Hugging Face pipelines are supported and can be deployed to the OCI Data Science platform."}),"\n",(0,a.jsx)(n.li,{children:"There are other software platforms that can be used to deploy ML models, for example, Pinferencia. I don't consider it ready for production use, but it has been very easy to set up, run, and host a ML model."}),"\n"]}),"\n",(0,a.jsxs)(n.p,{children:["Hope you enjoyed reading about this as much as I had working and writing on the project. If you are interested to discuss possible solutions to your unique business challenge, please reach out to me using this ",(0,a.jsx)(n.a,{href:"https://cal.com/fuzziebrain/iaas-consult",children:"form"}),"."]}),"\n",(0,a.jsxs)(n.blockquote,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Credits"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["Banner image generated using Stability AI's ",(0,a.jsx)(n.a,{href:"https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0",children:"Stable Diffusion"})," model."]}),"\n"]}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,i.a)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(d,{...e})}):d(e)}},39448:(e,n,t)=>{t.d(n,{Z:()=>a});const a=t.p+"assets/images/image20231119221347-f7f8e843ef24108f6ff6b29e10bd7527.png"},51918:(e,n,t)=>{t.d(n,{Z:()=>a});const a=t.p+"assets/images/apigw-route-29651e67af64b9e698f6b103f0e3cf44.png"},85352:(e,n,t)=>{t.d(n,{Z:()=>a});const a=t.p+"assets/images/elapsed-time-with-cpu-only-eeaf9ccb0ae9facf788d9eb69a4d0afb.png"},57699:(e,n,t)=>{t.d(n,{Z:()=>a});const a=t.p+"assets/images/image20231119221347-f7f8e843ef24108f6ff6b29e10bd7527.png"},95604:(e,n,t)=>{t.d(n,{Z:()=>a});const a=t.p+"assets/images/swagger-ui-68ecb832cc8fc650997e2232db31a795.png"},11151:(e,n,t)=>{t.d(n,{Z:()=>r,a:()=>o});var a=t(67294);const i={},s=a.createContext(i);function o(e){const n=a.useContext(s);return a.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:o(e.components),a.createElement(s.Provider,{value:n},e.children)}}}]);