"use strict";(self.webpackChunknewsite=self.webpackChunknewsite||[]).push([[4774],{3905:(e,n,t)=>{t.r(n),t.d(n,{MDXContext:()=>c,MDXProvider:()=>u,mdx:()=>b,useMDXComponents:()=>p,withMDXComponents:()=>d});var a=t(67294);function o(e,n,t){return n in e?Object.defineProperty(e,n,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[n]=t,e}function r(){return r=Object.assign||function(e){for(var n=1;n<arguments.length;n++){var t=arguments[n];for(var a in t)Object.prototype.hasOwnProperty.call(t,a)&&(e[a]=t[a])}return e},r.apply(this,arguments)}function i(e,n){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);n&&(a=a.filter((function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable}))),t.push.apply(t,a)}return t}function l(e){for(var n=1;n<arguments.length;n++){var t=null!=arguments[n]?arguments[n]:{};n%2?i(Object(t),!0).forEach((function(n){o(e,n,t[n])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):i(Object(t)).forEach((function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(t,n))}))}return e}function s(e,n){if(null==e)return{};var t,a,o=function(e,n){if(null==e)return{};var t,a,o={},r=Object.keys(e);for(a=0;a<r.length;a++)t=r[a],n.indexOf(t)>=0||(o[t]=e[t]);return o}(e,n);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(a=0;a<r.length;a++)t=r[a],n.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(o[t]=e[t])}return o}var c=a.createContext({}),d=function(e){return function(n){var t=p(n.components);return a.createElement(e,r({},n,{components:t}))}},p=function(e){var n=a.useContext(c),t=n;return e&&(t="function"==typeof e?e(n):l(l({},n),e)),t},u=function(e){var n=p(e.components);return a.createElement(c.Provider,{value:n},e.children)},m="mdxType",h={inlineCode:"code",wrapper:function(e){var n=e.children;return a.createElement(a.Fragment,{},n)}},g=a.forwardRef((function(e,n){var t=e.components,o=e.mdxType,r=e.originalType,i=e.parentName,c=s(e,["components","mdxType","originalType","parentName"]),d=p(t),u=o,m=d["".concat(i,".").concat(u)]||d[u]||h[u]||r;return t?a.createElement(m,l(l({ref:n},c),{},{components:t})):a.createElement(m,l({ref:n},c))}));function b(e,n){var t=arguments,o=n&&n.mdxType;if("string"==typeof e||o){var r=t.length,i=new Array(r);i[0]=g;var l={};for(var s in n)hasOwnProperty.call(n,s)&&(l[s]=n[s]);l.originalType=e,l[m]="string"==typeof e?e:o,i[1]=l;for(var c=2;c<r;c++)i[c]=t[c];return a.createElement.apply(null,i)}return a.createElement.apply(null,t)}g.displayName="MDXCreateElement"},11032:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>s,contentTitle:()=>i,default:()=>p,frontMatter:()=>r,metadata:()=>l,toc:()=>c});var a=t(87462),o=(t(67294),t(3905));const r={title:"Revolutionising Language Learning: How AI and Oracle APEX Transform Everyday Challenges",tags:["artificial intelligence","machine learning","speech recognition","language learning","oracle ai","oracle apex","ai applications","cloud solutions","technical workflow","innovative technology"],categories:["Technology"],authors:["fuzziebrain"],date:new Date("2023-08-20T17:00:00.000Z")},i=void 0,l={permalink:"/content/revolutionising-language-learning",source:"@site/posts/revolutionising-language-learning/index.md",title:"Revolutionising Language Learning: How AI and Oracle APEX Transform Everyday Challenges",description:"I'm finally finding more time, post-conference, to catch up on my writing. In this latest post, I'd like to talk about a presentation that I did at Kscope23, \"Solving Everyday Problems with the Oracle Cloud\". During the session, I had described and demonstrated how we can use the Oracle Cloud Infrastructure (OCI) AI services and Oracle APEX to build compelling solutions, even for personal life challenges.",date:"2023-08-20T17:00:00.000Z",formattedDate:"August 20, 2023",tags:[{label:"artificial intelligence",permalink:"/content/tags/artificial-intelligence"},{label:"machine learning",permalink:"/content/tags/machine-learning"},{label:"speech recognition",permalink:"/content/tags/speech-recognition"},{label:"language learning",permalink:"/content/tags/language-learning"},{label:"oracle ai",permalink:"/content/tags/oracle-ai"},{label:"oracle apex",permalink:"/content/tags/oracle-apex"},{label:"ai applications",permalink:"/content/tags/ai-applications"},{label:"cloud solutions",permalink:"/content/tags/cloud-solutions"},{label:"technical workflow",permalink:"/content/tags/technical-workflow"},{label:"innovative technology",permalink:"/content/tags/innovative-technology"}],readingTime:10.49,hasTruncateMarker:!0,authors:[{name:"Adrian Png",title:"Senior Cloud Solutions Architect @ Insum",url:"https://github.com/fuzziebrain",imageURL:"https://github.com/fuzziebrain.png",key:"fuzziebrain"}],frontMatter:{title:"Revolutionising Language Learning: How AI and Oracle APEX Transform Everyday Challenges",tags:["artificial intelligence","machine learning","speech recognition","language learning","oracle ai","oracle apex","ai applications","cloud solutions","technical workflow","innovative technology"],categories:["Technology"],authors:["fuzziebrain"],date:"2023-08-20T17:00:00.000Z"},prevItem:{title:"Train an AI Model to Recognize Oracle APEX Challenge Coins",permalink:"/content/train-an-ai-model-to-recognize-oracle-apex-challenge-coins"},nextItem:{title:"Say Hello From An Autonomous Database",permalink:"/content/say-hello-from-an-autonomous-database"}},s={authorsImageUrls:[void 0]},c=[{value:"Background",id:"background",level:2},{value:"New AI-Enhanced Data Entry Workflow",id:"new-ai-enhanced-data-entry-workflow",level:2},{value:"Speech-to-Text",id:"speech-to-text",level:3},{value:"Translate Text",id:"translate-text",level:3},{value:"Summary",id:"summary",level:2}],d={toc:c};function p(e){let{components:n,...r}=e;return(0,o.mdx)("wrapper",(0,a.Z)({},d,r,{components:n,mdxType:"MDXLayout"}),(0,o.mdx)("youtube",{youTubeId:"aYnYQu7MY2g"}),(0,o.mdx)("p",null,"I'm finally finding more time, post-conference, to catch up on my writing. In this latest post, I'd like to talk about a presentation that I did at ",(0,o.mdx)("a",{parentName:"p",href:"https://kscope23.odtug.com"},"Kscope23"),', "Solving Everyday Problems with the Oracle Cloud". During the session, I had described and demonstrated how we can use the ',(0,o.mdx)("a",{parentName:"p",href:"https://www.oracle.com/cloud"},(0,o.mdx)("em",{parentName:"a"},"Oracle Cloud Infrastructure"))," (OCI) ",(0,o.mdx)("a",{parentName:"p",href:"https://www.oracle.com/artificial-intelligence/ai-services/"},(0,o.mdx)("em",{parentName:"a"},"AI services"))," and ",(0,o.mdx)("a",{parentName:"p",href:"https://apex.oracle.com"},(0,o.mdx)("em",{parentName:"a"},"Oracle APEX"))," to build compelling solutions, even for personal life challenges."),(0,o.mdx)("h2",{id:"background"},"Background"),(0,o.mdx)("p",null,"We are constantly bombarded by news about ",(0,o.mdx)("em",{parentName:"p"},"Artificial Intelligence")," (AI) and ",(0,o.mdx)("em",{parentName:"p"},"Machine Learning")," (ML), driving many of us seek to find a strong use case to take advantage of these technologies and use them in our work and personal projects. I am no exception."),(0,o.mdx)("p",null,"A few years ago, I started learning the Japanese language using ",(0,o.mdx)("a",{parentName:"p",href:"https://www.duolingo.com/"},"Duolingo"),", but had a lot of trouble remembering the Hiragana, Katakana, Kanji characters, same issues when I learned Mandarin in school. Back in the days, learning Mandarin was through rote learning, and that sometimes involved flashcards. In August 2021, I published an ",(0,o.mdx)("a",{parentName:"p",href:"https://developer.oracle.com/learn/technical-articles/learning-languages-with-oracle-cloud"},"article")," through the Oracle Developer Relations team, about how I had used a ",(0,o.mdx)("a",{parentName:"p",href:"https://www.m5stack.com"},"M5Stack")," IoT device and some OCI resources to build a self-updating electronic flashcard that I could place on the refrigerator door. However, constantly updating the content repository can be tedious."),(0,o.mdx)("p",null,"Less than a year later, two developments at Oracle made it easier for me to enhance the data entry process with AI-based technologies, specifically, ",(0,o.mdx)("em",{parentName:"p"},"Natural Language Processing"),' (NLP). First, the Speech service was released on Feb 23, 2022. Later, in October 2022, a second version of the Language service was released with a new feature for performing language translation tasks. Both these added features from Oracle AI provided me an opportunity to "use AI" to improve my approach for entering new Japanese words into my online personal word bank.'),(0,o.mdx)("p",null,"I had blogged about this project ",(0,o.mdx)("a",{parentName:"p",href:"/content/id/2201"},"previously"),", to celebrate the Joel Kallman Day last year. This follow-up article provides a technical deep dive to guide readers on how to implement as similar solution."),(0,o.mdx)("h2",{id:"new-ai-enhanced-data-entry-workflow"},"New AI-Enhanced Data Entry Workflow"),(0,o.mdx)("p",null,'"A picture speaks a thousand words", so here\'s are two workflow diagrams that provides a crude overview of how new words are entered today.'),(0,o.mdx)("h3",{id:"speech-to-text"},"Speech-to-Text"),(0,o.mdx)("mermaid",{value:"flowchart TD\n    A([Create a new entry]) --\x3e B[Click Record and say a word or sentence.]\n    B --\x3e C{Has the recording stopped?}\n    C --\x3e |Yes| DY[Upload Base64-encoded audio file to the input object storage location.]\n    C --\x3e |No| DN[Continue waiting] --\x3e C\n    DY --\x3e E[Create a Speech transcription job.]\n    E --\x3e F{Is transcription job's status = SUCCEEDED}\n    F --\x3e |Yes| GY[Retrieve transcription job details.]\n    F --\x3e |No| FA{Is it CANCELED or FAILED?}\n    FA --\x3e |Yes| FAY[/Display an error message./]\n    FAY ----\x3e Z\n    FA --\x3e |No| FAN[Continue waiting] --\x3e F\n    GY --\x3e H[Parse the transcription task result and extract the output's object storage URL from the job details.]\n    H --\x3e I[Retrieve the transcription task result using the object storage URL.]\n    I --\x3e J[Parse the transcription task result and extract the transcribed text.]\n    J --\x3e K[/Display the transcribed text in the target APEX page item./]\n    K --\x3e Z([End])"}),(0,o.mdx)("p",null,"As mentioned in my earlier post, when the speech service was first launched, it only supported the Waveform Audio File Format (WAV), and had strict rules on sample rates and channels. Today, the service now supports a much larger number of formats including ",(0,o.mdx)("em",{parentName:"p"},"FLAC"),", ",(0,o.mdx)("em",{parentName:"p"},"OGG"),", and ",(0,o.mdx)("em",{parentName:"p"},"WEBM"),". In the latest version of my application, only the ",(0,o.mdx)("a",{parentName:"p",href:"https://developer.mozilla.org/docs/Web/API/MediaStream_Recording_API"},(0,o.mdx)("em",{parentName:"a"},"MediaStream Recording API"))," is used."),(0,o.mdx)("p",null,"The audio capture and transcription job requests are performed through an Oracle APEX modal page shown below."),(0,o.mdx)("p",null,(0,o.mdx)("img",{alt:"The Oracle APEX modal page for recording and transcribing audio.",src:t(40434).Z,width:"2560",height:"1440"})),(0,o.mdx)("p",null,"To start off, we will need some utility JavaScript functions, and for simplicity, I had declared this in the page's ",(0,o.mdx)("em",{parentName:"p"},"Function and Global Variable Declaration"),". Firstly, there's the ",(0,o.mdx)("inlineCode",{parentName:"p"},"mediaRecorder")," variable that stores the reference to the ",(0,o.mdx)("em",{parentName:"p"},"MediaRecorder")," object that we will instantiate at page load. It is followed by a simple utility functions that I had borrowed from ",(0,o.mdx)("a",{parentName:"p",href:"https://apexplained.wordpress.com"},"Nick Buytaert"),". Reference to his blog post is in the comments."),(0,o.mdx)("pre",null,(0,o.mdx)("code",{parentName:"pre",className:"language-javascript"},"let mediaRecorder;\n\nconst util = {\n  // builds a js array from long string\n  // credit: https://apexplained.wordpress.com/2016/09/12/chunked-multi-file-upload-with-ajax/\n  clob2Array: function (clob, size) {\n    let array = [];\n    loopCount = Math.floor(clob.length / size) + 1;\n    for (let i = 0; i < loopCount; i++) {\n      array.push(clob.slice(size * i, size * (i + 1)));\n    }\n    return array;\n  },\n  // converts blob to base64 string\n  blob2base64: function (blob) {\n    return new Promise(function(resolve, reject){\n      const fileReader = new FileReader();\n      fileReader.onerror = reject;\n      fileReader.onload = function() {\n        const dataURI = fileReader.result;\n        resolve(dataURI.substr(dataURI.indexOf(',') + 1));\n      }\n      fileReader.readAsDataURL(blob);\n    });\n  }\n};\n")),(0,o.mdx)("p",null,"Next, I added the following JavaScript code in the ",(0,o.mdx)("em",{parentName:"p"},"Execute when Page Loads")," that would instantiate the ",(0,o.mdx)("em",{parentName:"p"},"MediaRecorder")," object, and implement the callback function when recording is stopped. When that function is called, the recorded audio will be loaded into an embedded audio player, allowing the user to playback the recording. It will also call an AJAX callback (",(0,o.mdx)("inlineCode",{parentName:"p"},"TRANSCRIBE_AUDIO"),") process that includes the audio MIME-type, and the recording as a base64-encoded string. That's where the ",(0,o.mdx)("inlineCode",{parentName:"p"},"util")," package comes in handy."),(0,o.mdx)("pre",null,(0,o.mdx)("code",{parentName:"pre",className:"language-javascript"},'if (navigator.mediaDevices) {\n  console.debug("getUserMedia supported.");\n  /**\n   * While FLAC is preferred, it appears Firefox and Chrome currently do not\n   * support this lossless audio format.\n   **/\n  let audioMimeType = \'audio/webm\';\n\n  console.debug(audioMimeType + \' is\' +\n    (MediaRecorder.isTypeSupported(audioMimeType) ? \'\' : \' not\')\n    + \' supported.\');\n\n  const constraints = { audio: true };\n  let chunks = [];\n\n  navigator.mediaDevices\n    .getUserMedia(constraints)\n    .then((stream) => {\n      mediaRecorder = new MediaRecorder(stream);\n\n      mediaRecorder.onstop = async (e) => {\n        console.debug("data available after MediaRecorder.stop() called.");\n        var popup = apex.widget.waitPopup();\n\n        const blob = new Blob(chunks, { type: audioMimeType });\n        chunks = [];\n        const audioURL = URL.createObjectURL(blob);\n        player.src = audioURL;\n        console.debug("recorder stopped");\n\n        const base64 = await util.blob2base64(blob);\n\n        var process = apex.server.process(\n          "TRANSCRIBE_AUDIO",\n          {\n            x01: audioMimeType,\n            f01: util.clob2Array(base64, 32000)\n          }\n        );\n\n        process.done(function(result) {\n          apex.item("P2_TRANSCRIBED_TEXT").setValue(result.transcribedText);\n          apex.message.showPageSuccess("Audio transcribed.");\n        }).fail(function(jqXHR, textStatus, errorMessage) {\n          apex.message.alert("Failed to process audio.");\n          console.error("Failed to save. Error: " + errorMessage);\n        }).always(function() {\n          popup.remove();\n        });\n      };\n\n      mediaRecorder.ondataavailable = (e) => {\n        chunks.push(e.data);\n      };\n    })\n    .catch((err) => {\n      console.error(`The following error occurred: ${err}`);\n    });\n}\n')),(0,o.mdx)("p",null,"The AJAX callback named ",(0,o.mdx)("inlineCode",{parentName:"p"},"TRANSCRIBE_AUDIO")," contains the following PL/SQL procedure:"),(0,o.mdx)("pre",null,(0,o.mdx)("code",{parentName:"pre",className:"language-sql"},"declare\n  c_audio_mime_type constant apex_application.g_x01%type := apex_application.g_x01;\n  l_clob_temp clob;\n  l_blob_content blob;\n  l_buffer varchar2(32767);\n  l_filename varchar2(30) := 'sample-' || to_char(systimestamp, 'YYYYMMSSHH24MISS')\n    || '.' || regexp_replace(c_audio_mime_type, '^audio/(.+)$', '\\1');\n  l_transcribed_text varchar2(32767);\nbegin\n  dbms_lob.createtemporary(\n    lob_loc => l_clob_temp\n    , cache => false\n    , dur => dbms_lob.session\n  );\n\n  for i in 1..apex_application.g_f01.count loop\n    l_buffer := apex_application.g_f01(i);\n\n    dbms_lob.writeappend(\n      lob_loc => l_clob_temp\n      , amount => length(l_buffer)\n      , buffer => l_buffer\n    );\n  end loop;\n\n  l_blob_content := apex_web_service.clobbase642blob(l_clob_temp);\n\n  pkg_oci_os_util.p_upload_object(\n    p_bucket_name => :G_INPUT_BUCKET_NAME\n    , p_file_blob => l_blob_content\n    , p_filename => l_filename\n    , p_mime_type =>  c_audio_mime_type\n  );\n\n  dbms_lob.freetemporary(l_clob_temp);\n\n  l_transcribed_text := pkg_oci_speech_util.f_transcribe_audio(\n    p_input_bucket_name => :G_INPUT_BUCKET_NAME\n    , p_output_bucket_name => :G_OUTPUT_BUCKET_NAME\n    , p_filename => l_filename\n  );\n\n  sys.htp.p('{ \"transcribedText\": \"' || l_transcribed_text || '\"}');\nend;\n")),(0,o.mdx)("p",null,"The callback process converts the base64-encoded string back to a BLOB, and then uploads it to the target Object Storage bucket defined by the APEX substitution string ",(0,o.mdx)("inlineCode",{parentName:"p"},"G_INPUT_BUCKET_NAME"),". Once the file has been uploaded successfully, the Speech service transcription job is then created using a facade. The PL/SQL function takes in the input and output bucket names, as well as the audio filename, and returns the transcribed text. The function also has an optional parameter ",(0,o.mdx)("inlineCode",{parentName:"p"},"p_source_language_code")," that lets the developer specify the language used in audio recording. The default value is ",(0,o.mdx)("inlineCode",{parentName:"p"},"en-US"),"."),(0,o.mdx)("blockquote",null,(0,o.mdx)("p",{parentName:"blockquote"},(0,o.mdx)("strong",{parentName:"p"},"NOTE")),(0,o.mdx)("p",{parentName:"blockquote"},"If you have not worked with the Object Storage service in APEX before, be sure to check out the LiveLabs ",(0,o.mdx)("a",{parentName:"p",href:"https://apexapps.oracle.com/pls/apex/r/dbpm/livelabs/view-workshop?wid=3558"},"workshop")," I had prepared on this subject. I will also be presenting this workshop in-person at the upcoming ",(0,o.mdx)("a",{parentName:"p",href:"https://www.oracle.com/cloudworld/"},"Oracle CloudWorld 2023"),". The session number is ",(0,o.mdx)("a",{parentName:"p",href:"https://reg.rf.oracle.com/flow/oracle/cwoh23/catalog/page/catalog/session/1683745130623001qcie"},"HOL2327"),".")),(0,o.mdx)("p",null,"This function actually makes several OCI REST API calls. The first one creates the transcription job using standard ",(0,o.mdx)("inlineCode",{parentName:"p"},"APEX_WEB_SERVICE")," code shown in the snippet below:"),(0,o.mdx)("pre",null,(0,o.mdx)("code",{parentName:"pre",className:"language-sql"},"apex_web_service.g_request_headers(1).name := 'Content-Type';\napex_web_service.g_request_headers(1).value := 'application/json';\n\nl_response := apex_web_service.make_rest_request(\n  p_url => l_request_url\n  , p_http_method => 'POST'\n  , p_body => json_object(\n      key 'compartmentId' value pkg_oci_speech_util.gc_compartmentid\n      , key 'inputLocation' value json_object(\n          key 'locationType' value 'OBJECT_LIST_INLINE_INPUT_LOCATION'\n          , key 'objectLocations' value json_array(\n              json_object(\n                key 'bucketName' value p_input_bucket_name\n                , key 'namespaceName' value pkg_oci_speech_util.gc_namespace\n                , key 'objectNames' value json_array(p_filename)\n              )\n          )\n      )\n      , key 'outputLocation' value json_object(\n          key 'bucketName' value p_output_bucket_name\n          , key 'namespaceName' value pkg_oci_speech_util.gc_namespace\n          , key 'prefix' value 'apex'\n      )\n      , key 'normalization' value json_object(\n          key 'isPunctuationEnabled' value true\n      )\n      , key 'modelDetails' value json_object(\n          key 'languageCode' value p_source_language_code\n      )\n  )\n  , p_credential_static_id => pkg_oci_speech_util.gc_credential_static_id\n);\n")),(0,o.mdx)("p",null,"The ",(0,o.mdx)("em",{parentName:"p"},"CreateTranscriptionJob")," API calls are asynchronous. When the job was created, the API returns a response object containing details about the transcription job, including the ",(0,o.mdx)("em",{parentName:"p"},"OCID"),", and the job's status (the ",(0,o.mdx)("inlineCode",{parentName:"p"},"id")," and ",(0,o.mdx)("inlineCode",{parentName:"p"},"lifecycleState")," attribute respectively). Rarely will the job complete immediately, therefore, the code then loops, making a call to the ",(0,o.mdx)("em",{parentName:"p"},"GetTranscriptionJob")," endpoint to get the current status until either job has succeeded, failed, or was cancelled."),(0,o.mdx)("p",null,"If the transcription job was successful, the function then retrieves the task details using the ",(0,o.mdx)("em",{parentName:"p"},"GetTranscriptionTask")," API. The response will contain the required object storage location of the transcription results written in a JSON file (see example below)."),(0,o.mdx)("pre",null,(0,o.mdx)("code",{parentName:"pre",className:"language-json"},'{\n  "status": "SUCCESS",\n  "timeCreated": "2023-08-20 23:32:46.96",\n  "modelDetails": {\n    "domain": "GENERIC",\n    "languageCode": "en-US"\n  },\n  "audioFormatDetails": {\n    "format": "WEBM",\n    "numberOfChannels": 2,\n    "encoding": "OPUS",\n    "sampleRateInHz": 48000\n  },\n  "transcriptions": [\n    {\n      "transcription": "Hello, how are you?",\n      "confidence": "0.9600",\n      "tokens": [\n        {\n          "token": "This",\n          "startTime": "0.624s",\n          "endTime": "1.104s",\n          "confidence": "0.9527",\n          "type": "WORD"\n        },\n        ...\n      ]\n    }\n  ]\n}\n')),(0,o.mdx)("p",null,"In this file, you can find details about the source audio file, the transcription, the confidence value of the transcription, and details of individual tokens. The function retrieves this file from the object storage, parses it, and then returns the value of the ",(0,o.mdx)("inlineCode",{parentName:"p"},"transcription")," attribute."),(0,o.mdx)("p",null,"Finally, this page contains the following UI components:"),(0,o.mdx)("ul",null,(0,o.mdx)("li",{parentName:"ul"},"An audio player to playback the recording. It can be embedded within a ",(0,o.mdx)("em",{parentName:"li"},"Static Content")," region using the source:",(0,o.mdx)("pre",{parentName:"li"},(0,o.mdx)("code",{parentName:"pre",className:"language-html"},'<audio id="player" controls style="width: 100%;"></audio>\n'))),(0,o.mdx)("li",{parentName:"ul"},"A textarea page item called ",(0,o.mdx)("inlineCode",{parentName:"li"},"P2_TRANSCRIBED_TEXT"),"; and"),(0,o.mdx)("li",{parentName:"ul"},"Three buttons ",(0,o.mdx)("em",{parentName:"li"},"Record"),", ",(0,o.mdx)("em",{parentName:"li"},"Stop"),", and ",(0,o.mdx)("em",{parentName:"li"},"Accept"),".")),(0,o.mdx)("p",null,"The first two buttons trigger the media recorder object to start and stop recording. Simply attach dynamic actions for mouse click events and execute the corresponding JavaScript code."),(0,o.mdx)("pre",null,(0,o.mdx)("code",{parentName:"pre",className:"language-javascript"},"// For the start button.\nif(mediaRecorder) {\n  mediaRecorder.start();\n}\n\n// For the stop button.\nif(mediaRecorder) {\n  mediaRecorder.stop();\n}\n")),(0,o.mdx)("p",null,"The ",(0,o.mdx)("em",{parentName:"p"},"Accept")," button executes the ",(0,o.mdx)("em",{parentName:"p"},"Close Dialog")," process, returning the value of the page item ",(0,o.mdx)("inlineCode",{parentName:"p"},"P2_TRANSCRIBED_TEXT")," that is used to set the value of ",(0,o.mdx)("inlineCode",{parentName:"p"},"P1_TEXT")," in the parent page."),(0,o.mdx)("h3",{id:"translate-text"},"Translate Text"),(0,o.mdx)("mermaid",{value:"flowchart TD\n    A([Click the button Translate.]) --\x3e B[Call the Language service's REST API, specifying the source and target language codes.]\n    B --\x3e C{Is the HTTP response status code = 200?}\n    C --\x3e |Yes| CY[Parse the response and extract the translated text.]\n    CY --\x3e D[Display the translated text in the target APEX page item.]\n    C --\x3e |No| CN[/Display error message./]\n    CN ---\x3e Z\n    D --\x3e Z([End])"}),(0,o.mdx)("p",null,"The Language service API is a lot simpler to consume. When the ",(0,o.mdx)("em",{parentName:"p"},"Translate")," button is clicked, the page is submitted, and a page submission process is called with to execute the following PL/SQL procedure:"),(0,o.mdx)("pre",null,(0,o.mdx)("code",{parentName:"pre",className:"language-sql"},"declare\n  c_rest_url constant varchar2(200) := 'https://language.aiservice.us-phoenix-1.oci.oraclecloud.com/20221001/actions/batchLanguageTranslation';\n  c_from_lang constant varchar2(2) := 'en';\n  c_compartment_ocid varchar2(1024) := 'ocid1.compartment.oc1..*****';\n  c_credential_static_id constant varchar2(50) := 'OCI_CREDENTIALS';\n  c_number_of_text_to_translate pls_integer := 1;\n\n  l_request_body json_object_t;\n  l_document json_object_t;\n  l_documents json_array_t;\n  l_response clob;\nbegin\n  l_request_body := json_object_t();\n  l_documents := json_array_t();\n\n  /**\n   * The Language service APIs process requests in batches. Even though we are\n   * only submitting a single text to be translated, I have written a loop to\n   * construct the request body to \"future proof\" the implementation. ;-)\n   **/\n  for i in 1..c_number_of_text_to_translate\n  loop\n    l_document := json_object_t();\n    l_document.put('key', to_char(i));\n    l_document.put('text', :P1_TEXT);\n    l_document.put('languageCode', c_from_lang);\n    l_documents.append(l_document);\n  end loop;\n\n  l_request_body.put('documents', treat(l_documents as json_element_t));\n  l_request_body.put('targetLanguageCode', :P1_TO_LANG);\n  l_request_body.put('compartmentId', c_compartment_ocid);\n\n  apex_debug.info(l_request_body.to_string());\n\n  apex_web_service.g_request_headers(1).name := 'Content-Type';\n  apex_web_service.g_request_headers(1).value := 'application/json';\n\n  l_response := apex_web_service.make_rest_request(\n    p_url => c_rest_url\n    , p_http_method => 'POST'\n    , p_body => l_request_body.to_string()\n    , p_credential_static_id => c_credential_static_id\n  );\n\n  if apex_web_service.g_status_code != 200 then\n    apex_debug.error('HTTP Status Code: ' || apex_web_service.g_status_code);\n    apex_debug.error(l_response);\n    raise_application_error(-20002, 'Translation unsuccessful! HTTP Status Code: '\n      || apex_web_service.g_status_code);\n  else\n    apex_debug.info(l_response);\n    l_documents := treat(json_object_t.parse(l_response).get('documents') as json_array_t);\n    for i in 0..(l_documents.get_size() - 1)\n    loop\n      l_document := treat(l_documents.get(0) as json_object_t);\n      :P1_TRANSLATED_TEXT := l_document.get_string('translatedText');\n    end loop;\n  end if;\nend;\n")),(0,o.mdx)("p",null,"That's it!"),(0,o.mdx)("h2",{id:"summary"},"Summary"),(0,o.mdx)("p",null,"In this article, we have delved into the remarkable synergy between AI and Oracle APEX. By harnessing cutting-edge AI features like speech-to-text transcription and language translation, we've unveiled a world of possibilities for enhancing communication and learning. Whether you're a beginner or an expert in the realm of machine learning, I hope this article has provided you a head start and inspiration for creating your own innovative solutions to solving day-to-day challenges using low-code technologies."))}p.isMDXComponent=!0},40434:(e,n,t)=>{t.d(n,{Z:()=>a});const a=t.p+"assets/images/record-speech-modal-a556f4504c0203ca1cbd083f6bd024d3.png"}}]);