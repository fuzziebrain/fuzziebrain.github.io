"use strict";(self.webpackChunknewsite=self.webpackChunknewsite||[]).push([[93316],{91145:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>s,default:()=>h,frontMatter:()=>o,metadata:()=>r,toc:()=>c});var a=t(85893),i=t(11151);const o={title:"Deploy and Use Fine-Tuned LLMs in Oracle APEX",description:"Learn how Oracle Cloud Infrastructure (OCI) resources can be used to fine-tune and deploy open source large language models (LLMs). Then, build an Oracle APEX application that users may use to submit feedback and have their reviews automatically scored.",image:"./assets/llms-for-analyzing-customer-reviews.png",tags:["large language models","llms","machine learning","oci","data science","python","jupyter","conda","oracle apex"],categories:["Technology"],authors:["fuzziebrain"],date:new Date("2023-11-05T01:35:00.000Z")},s=void 0,r={permalink:"/content/deploy-and-use-fine-tuned-llms-in-oracle-apex",source:"@site/posts/deploy-and-use-fine-tuned-llms-in-oracle-apex/index.md",title:"Deploy and Use Fine-Tuned LLMs in Oracle APEX",description:"Learn how Oracle Cloud Infrastructure (OCI) resources can be used to fine-tune and deploy open source large language models (LLMs). Then, build an Oracle APEX application that users may use to submit feedback and have their reviews automatically scored.",date:"2023-11-05T01:35:00.000Z",formattedDate:"November 5, 2023",tags:[{label:"large language models",permalink:"/content/tags/large-language-models"},{label:"llms",permalink:"/content/tags/llms"},{label:"machine learning",permalink:"/content/tags/machine-learning"},{label:"oci",permalink:"/content/tags/oci"},{label:"data science",permalink:"/content/tags/data-science"},{label:"python",permalink:"/content/tags/python"},{label:"jupyter",permalink:"/content/tags/jupyter"},{label:"conda",permalink:"/content/tags/conda"},{label:"oracle apex",permalink:"/content/tags/oracle-apex"}],readingTime:9.955,hasTruncateMarker:!0,authors:[{name:"Adrian Png",title:"Senior Cloud Solutions Architect @ Insum",url:"https://github.com/fuzziebrain",imageURL:"https://github.com/fuzziebrain.png",key:"fuzziebrain"}],frontMatter:{title:"Deploy and Use Fine-Tuned LLMs in Oracle APEX",description:"Learn how Oracle Cloud Infrastructure (OCI) resources can be used to fine-tune and deploy open source large language models (LLMs). Then, build an Oracle APEX application that users may use to submit feedback and have their reviews automatically scored.",image:"./assets/llms-for-analyzing-customer-reviews.png",tags:["large language models","llms","machine learning","oci","data science","python","jupyter","conda","oracle apex"],categories:["Technology"],authors:["fuzziebrain"],date:"2023-11-05T01:35:00.000Z"},unlisted:!1,prevItem:{title:"Generate Art Using Latent Diffusion Models and NFC Tags",permalink:"/content/generate-art-using-diffusion-models-and-nfc-tags"},nextItem:{title:"Predict the Rugby World Cup 2023 Winner",permalink:"/content/predict-the-rugby-world-cup-2023-winner"}},l={image:t(309).Z,authorsImageUrls:[void 0]},c=[{value:"Provisioning the Tools",id:"provisioning-the-tools",level:2},{value:"Fine-tune the LLM",id:"fine-tune-the-llm",level:2},{value:"Deploy the Fine-tuned LLM",id:"deploy-the-fine-tuned-llm",level:2},{value:"What About the OCI AI Language Service",id:"what-about-the-oci-ai-language-service",level:2},{value:"Summary",id:"summary",level:2}];function d(e){const n={a:"a",blockquote:"blockquote",code:"code",em:"em",h2:"h2",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.a)(),...e.components},{Youtube:o}=n;return o||function(e,n){throw new Error("Expected "+(n?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}("Youtube",!0),(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.p,{children:(0,a.jsx)(n.img,{alt:"LLMs for analyzing customer reviews.",src:t(50553).Z+"",width:"1280",height:"720"})}),"\n",(0,a.jsxs)(n.p,{children:["In my previous ",(0,a.jsx)(n.a,{href:"/content/predict-the-rugby-world-cup-2023-winner",children:"post"}),", I wrote about how a ",(0,a.jsx)(n.a,{href:"https://scikit-learn.org",children:"scikit-learn"})," machine learning (ML) could be trained and deployed on the ",(0,a.jsx)(n.a,{href:"https://oracle.com/cloud",children:"Oracle Cloud Infrastructure"})," (OCI) ",(0,a.jsx)(n.a,{href:"https://www.oracle.com/artificial-intelligence/data-science/",children:"Data Science"})," service. The model is deployed on the service's managed infrastructure, allowing developers to simply call a HTTP endpoint to perform ML model inference on the submitted data. In my latest adventure, I built an ",(0,a.jsx)(n.a,{href:"https://apex.oracle.com",children:"Oracle APEX"})," application that takes product reviews and then automatically ranks the review using a fine-tuned large language model (LLM) available through ",(0,a.jsx)(n.a,{href:"https://huggingface.co",children:"Hugging Face"}),"."]}),"\n",(0,a.jsxs)(n.p,{children:["While OpenAI's ",(0,a.jsx)(n.a,{href:"https://openai.com/chatgpt",children:"ChatGPT"})," has largely dominated the news headlines when it comes to ",(0,a.jsx)(n.em,{children:"Generative AI"}),", there is a plethora of LLMs out there. Many of them are open sourced and readily available through hubs like Hugging Face. There are a bunch of tutorials that introduce the platform including the available datasets, models, Python, and JavaScript libraries. In this post, I will use the example from their tutorial on fine-tuning a pre-trained model."]}),"\n",(0,a.jsxs)(n.p,{children:["The goal is to use the ",(0,a.jsx)(n.a,{href:"https://huggingface.co/datasets/yelp_review_full",children:"Yelp review"})," dataset to fine-tune ",(0,a.jsx)(n.a,{href:"https://arxiv.org/abs/1810.04805",children:"BERT"}),", specifically, the ",(0,a.jsx)(n.a,{href:"https://huggingface.co/distilbert-base-uncased",children:"distilbert-base-uncased"})," model, to assign a rank that describes the sentiment expressed in a body of text."]}),"\n",(0,a.jsx)(n.h2,{id:"provisioning-the-tools",children:"Provisioning the Tools"}),"\n",(0,a.jsx)(n.p,{children:"Machine learning tasks can be performed on a CPU, but more often than not, we have a huge performance gained by using a graphical processing unit (GPU). For most people, that's going to be a Nvidia chip. On the OCI, we currently have a choice of the P100, V100, and more recently, A10 and A100. I went with an A10 as it was newer, cheaper than the A100, and has adequate amount of GPU memory. And, more importantly, the tenancy that I was on had sufficient number of GPUs assigned."}),"\n",(0,a.jsxs)(n.p,{children:["You can check your tenancy's service limits to see if you can provision a GPU instance. The resource that you will need is either ",(0,a.jsx)(n.code,{children:"gpu-a10-count"})," if you wish to run the training on an OCI Compute instance, or ",(0,a.jsx)(n.code,{children:"ds-gpu-a10-count"})," for an OCI Data Science notebook. These limits are assigned differently for Compute instance and Data Science notebooks. If your tenancy is either a trial, or a ",(0,a.jsx)(n.em,{children:"PAYG"})," (Pay-as-you-go) account, then these limits are likely ",(0,a.jsx)(n.code,{children:"0"}),". For ",(0,a.jsx)(n.em,{children:"PAYG"}),", you could try making a service limit request, but for trial accounts, it's an uphill battle. And for accounts with a ",(0,a.jsx)(n.em,{children:"Universal Credits"}),", you might find ",(0,a.jsx)(n.code,{children:"16"})," is the number for ",(0,a.jsx)(n.code,{children:"gpu-a10-count"}),". Make a service limit request for ",(0,a.jsx)(n.code,{children:"ds-gpu-a10-count"})," if you wish to run the training in a GPU-powered notebook session."]}),"\n",(0,a.jsxs)(n.p,{children:["I initially did not have access to GPU resources on OCI Data Science, so I had provisioned a Compute with a GPU, and then all I needed was to install ",(0,a.jsx)(n.a,{href:"https://conda.io/miniconda",children:"Miniconda"}),", and I was good to go!"]}),"\n",(0,a.jsx)(n.h2,{id:"fine-tune-the-llm",children:"Fine-tune the LLM"}),"\n",(0,a.jsxs)(n.p,{children:["In OCI Data Science, you can probably use the ",(0,a.jsx)(n.a,{href:"https://conda.io",children:"Conda"})," environments ",(0,a.jsx)(n.code,{children:"generalml_p38_cpu_v1"})," or ",(0,a.jsx)(n.code,{children:"pytorch20_p39_gpu_v2"})," (if you have a GPU instance provisioned) for fine-tuning the model. However, for deployment, a GPU instance probably isn't necessary as the time needed to complete the inference is relatively much shorter."]}),"\n",(0,a.jsx)(n.p,{children:"If you do use the Oracle-provided Conda environments, please note that you might need to add any missing dependencies that are specific for working with Hugging Face models and datasets. For me, I opted to create a custom Conda environment as I had started out training the model on a GPU-powered compute instance. This environment was published and later used for the model deployment. Below is the Conda environment file I used."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-yaml",children:"channels:\n    - conda-forge\n\ndependencies:\n    - python>=3.8\n    - transformers\n    - evaluate\n    - scikit-learn\n    - accelerate\n    - pillow\n    - numpy\n    - pip\n    - pip:\n        - torch==2.1.0\n        - oracle-ads[data,notebook]\n"})}),"\n",(0,a.jsxs)(n.blockquote,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"NOTE"})}),"\n",(0,a.jsxs)(n.p,{children:["If you're running this in an OCI Data Science notebook, then the ",(0,a.jsx)(n.code,{children:"jupyterlab"})," and ",(0,a.jsx)(n.code,{children:"ipywidgets"})," dependencies can be omitted as they are automatically added when you create a conda environment using the ",(0,a.jsx)(n.code,{children:"odsc"})," command."]}),"\n"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'from datasets import load_dataset\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\nimport numpy as np\nimport evaluate\nimport torch\n\ndef tokenize_function(examples):\n    return tokenizer(examples["text"], padding="max_length", truncation=True)\n\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    predictions = np.argmax(logits, axis=-1)\n    return metric.compute(predictions=predictions, references=labels)\n\ndevice = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")\n\nmodel_name = "distilbert-base-uncased"\ndataset = load_dataset("yelp_review_full")\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmetric = evaluate.load("accuracy")\n\ntokenized_datasets = dataset.map(tokenize_function, batched=True)\n\nsmall_train_dataset = tokenized_datasets["train"].shuffle(seed=42).select(range(1000))\nsmall_eval_dataset = tokenized_datasets["test"].shuffle(seed=42).select(range(1000))\n\nmodel = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=5)\nmodel.to(device)\ntraining_args = TrainingArguments(output_dir="test_trainer",\n                                  evaluation_strategy="epoch",\n                                  num_train_epochs=20,\n                                  fp16=True)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=small_train_dataset,\n    eval_dataset=small_eval_dataset,\n    compute_metrics=compute_metrics,\n)\n\ntrainer.train()\n'})}),"\n",(0,a.jsx)(n.p,{children:"I wanted to use this model in a different tenancy, and so, after fine-tuning the model in the Compute instance, I exported the model to be deployed in an OCI Data Science project. To save the model, I ran this function:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'model.save_pretrained("./mymodel")\n'})}),"\n",(0,a.jsxs)(n.p,{children:["The model artifacts are saved in the local directory ",(0,a.jsx)(n.code,{children:"mymodel"}),". It contains the files ",(0,a.jsx)(n.code,{children:"config.json"})," and ",(0,a.jsx)(n.code,{children:"pytorch_model.bin"}),". A tarball was then created, and transferred to the OCI Data Science project where I would deploy the model."]}),"\n",(0,a.jsxs)(n.p,{children:["The ",(0,a.jsx)(n.code,{children:"distilbert-base-uncased"})," model is relatively small with only 67 million parameters, and thus required only about 4 GB of GPU memory for the training. Just for fun, I attempted to do the training on ",(0,a.jsx)(n.code,{children:"bert-large-uncased"})," that is a large model with 336 million parameters. Here's how much memory it took."]}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.img,{alt:"GPU status reported by nvidia-smi command.",src:t(18809).Z+"",width:"1894",height:"1356"})}),"\n",(0,a.jsx)(n.p,{children:"Unfortunately, the accuracy wasn't as impressive even with more parameters. More work would be required before I could eventually use larger LLMs such as this."}),"\n",(0,a.jsx)(n.h2,{id:"deploy-the-fine-tuned-llm",children:"Deploy the Fine-tuned LLM"}),"\n",(0,a.jsxs)(n.p,{children:["The Oracle ",(0,a.jsx)(n.a,{href:"https://accelerated-data-science.readthedocs.io",children:"Accelerated Data Science"})," Python libraries provides the data scientists and machine learning engineers as suite of utility packages to load datasets from various sources on the OCI, and other repositories, perform machine learning and MLOps tasks, and many more. Since version 2.8.2, the ADS provides support for Hugging Face models using the class ",(0,a.jsx)(n.em,{children:"HuggingFacePipelineModel"}),". Hugging Face pipelines are convenient facade for developers to use models for inference."]}),"\n",(0,a.jsxs)(n.p,{children:["If, like me, you had performed the fine-tuning outside of the OCI Data Science notebook that you will use to prepare and deploy the model, then first upload the tarball containing the model's artifacts, and then extract the files. You can then use the following script to create a ",(0,a.jsx)(n.em,{children:"Pipeline"})," and test out some predictions. If not, simply create the pipeline using the trained model."]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n\ntokenizer = AutoTokenizer.from_pretrained("distilbert-base-uncased")\nmodel = AutoModelForSequenceClassification.from_pretrained("./mymodel")\n\npipe = pipeline(task="sentiment-analysis", model=model, tokenizer=tokenizer)\n\nreview = """\nI recently purchased Tim Horton\'s Nespresso pods for use at home, and I\nwas impressed by the consistent quality of the coffee. The pods are easy\nto store and use, making them a convenient option for busy individuals or\nsmall households. However, the pods can be expensive, especially if you\nare a frequent coffee drinker, and may not be available in all locations,\nlimiting your ability to enjoy Tim Horton\'s espresso on-the-go. Overall, I\nwould recommend Tim Horton\'s Nespresso pods as a convenient way to enjoy\nhigh-quality espresso at home, but keep in mind the cost and availability\nlimitations.\n"""\n\npipe(review)\n'})}),"\n",(0,a.jsx)(n.p,{children:"Here's a sample of the results:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"[{'label': 'LABEL_3', 'score': 0.9998452663421631}]\n"})}),"\n",(0,a.jsx)(n.p,{children:"The next task involves using ADS to prepare, save, and deploy the model on OCI Data Science. The following code assumes that:"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsx)(n.li,{children:"A dynamic group has been created and includes both the notebook sessions and model deployment resource types."}),"\n",(0,a.jsx)(n.li,{children:"An Identity and Access Management (IAM) policy has been created and contains the required statements to allow the dynamic group to create and manage the associated OCI resources."}),"\n",(0,a.jsx)(n.li,{children:"The Conda environment to be used for the model deployment has been published to the assigned Object Storage bucket."}),"\n"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'import ads\nfrom ads.common.model_metadata import UseCaseType\nfrom ads.model.framework.huggingface_model import HuggingFacePipelineModel\n\nads.set_auth(auth=\'resource_principal\')\n\nartifact_dir = "huggingface_pipeline_model_artifact"\nhuggingface_pipeline_model = HuggingFacePipelineModel(estimator=pipe, artifact_dir=artifact_dir)\n\n# 1. Prepare\nhuggingface_pipeline_model.prepare(\n    inference_conda_env="oci://conda-envs@*****/conda_environments/cpu/review/0.0.1/review_v0_0_1",\n    inference_python_version="3.8",\n    training_conda_env="oci://conda-envs@*****/conda_environments/cpu/review/0.0.1/review_v0_0_1",\n    use_case_type=UseCaseType.SENTIMENT_ANALYSIS,\n    force_overwrite=True,\n)\n\n# 2. Save\nhuggingface_pipeline_model.save()\n\n# 3. Deploy\nhuggingface_pipeline_model.deploy(\n    display_name="Demo Review Model",\n    deployment_instance_shape="VM.Standard.E4.Flex",\n    deployment_ocpus=1,\n    deployment_memory_in_gbs=16,\n)\n\n# 4. Print the invocation endpoint\nprint(f"Endpoint: {huggingface_pipeline_model.model_deployment.url}/predict")\n'})}),"\n",(0,a.jsx)(n.p,{children:"If everything ran as planned, then you should first see that the model has been saved to the model catalog in your OCI Data Science project."}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.img,{alt:"Hugging Face model saved in the OCI Data Science project.",src:t(98061).Z+"",width:"2880",height:"1800"})}),"\n",(0,a.jsxs)(n.p,{children:["You should also see that the model has been deployed with the resources specified. The ",(0,a.jsx)(n.em,{children:"Invoking your model"})," view provides the URL to call in your application."]}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.img,{alt:"Model deployment details and endpoint for invoking the model.",src:t(33572).Z+"",width:"2880",height:"1800"})}),"\n",(0,a.jsxs)(n.p,{children:["Once deployed, you may invoke the endpoint using ",(0,a.jsx)(n.code,{children:"APEX_WEB_SERVICE"})," to perform inference on the submitted text. However, like all secure OCI REST APIs, calling this endpoint will require the same HTTP request signature to authenticate with the service. This is an easy task with APEX web credentials support for OCI API keys. If you are new to this, check out either this Oracle blog ",(0,a.jsx)(n.a,{href:"https://blogs.oracle.com/connect/post/better-file-storage-in-oracle-cloud",children:"article"}),", or follow the steps outlined in this LiveLabs ",(0,a.jsx)(n.a,{href:"https://apexapps.oracle.com/pls/apex/r/dbpm/livelabs/view-workshop?wid=3558",children:"workshop"}),"."]}),"\n",(0,a.jsx)(n.p,{children:"Also, be sure to add an IAM policy with a statement similar to this one:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:"allow group ApexAgents to {DATA_SCIENCE_MODEL_DEPLOYMENT_PREDICT} in compartment Demo\n"})}),"\n",(0,a.jsx)(n.p,{children:"The IAM user used by Oracle APEX should be added to this group. This would allow it to call the prediction endpoint for an model deployed in the specified compartment."}),"\n",(0,a.jsx)(n.p,{children:"Create a page that allows users to enter a review, and then a page process that executes the PL/SQL code below, when the page is submitted."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-sql",children:"declare\n  l_response clob;\n  l_values apex_json.t_values;\n  l_prediction_label varchar2(10);\nbegin\n  apex_web_service.g_request_headers(1).name := 'Content-Type';\n  apex_web_service.g_request_headers(1).value := 'application/json';\n\n  l_response := apex_web_service.make_rest_request(\n    p_credential_static_id => 'OCI_CREDENTIALS'\n    , p_http_method => 'POST'\n    , p_url => 'https://modeldeployment.us-phoenix-1.oci.customer-oci.com/ocid1.datasciencemodeldeployment.oc1.phx.***/predict'\n    , p_body => '[' || apex_json.stringify(:P4_CONTENT) || ']'\n  );\n\n  if apex_web_service.g_status_code = 200 then\n    apex_json.parse(\n      p_values => l_values\n      , p_source => l_response\n    );\n\n    l_prediction_label := apex_json.get_varchar2(\n      p_values => l_values\n      , p_path => 'prediction[1].label'\n    );\n\n    insert into review(\n      prod_id\n      , content\n      , review_score\n      , reviewed_by\n      , reviewed_on\n    ) values (\n      :P4_PROD_ID\n      , :P4_CONTENT\n      , case l_prediction_label\n          when 'LABEL_0' then 1\n          when 'LABEL_1' then 2\n          when 'LABEL_2' then 3\n          when 'LABEL_3' then 4\n          when 'LABEL_4' then 5\n          else 0\n        end\n      , :APP_USER\n      , systimestamp\n    );\n  else\n    raise_application_error(-20001, 'Failed to process new review.');\n  end if;\nend;\n"})}),"\n",(0,a.jsxs)(n.p,{children:["From the JSON response from calling ",(0,a.jsx)(n.code,{children:"pipe(review)"}),", we know that it contains two attributes: ",(0,a.jsx)(n.code,{children:"label"})," and ",(0,a.jsx)(n.code,{children:"score"}),". The ",(0,a.jsx)(n.code,{children:"label"})," is what we are after, and thus, we extract the value of ",(0,a.jsx)(n.code,{children:"prediction[1].label"})," using the ",(0,a.jsx)(n.code,{children:"APEX_JSON"})," package. Sometimes, adding the confidence score can be helpful to the user."]}),"\n",(0,a.jsx)(n.p,{children:"Here's a simple demonstration of the fine-tuned model applied:"}),"\n",(0,a.jsx)(o,{videoId:"YlKM2vUrc0Q"}),"\n",(0,a.jsx)(n.h2,{id:"what-about-the-oci-ai-language-service",children:"What About the OCI AI Language Service"}),"\n",(0,a.jsxs)(n.p,{children:["If you are familiar with Oracle's suite of AI Service, there is the OCI ",(0,a.jsx)(n.a,{href:"https://www.oracle.com/artificial-intelligence/language/",children:"Language"}),' service that provides "sentiment analysis" using a pre-trained model. This is an easy-to-use API, and I have previously written on how to use it for ',(0,a.jsx)(n.a,{href:"/content/revolutionising-language-learning",children:"performing translation tasks"}),"."]}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.img,{alt:"Sentiment analysis results from invoking the OCI Language pre-trained model.",src:t(20018).Z+"",width:"2880",height:"1800"})}),"\n",(0,a.jsx)(n.p,{children:"However, with the fine-tuned LLM, my goal was to provide a review score, ranging from 1 to 5, based on the reviewer's comments. The OCI Language pre-trained model returns polarity (positive, negative, mixed, or neutral) scores at the document, sentence, and aspect levels. And at this time, there isn't a custom model that I could train for sentiment analysis."}),"\n",(0,a.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,a.jsxs)(n.p,{children:["The OCI Data Science platform is a great complement to Oracle APEX when it comes to training and integrating machine learning models for specialized tasks. Both the Hugging Face and Oracle ADS libraries make it very easy for a novice like me to quickly build applications with some intelligence. I hope it excites you as much as it does to me. If you have a use case where this combination of technology might help, I'd like to talk. You may reach out to me using this ",(0,a.jsx)(n.a,{href:"https://cal.com/fuzziebrain/iaas-consult",children:"form"}),"."]}),"\n",(0,a.jsxs)(n.blockquote,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Credits"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Hugging Face for the comprehensive documentation and tutorials, and making available the datasets and models used in this little experiment."}),"\n",(0,a.jsxs)(n.li,{children:["My employer, ",(0,a.jsx)(n.a,{href:"https://insum.ca",children:"Insum"}),", for kindly sponsoring the GPU resources. It can be rather challenging to get them on the OCI, and I hope that can change soon."]}),"\n",(0,a.jsxs)(n.li,{children:["Banner image generated using Stability AI's ",(0,a.jsx)(n.a,{href:"https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0",children:"Stable Diffusion"})," model."]}),"\n"]}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,i.a)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(d,{...e})}):d(e)}},309:(e,n,t)=>{t.d(n,{Z:()=>a});const a=t.p+"assets/images/llms-for-analyzing-customer-reviews-5f8f8c44197d2e7aba542aea8d90582d.png"},18809:(e,n,t)=>{t.d(n,{Z:()=>a});const a=t.p+"assets/images/gpu-status-4422868b3271e014bd79f3467a6c2235.png"},50553:(e,n,t)=>{t.d(n,{Z:()=>a});const a=t.p+"assets/images/llms-for-analyzing-customer-reviews-5f8f8c44197d2e7aba542aea8d90582d.png"},33572:(e,n,t)=>{t.d(n,{Z:()=>a});const a=t.p+"assets/images/model-deployed-2820d79824be4dbb49207c42f5e21a56.png"},98061:(e,n,t)=>{t.d(n,{Z:()=>a});const a=t.p+"assets/images/model-saved-346bdffafdadfbf6e31e1f24552b9dc9.png"},20018:(e,n,t)=>{t.d(n,{Z:()=>a});const a=t.p+"assets/images/oci-ai-language-results-9e4d6745bd2b0bc951c4ebee414c431f.png"},11151:(e,n,t)=>{t.d(n,{Z:()=>r,a:()=>s});var a=t(67294);const i={},o=a.createContext(i);function s(e){const n=a.useContext(o);return a.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:s(e.components),a.createElement(o.Provider,{value:n},e.children)}}}]);